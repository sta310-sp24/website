{
  "hash": "f06c8fef1de840e2b876f8010ded7c60",
  "result": {
    "markdown": "---\ntitle: \"HW 03: Multiple linear regression, Part 2\" \nsubtitle: \"due Wednesday, November 1 at 11:59pm\"\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\n  eval: false\n  warning: false\n  message: false\nbibliography: references.bib\n---\n\n\n# Introduction\n\nIn this analysis you will use multiple linear regression to fit and evaluate models using characteristics of LEGO sets to understand variability in the price.\n\n# Learning goals\n\nIn this assignment, you will...\n\n-   Use exploratory data analysis to inform feature engineering steps\n-   Use cross validation to evaluate and compare models\n-   Assess model conditions and multicollinearity\n-   Use inference to draw conclusions\n-   Continue developing a workflow for reproducible data analysis.\n\n# Getting started\n\nThe repo for this assignment is available on GitHub at [github.com/sta210-fa23](https://github.com/sta210-fa23 \"Course GitHub organization\") and starts with the prefix **hw-03**. See [Lab 01](../labs/lab-01.qmd) for more detailed instructions on getting started.\n\n# Packages\n\nThe following packages will be used in this assignment:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels) \nlibrary(knitr) \nlibrary(rms)\nlibrary(patchwork)\n# add other packages as needed\n```\n:::\n\n\n::: callout-important\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n:::\n\n# Data: LEGOs\n\nThe data for this analysis includes information about LEGO sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from @peterson2021.\n\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are\n\n-   `Pieces`: Number of pieces in the set from brickset.com.\n-   `Minifigures`: Number of minifigures (LEGO people) in the set scraped from brickset.com. LEGO sets with no minifigures have been coded as NA. NA's also represent missing data. This is due to how brickset.com reports their data.\n-   `Amazon_Price`: Amazon price of the set scraped from brickset.com (in U.S. dollars)\n-   `Size`: General size of the interlocking bricks (Large = LEGO Duplo sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends)\n-   `Theme`: Theme of the LEGO set\n-   `Year` : Year the LEGO set was produced\n-   `Pages`: Number of pages in the instruction booklet\n\nThe data are contained in `lego-sample.csv`. Use the code below to read in the data, replace the `NA`s in Minifigure with 0, and remove any observations that have missing values for the relevant variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlegos <- read_csv(\"data/lego-sample.csv\") |>\n  select(Size, Pieces, Theme, Amazon_Price, Year, Pages, Minifigures) |>\n  mutate(Minifigures = replace_na(Minifigures, 0)) |>\n  drop_na()\n```\n:::\n\n\n# Exercises\n\n::: callout-important\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n:::\n\n## Exercise 1\n\nIn this analysis, we dropped observations that have missing values for some of the relevant variables. What is a disadvantage of dropping observations that have missing values, instead of using a method to impute, i.e., fill in, the missing data? How might dropping these observations impact the generalizability of conclusions? What is the disadvantage of replacing the `NA`s in Minifigures to 0?\n\n## Exercise 2\n\nVisualize the distributions of the predictor variables `Pieces`, `Size`, `Year`, and `Pages`. Neatly arrange the plots using the [patchwork](https://patchwork.data-imaginist.com/index.html) package.\n\n## Exercise 3\n\nWhat are some feature engineering steps you might use to prepare the variables in the previous exercise for the model? Describe the steps and the function you would use. The list should incorporate at least three different `step_` functions.\n\n::: callout-tip\nUse the [recipes reference page](https://recipes.tidymodels.org/reference/index.html) for a list of `step_` functions.\n:::\n\n## Exercise 4\n\nThe distribution of `Theme` is shown below. The bars are ordered by the frequency they occur in the data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlegos |>\n  count(Theme) |>\nggplot(aes(x = fct_reorder(Theme, n), y = n)) +\n  geom_col() + \n    labs(title = \"Lego Set Theme\", \n         x = \"Theme\", \n         y = \"Number of LEGO sets\") + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](hw-03_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWhat is one reason we should avoid putting the variable `Theme` in a model as is?\n\n## Exercise 5\n\nWe will use v-fold cross validation to compare two models. We'll start by preparing the data, creating the folds, and defining the model specification that will be used for both models.\n\n-   Split the data into training (75%) and testing (25%) sets. Use a seed of `6`.\n\n-   Split the training data into 12 folds. Set the seed to `6` again.\n\n-   Specify the model.\n\n## Exercise 6\n\nUse the training data to create a recipe for the first model. The model uses the variables `Size`, `Theme`, and `Pages` to predict `Amazon_Price`. Conduct the following feature engineering steps:\n\n-   Use [`step_other()`](https://recipes.tidymodels.org/reference/step_other.html) to collapse `Theme` into fewer categories. Define the threshold such that any levels of `Theme` with fewer than 20 observations is defined as \"Other\".\n-   Mean-center `Pages`.\n-   Make dummy variables for all categorical predictors.\n-   Remove any predictors with zero variance.\n\nThen create the workflow that brings together this recipe and the model specification from the previous exercise.\n\n## Exercise 7\n\nConduct 12-fold cross validation using the workflow from the previous exercise. Calculate and display mean **RMSE** across the 12 folds.\n\n::: callout-note\nWe will just use RMSE to compare models for this assignment; however, in practice, it is best to take into account multiple model fit statistics to get a more holistic evaluation and comparison of the models.\n:::\n\n## Exercise 8\n\nNow let's consider a new model that includes all the variables used in model from Exercise 6 along with `Year`, `Pieces` and `Minifigures`.\n\n-   Use the training data to create a recipe that uses all the feature engineering steps in Exercise 6 with the addition of the following steps:\n\n    -   Create a new variable called `since2018` that calculates the number of years since 2018.\n    -   Remove `Year` as a potential predictor.\n    -   Mean-center `Pieces`.\n\n-   Create the workflow that brings together this recipe and the model specification from Exercise 5.\n\n-   Conduct 12-fold cross validation using this model workflow. Calculate and display mean **RMSE** across the 12 folds.\n\n## Exercise 9\n\nCompare the cross validation results from Exercises 7 and 8. Which model do you select based on RMSE? Briefly explain your choice.\n\n## Exercise 10\n\nRefit the selected model on the entire training data. Neatly display the model using 3 digits.\n\nThen, calculate VIF for the model and use it to comment on whether there are potential issues with multicollinearity.\n\n::: callout-tip\nWhen we fit a model using `recipe` and `workflow`, we need to extract the model object before using `augment` or `vif` functions. Fill in the name of the selected model in both blanks in the code below to extract the model object and calculate VIF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlegos_fit_model <- extract_fit_parsnip(______)\nvif(legos_fit_model$fit)\n```\n:::\n\n:::\n\n## Exercise 11\n\nCalculate RMSE on the training data and on the testing data. Use it to comment on how well the model performs on new data and whether there are signs of model overfit.\n\n## Exercise 12\n\n*Though we do not check the model conditions in this assignment, complete this exercise assuming the model conditions are met.*\n\nDescribe the effect of `Theme` on the price of LEGO sets, including an indication of which levels are statistically significant. Use a threshold of 0.05 to determine significance.\n\n::: render-commit-push\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates.\n:::\n\n## Exercise 13\n\nThe World Bank collects \"world development indicators\" about the past and current development of countries. These data are made available on the [World Bank's website](https://data.worldbank.org/indicator). It can be used to understand the relationships between these various factors and trends over time.\\\n\\\nFor this analysis, we focus on indicators from 2011 on 165 countries. The variables of interest are:\n\n-   `gdp.per.capita`: gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.\n\n-   `sanit.access.factor`: Population access to sanitation facilities (Low, High)\n\n-   `edu.expend`: Government expenditure on education, total (% of government expenditure)\n\n-   `life.expect`: Life expectancy at birth (in years)\n\nYou fit a model using sanitation access, education expenditures, and life expectancy to understand variability in GDP. The model takes the form\n\n$$\n\\begin{aligned}\\widehat{\\log(GDP)} = \\hat{\\beta}_0 &+ \\hat{\\beta}_1 ~ sanit.access.factor + \\hat{\\beta}_2 ~ edu.expend + \\hat{\\beta}_3 ~life.expect \\\\ &+ \\hat{\\beta}_4 ~ sanit.access.factor \\times life.expect\\end{aligned}\n$$\n\nThe output from R is shown below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n|term                                | estimate| std.error| statistic| p.value|\n|:-----------------------------------|--------:|---------:|---------:|-------:|\n|(Intercept)                         |    4.491|     1.638|     2.742|   0.007|\n|sanit.access.factorhigh             |   -6.993|     1.971|    -3.548|   0.001|\n|edu.expend                          |    0.097|     0.038|     2.550|   0.012|\n|life.expect                         |    0.030|     0.029|     1.061|   0.291|\n|sanit.access.factorhigh:life.expect |    0.122|     0.032|     3.853|   0.000|\n:::\n:::\n\n\n-   Interpret the coefficient of `edu.expend` in the context of the data.\n\n-   Interpret the coefficient of `life.expect` for countries with high sanitation access in the context of the data.\n\n    ::: callout-important\n    Write all interpretations in terms of the original units, [not]{.underline} the log-transformed units.\n    :::\n\n## Exercise 14\n\nBelow are plots from the exploratory data analysis of the relationships between the predictor variables. Based on these plots, what appears to be a potential issue with the model from Exercise 13? Briefly explain your response.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](hw-03_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n# Submission\n\n::: callout-warning\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\n\nRemember -- you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n:::\n\nTo submit your assignment:\n\n-   Go to [http://www.gradescope.com](http://www.gradescope.com/) and click *Log in* in the top right corner.\n-   Click *School Credentials* ➡️ *Duke NetID* and log in using your NetID credentials.\n-   Click on your *STA 210* course.\n-   Click on the assignment, and you'll be prompted to submit it.\n-   Mark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be \"checked\").\n-   Select the first page of your PDF submission to be associated with the *\"Workflow & formatting\"* section.\n\n# Grading (50 points)\n\n| Component             | Points |\n|-----------------------|--------|\n| Ex 1 - 14             | 47     |\n| Workflow & formatting | 3[^1]  |\n\n[^1]: The \"Workflow & formatting\" grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.[↩︎](https://sta210-fa23.netlify.app/hw/hw-01#fnref2)\n\n## References\n",
    "supporting": [
      "hw-03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}