{
  "hash": "820cb37a19b4f962d27bc61a17d69441",
  "result": {
    "markdown": "---\ntitle: \"SLR: Conditions + Model evaluation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2022-09-20\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[üîó STA 210 - Fall 2023 -  Schedule](https://sta210-fa23.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n## Announcements\n\n-   HW 01: due TODAY at 11:59pm\n\n-   Lab 03:\n\n    -   due Fri at 11:59pm (Tue labs)\n\n    -   due Sun at 11:59pm (Thu labs)\n\n-   Looking ahead: Exam 01:\n\n    -   Closed note in-class: Wed, Oct 4\n\n    -   Open note take-home: Wed, Oct 4 - Fri, Oct 6\n\n        -   Released after Section 002\n\n    -   More about the exam next week\n\n# Questions from last class?\n\n## Computational set up\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n## Regression model, revisited\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_fit <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 116652.325 </td>\n   <td style=\"text-align:right;\"> 53302.463 </td>\n   <td style=\"text-align:right;\"> 2.188 </td>\n   <td style=\"text-align:right;\"> 0.031 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> area </td>\n   <td style=\"text-align:right;\"> 159.483 </td>\n   <td style=\"text-align:right;\"> 18.171 </td>\n   <td style=\"text-align:right;\"> 8.777 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Mathematical representation, visualized {.midi}\n\n$$\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n$$\n\n![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/06/regression.png){fig-align=\"center\"}\n\n# Model conditions\n\n## Model conditions\n\n1.  **Linearity:** There is a linear relationship between the outcome and predictor variables\n2.  **Constant variance:** The variability of the errors is equal for all values of the predictor variable\n3.  **Normality:** The errors follow a normal distribution\n4.  **Independence:** The errors are independent from each other\n\n## Linearity\n\n-   If the linear model, $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i$ adequately describes the relationship between $X$ and $Y$, then the residuals should reflect random (chance) error\n\n-   To assess this, we can look at a plot of the residuals vs. the fitted values\n\n-   **Linearity satisfied** if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\n\n-   A non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between $X$ and $Y$\n\n## Linearity\n\n‚úÖ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/res-vs-fit-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Residuals vs. fitted values (code)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_aug <- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n```\n:::\n\n\n## Non-linear relationships\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Constant variance\n\n-   If the spread of the distribution of $Y$ is equal for all values of $X$then the spread of the residuals should be approximately equal for each value of $X$\n\n-   To assess this, we can look at a plot of the residuals vs. the fitted values\n\n-   **Constant variance satisfied** if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no \"fan\" pattern)\n\n-   A fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)\n\n## Constant variance\n\n‚úÖ The vertical spread of the residuals is relatively constant across the plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Non-constant variance\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Normality {.midi}\n\n-   The linear model assumes that the distribution of $Y$ is Normal for every value of $X$\n\n-   This is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\n\n-   **Normality satisfied** if a histogram of the residuals is approximately normal\n\n    -   Can also check that the points on a normal QQ-plot falls along a diagonal line\n\n-   Most inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, $n > 30$\n\n## Normality\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Check normality using a QQ-plot\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(df_aug, aes(x = .resid)) +\n  geom_histogram(binwidth = 50000, color = \"white\")  +\n  labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n```\n\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n```         \n```\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(df_aug, aes(sample = .resid)) +\n  stat_qq()+\n  stat_qq_line() + \n  labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n```\n\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n:::\n\n-   Assess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\n\n-   If so, the residuals are normally distributed.\n\n## Normality\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slr-conditions-eval_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n‚ùå The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n\n‚úÖ The sample size $n = 98 > 30$, so the sample size is large enough to relax this condition and proceed with inference.\n\n## Independence {.midi}\n\n-   We can often check the independence assumption based on the context of the data and how the observations were collected\n\n-   Two common violations of the independence assumption:\n\n    -   **Serial Effect**: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\n\n    -   **Cluster Effect**: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of house), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup\n\n## Independence {.nonincremental}\n\nRecall the description of the data:\n\n> -   Data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\n>\n> -   Scraped from Zillow\n\n<br>\n\n‚úÖ Based on the information we have, we can reasonably treat this as a random sample of Duke Forest Houses and assume the error for one house does not tell us anything about the error for another house.\n\n## Recap\n\nUsed residual plots to check conditions for SLR:\n\n::: columns\n::: {.column width=\"50%\"}\n::: nonincremental\n-   Linearity\n-   Constant variance\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: nonincremental\n-   Normality\n-   Independence\n:::\n:::\n:::\n\n. . .\n\n::: question\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\\\n\nEd Discussion \\[[Section 001](https://edstem.org/us/courses/44523/discussion/439750)\\]\\[[Section 002](https://edstem.org/us/courses/44523/discussion/439753)\\]\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_a696a3bf\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:2em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Comparing inferential methods\n\n::: question\n-   What are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\n\n-   Under what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_49fc997b\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:2em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n# Application exercise\n\n::: appex\nüìã [sta210-fa23.netlify.app/ae/ae-06-conditions](https://sta210-fa23.netlify.app/ae/ae-06-conditions.html)\n:::\n\n# Model evaluation\n\n## Two statistics {.small}\n\n::: incremental\n-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n    $$\n    R^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n    $$\n\n-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)\n\n    $$\n    RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n    $$\n:::\n\n. . .\n\n::: question\nWhat indicates a good model fit? Higher or lower $R^2$? Higher or lower RMSE?\n:::\n\n## $R^2$\n\n::: incremental\n-   Ranges between 0 (terrible predictor) and 1 (perfect predictor)\n\n-   Has no units\n\n-   Calculate with `rsq()` using the augmented data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsq(df_aug, truth = price, estimate = .fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.445\n```\n:::\n:::\n\n:::\n\n## Interpreting $R^2$ {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: poll\nüó≥Ô∏è **Discussion**\n\n::: midi\n::: poll\nThe $R^2$ of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n:::\n\n1.  Area correctly predicts 44.5% of price for houses in Duke Forest.\n2.  44.5% of the variability in price for houses in Duke Forest can be explained by area.\n3.  44.5% of the variability in area for houses in Duke Forest can be explained by price\n4.  44.5% of the time price for houses in Duke Forest can be predicted by area.\n:::\n:::\n\n## Alternative approach for $R^2$\n\nAlternatively, use `glance()` to construct a single row summary of the model fit, including $R^2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(df_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl>   <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ‚Ñπ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(df_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4451945\n```\n:::\n:::\n\n\n## RMSE\n\n::: incremental\n-   Ranges between 0 (perfect predictor) and infinity (terrible predictor)\n\n-   Same units as the response variable\n\n-   Calculate with `rmse()` using the augmented data:\n\n\n    ::: {.cell layout-align=\"center\"}\n    \n    ```{.r .cell-code}\n    rmse(df_aug, truth = price, estimate = .fitted)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    # A tibble: 1 √ó 3\n      .metric .estimator .estimate\n      <chr>   <chr>          <dbl>\n    1 rmse    standard     167067.\n    ```\n    :::\n    :::\n\n\n-   The value of RMSE is not very meaningful on its own, but it's useful for comparing across models (more on this when we get to regression with multiple predictors)\n:::\n\n## Obtaining $R^2$ and RMSE\n\n::: incremental\n-   Use `rsq()` and `rmse()`, respectively\n\n\n    ::: {.cell layout-align=\"center\"}\n    \n    ```{.r .cell-code}\n    rsq(df_aug, truth = price, estimate = .fitted)\n    rmse(df_aug, truth = price, estimate = .fitted)\n    ```\n    :::\n\n\n-   First argument: data frame containing `truth` and `estimate` columns\n\n-   Second argument: name of the column containing `truth` (observed outcome)\n\n-   Third argument: name of the column containing `estimate` (predicted outcome)\n:::\n\n# Application exercise\n\n::: appex\nüìã [sta210-fa23.netlify.app/ae/ae-06-conditions](https://sta210-fa23.netlify.app/ae/ae-06-conditions.html)\n:::\n",
    "supporting": [
      "07-slr-conditions-eval_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}