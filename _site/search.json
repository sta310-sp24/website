[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here."
  },
  {
    "objectID": "schedule.html#notes",
    "href": "schedule.html#notes",
    "title": "Schedule",
    "section": "Notes",
    "text": "Notes\n\nLab:\n\nLab sections 01 & 02 (Tuesdays): Labs will be due on Fridays at 11:59pm.\nLab sections 03 & 04 (Thursdays): Labs will be due on Sundays at 11:59pm.\n\nProject dates will be added when the project is assigned."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nSection 001\nMon & Wed 10:05 - 11:20am\nReuben-Cooke 130\n\n\n\nSection 002\nMon & Wed 1:25 - 2:40pm\nOld Chem 116\n\n\nLab\nLab 01\nTue 10:05 - 11:20am\nReuben-Cooke 129\n\n\n\nLab 02\nTue 11:45am - 1pm\nLSRC A155\n\n\n\nLab 03\nThu 10:05 - 11:20am\nReuben-Cooke 129\n\n\n\nLab 04\nThu 11:45am - 1pm\nSocial Sciences 124\n\n\n\n\n\n\n\n\n\nName\nRole\n\n\n\n\n\nProf.¬†Maria Tackett\nInstructor\nOffice Hours: Fri 1 - 3pm\n\n\nSam Rosen\nHead TA\nLab 01: Tue 10:05 - 11:20am\n\n\nBethany Astor\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nJon Campbell\nTA\n\n\n\nDonald Cayton\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nAllison Li\nTA\n\n\n\nMitchelle Mojekwu\nTA\nLab 04: Thu 11:45am - 1pm\n\n\nBen Thorpe\nTA\n\n\n\nLinxuan Wang\nTA\nLab 03: Thu 10:05 - 11:20am\n\n\nXiaojun Zheng\nTA\nLab 04: Thu 11:45am - 1pm\n\n\n\n\n\n\n\n\n\nClick here for schedule of office hours."
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nSection 001\nMon & Wed 10:05 - 11:20am\nReuben-Cooke 130\n\n\n\nSection 002\nMon & Wed 1:25 - 2:40pm\nOld Chem 116\n\n\nLab\nLab 01\nTue 10:05 - 11:20am\nReuben-Cooke 129\n\n\n\nLab 02\nTue 11:45am - 1pm\nLSRC A155\n\n\n\nLab 03\nThu 10:05 - 11:20am\nReuben-Cooke 129\n\n\n\nLab 04\nThu 11:45am - 1pm\nSocial Sciences 124\n\n\n\n\n\n\n\n\n\nName\nRole\n\n\n\n\n\nProf.¬†Maria Tackett\nInstructor\nOffice Hours: Fri 1 - 3pm\n\n\nSam Rosen\nHead TA\nLab 01: Tue 10:05 - 11:20am\n\n\nBethany Astor\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nJon Campbell\nTA\n\n\n\nDonald Cayton\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nAllison Li\nTA\n\n\n\nMitchelle Mojekwu\nTA\nLab 04: Thu 11:45am - 1pm\n\n\nBen Thorpe\nTA\n\n\n\nLinxuan Wang\nTA\nLab 03: Thu 10:05 - 11:20am\n\n\nXiaojun Zheng\nTA\nLab 04: Thu 11:45am - 1pm\n\n\n\n\n\n\n\n\n\nClick here for schedule of office hours."
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze real-world data to answer questions about multivariable relationships.\nuse R to fit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate statistical results through writing and oral presentations."
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard(DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can learn more at the Center for Sexual and Gender Diversity‚Äôs website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, sta210-fa23.netlify.app.\nLinks to Zoom meetings may be found in Sakai. Periodic announcements will be sent via email and will also be available through Ed Discussion and Sakai Announcements. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts in Ed Discussion before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\n\n\nEmail\nIf you have questions about assignment extensions or accommodations, please email sta210@duke.edu. This email address is accessible to me and the head TA. Please see Assignment extensions for more information. If you have any other questions that are not appropriate for the public forum, you are welcome to email me directly at maria.tackett@duke.edu. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources."
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nWhile there is no official textbook for the course, readings will primarily be assigned from the following texts (all freely available online).\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine √áetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler"
  },
  {
    "objectID": "syllabus.html#lectures-and-labs",
    "href": "syllabus.html#lectures-and-labs",
    "title": "Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nLectures and labs are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to prepare for class by completing assigned readings, attend all lecture and lab sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or Chromebook to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone."
  },
  {
    "objectID": "syllabus.html#teams",
    "href": "syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of the semester. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of the labs and project, and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team‚Äôs overall mark.\nYou are expected to make use of the provided GitHub repository as the central collaborative platform. Commits to this repository will be used as a metric (one of several) of each team member‚Äôs relative contribution for each project."
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on six components: application exercises, homework, labs, exams, project, and teamwork.\n\nApplication Exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice apply the statistical concepts and code introduced in the prepare assignment. These AEs are due within three days of the corresponding lecture period. Specifically, AEs from Monday lectures are due Thursday by 11:59p ET, and AEs from Wednesday lectures are due Saturday by 11:59p ET.\nBecause these AEs are for practice, they will be graded based on making a good-faith effort in attempting all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture. Successful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation and communication. Most lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team‚Äôs Git repository on the course‚Äôs GitHub page as the central platform for collaboration. Commits to this repository will be used as a metric of each team member‚Äôs relative contribution for each lab, and there will be periodic peer evaluation on the team collaboration. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted for grading in Gradescope.\nThe lowest lab grade will be dropped at the end of the semester.\n\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you‚Äôre learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will include a closed-notes in-class component and an open-note take-home component. Through these exams you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The exams will focus on both conceptual understanding of the content and application through analysis and computational tasks. The content of the exam will be related to the content in reading assignments, lectures, application exercises, homework, and lab assignments. More detail about the exams will be given during the semester.\n\n\nProject\nThe purpose of the final project is to apply what you‚Äôve learned throughout the semester to analyze an interesting data-driven research question. The project will be completed with your lab teams, and each team will present their work through a written report and presentation. More information about the project will be provided during the semester."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n35%\n\n\nFinal project\n15%\n\n\nLab\n15%\n\n\nExam 01\n15%\n\n\nExam 02\n15%\n\n\nApplication exercises\n2.5%\n\n\nTeamwork\n2.5%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60"
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TAs and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TAs, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you‚Äôre not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. It‚Äôs not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon‚Äôt procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don‚Äôt let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don‚Äôt cheat!\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\n\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\nFor the projects and team labs, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project or team labs across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g.¬†StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:2 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one‚Äôs own work, following proper citation of sources, adhering to guidance around group work projects,and more).Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work is accepted for application exercises, since these are designed as in-class activities to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email sta210@duke.edu before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let me know if you need help contacting your academic dean.\n\n\nAssignment extensions\nSam Rosen (sta210@duke.edu) will serve as the Head TA this semester. Among many things, Sam‚Äôs role is to handle all assignment extension questions. This includes answering questions about the late work policy, along with granting extensions based on the documented arrangements for accommodations and extenuating circumstances via the one-time late waiver or documentation from the Academic Dean. Please see Late work policy and Waiver for extenuating circumstances for more information about these policies.\nPlease direct all questions about late work policies to sta210@duke.edu. This email will be directed to Sam and myself. This email address only accept emails from Duke email addresses.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture and labs. There may be times, however, when you cannot attend class. Lecture recordings are available upon request for students who have an excused absence. See the Lecture recording request policy for more detail. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture. Labs dedicated to completing the lab assignment and collaborating with your lab team. If you miss a lab session, make sure to communicate with your lab TA and teammates about how you can make up your contribution. If you know you‚Äôre going to miss a lab session and you‚Äôre feeling well enough to do so, notify your lab TA and teammates ahead of time.\nMore details on Trinity attendance policies are available here.\n\nAttendance Policy Related to COVID Symptoms, Exposure, or Infection\nStudent health, safety, and well-being are the university‚Äôs top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have tested positive for COVID-19 or have possible symptoms and have not yet been tested. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health (dshcheckin@duke.edu, 919-681-9355). Learn more about current university policy related to COVID-19 at coronavirus.duke.edu.\nTo keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\n\nLecture recording request\nLectures will be recorded on Panopto and will be made available to students with an excused absence upon request. Videos shared with such students will be available for a week after the lecture date. To request a particular lecture‚Äôs video, please fill out the form at forms.office.com/r/FbRXJm4Ln1. Please submit the form within 24 hours of missing lecture to ensure you have sufficient time to watch the recording. Please also make sure that any official documentation, such as STINFs, Dean‚Äôs excuses, NOVAPs, and quarantine/removal from class notices from student health are also uploaded to the form.\nAbout one week before each exam, the class recordings will be available to all students. These recordings will be available until 10am the day of the exam."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you are a student with a disability and need accommodations for this class, it is your responsibility to register with the Student Disability Access Office (SDAO) and provide them with documentation of your disability. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.\n\n\nReligious accommodations\nStudents are permitted by university policy to be absent from class to observe a religious holiday. Accordingly, Trinity College of Arts & Sciences and the Pratt School of Engineering have established procedures to be followed by students for notifying their instructors of an absence necessitated by the observance of a religious holiday. Please submit requests for religious accommodations at the beginning of the semester so that we can work to make suitable arrangements well ahead of time. You can find the policy and relevant notification form here: trinity.duke.edu/undergraduate/academic-policies/religious-holidays"
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\nAcademic Resource Center\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917.\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000."
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nAug 28: Classes begin\nSep 8: Drop/add ends\nOct 16 - 17: Fall break\nNov 10: Last day to withdraw with W\nNov 22 - 26: Thanksgiving recess\nDec 8: Classes end\nDec 9 - 12: Reading period\nDec 13 - 18: Final exams\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.‚Ü©Ô∏é\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é‚Ü©Ô∏é"
  },
  {
    "objectID": "stats-experience.html",
    "href": "stats-experience.html",
    "title": "Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1Ô∏è‚É£ Have a statistics experience.\n2Ô∏è‚É£ Make a slide reflecting on your experience.\nYou must complete both parts to receive credit. The statistics experience will count as a homework grade."
  },
  {
    "objectID": "stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you‚Äôd like to do but you‚Äôre not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf talks\n\n2022 conference\n2021 conference\n2020 conference\n\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask Professor Tackett if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn‚Äôt on this list, ask Professor Tackett to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O‚ÄôNeil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don‚Äôt by Nate Silver\nList of books about data science ethics\n\nThis list is not exhaustive.\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n‚úÖ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n‚úÖ The visualization should include features or customization that are beyond what we‚Äôve done in class .\n‚úÖ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: CURV - connecting, uplifting, and recognizing voices\nCURV is a project by Dr.¬†Jo Hardin at Pomona College to highlight statisticians and data scientists from groups who have been historically marginalized in the discipline. We will highlight some of the scholars in the CURV data base through the ‚ÄúStatistician of the Day‚Äù during lecture each week.\nThere are two options for this statistics experience:\n1Ô∏è‚É£. Present the Statistician of the Day. Learn about one of the scholars in the CURV data base and present what you learn in class.\nA few guidelines:\n\nLet Professor Tackett know at least one week in advance of when you‚Äôd like to present a Statistician of the Day. They will take place at the beginning of lectures on Mondays.\nCreate 1 - 2 slides about the scholar.\nPresent at the beginning of lecture for both lectures. If you are unable to attend the beginning of both lectures, you can present in-person during your lecture period and provide a short video for the other lecture period.\n\n2Ô∏è‚É£ Contribute to the CURV data base. If there is a scholar you would like to suggest for the data base, submit your suggestion as an issue or pull request on the CURV GitHub repo and create a sample CURV page.\nA few guidelines:\n‚úÖ Create a draft of the CURV page for your suggested scholar. For reference, click here for the CURV page for W.E.B. Du Bois. The page must be created in a Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can find the Quarto documents for current scholars in the data base in the CURV GitHub repo. You can use one of these as a template to format your page.\n\n\n‚úÖ Make a pull request to the CURV GitHub repo to add the .qmd file for your suggested scholar, OR open an issue with a link to the .qmd file for your suggested scholar. You can ask a member of the teaching team if you have questions about how to do this.\n‚úÖ Include the URL to your pull request or issue in your one-slide reflection."
  },
  {
    "objectID": "stats-experience.html#part-2-reflect-on-your-experience",
    "href": "stats-experience.html#part-2-reflect-on-your-experience",
    "title": "Statistics Experience",
    "section": "Part 2: Reflect on your experience",
    "text": "Part 2: Reflect on your experience\nMake one slide summarizing and reflecting on your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we‚Äôve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e.¬†use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "stats-experience.html#submission",
    "href": "stats-experience.html#submission",
    "title": "Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Statistics Experience assignment on Gradescope by Mon, Nov 20 at 11:59pm. Standard homework late policy applies."
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#announcements",
    "href": "slides/14-cross-validation-pt2.html#announcements",
    "title": "Cross validation application",
    "section": "Announcements",
    "text": "Announcements\n\nSee Ed Discussion for upcoming events and internship opportunities\nStatistics Experience due Mon, Nov 20 at 11:59pm\nProf.¬†Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester\nStart the final project in lab this week - start thinking about the data your team wants to use"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#mid-semester-survey",
    "href": "slides/14-cross-validation-pt2.html#mid-semester-survey",
    "title": "Cross validation application",
    "section": "Mid-semester survey",
    "text": "Mid-semester survey\nThank you to everyone who filled out the mid-semester survey!\nAspect of class most helpful with learning\n\nApplication exercises\nLectures\nDiscussing content with others\n\nSomething to do in class to better help with learning\n\nZooming out more / reminder of the big picture\nTaking time to finish AEs (perhaps do some of this in lab)\nMore conceptual questions on assignments, specifically HW\n\nThings you do that are helpful with learning\n\nAttend office hours!\nReview course materials\nLots practice - review AEs, HW, labs"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#mid-semester-survey-1",
    "href": "slides/14-cross-validation-pt2.html#mid-semester-survey-1",
    "title": "Cross validation application",
    "section": "Mid-semester survey",
    "text": "Mid-semester survey\nWhy we do in-class exams\n\nOpportunity to demonstrate understanding of concepts and how they apply to application\n\nThis is what will make you stand out as a statistician/ data scientist!\n\nIn-class provides the most ‚Äúlevel‚Äù playing field to demonstrate conceptual understanding, given all the online resources available now\nLots of other opportunities to demonstrate application skills through labs, HW, final project, and take-home portion of exam"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#statistician-of-the-day-felicity-enders",
    "href": "slides/14-cross-validation-pt2.html#statistician-of-the-day-felicity-enders",
    "title": "Cross validation application",
    "section": "Statistician of the day: Felicity Enders",
    "text": "Statistician of the day: Felicity Enders\n\n\n\n\n\n\n\n\nDr.¬†Felicity Enders received her PhD from Johns Hopkins Bloomberg School of Public Health. She is a Professor of Biostatistics at the Mayo Clinic. With close to 200 publications, she has worked closely with clinicians, with particular focus on women‚Äôs health and psychology. Across the medical spectrum, Dr.¬†Enders has provided advanced statistical modeling collaboration in clinical trials.\nShe is also passionate about biostatistics education and works to dissolve the hidden curriculum for research, particularly statistical knowledge needed for non-statisticians.\n\n\nSource: hardin47.github.io/CURV/scholars/enders"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#felicity-enders",
    "href": "slides/14-cross-validation-pt2.html#felicity-enders",
    "title": "Cross validation application",
    "section": "Felicity Enders",
    "text": "Felicity Enders\nDr.¬†Enders was a statistician on an interdisciplinary research team that used logistic regression to identify demographic, clinical, and laboratory variables associated with the presence (or absence) of advanced fibrosis with the aim to create a scoring system that could be used by clinicians.\n‚ÄúData from each of the 4 countries were randomly separated into 2/3 and 1/3 of patients for model building and model validation, respectively. Hence, data on 480 patients were used to build a model, whereas data on 253 patients were used to validate the model.‚Äù\n‚Äú‚Ä¶cross-validation was used with 20 subgroups, so that at most 5% of the data under consideration was excluded at any one time. By employing cross-validation, the possibility of an unusually positive or negative validation subset could be assessed.‚Äù\nAngulo, Paul, et al.¬†‚ÄúThe NAFLD fibrosis score: a noninvasive system that identifies liver fibrosis in patients with NAFLD.‚Äù Hepatology 45.4 (2007): 846-854."
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#topics",
    "href": "slides/14-cross-validation-pt2.html#topics",
    "title": "Cross validation application",
    "section": "Topics",
    "text": "Topics\n\n\nCross validation application exercise"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#computational-setup",
    "href": "slides/14-cross-validation-pt2.html#computational-setup",
    "title": "Cross validation application",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#data-restaurant-tips",
    "href": "slides/14-cross-validation-pt2.html#data-restaurant-tips",
    "title": "Cross validation application",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 √ó 5\n     Tip Party Meal   Age    Alcohol\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  \n 1  2.99     1 Dinner Yadult No     \n 2  2        1 Dinner Yadult No     \n 3  5        1 Dinner SenCit No     \n 4  4        3 Dinner Middle No     \n 5 10.3      2 Dinner SenCit No     \n 6  4.85     2 Dinner Middle No     \n 7  5        4 Dinner Yadult No     \n 8  4        3 Dinner Middle No     \n 9  5        2 Dinner Middle No     \n10  1.58     1 Dinner SenCit No     \n# ‚Ñπ 159 more rows"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#variables",
    "href": "slides/14-cross-validation-pt2.html#variables",
    "title": "Cross validation application",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\nAlcohol: Whether the party ordered alcohol with the meal (Yes, No)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#outcome-tip",
    "href": "slides/14-cross-validation-pt2.html#outcome-tip",
    "title": "Cross validation application",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#predictors",
    "href": "slides/14-cross-validation-pt2.html#predictors",
    "title": "Cross validation application",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#outcome-vs.-predictors",
    "href": "slides/14-cross-validation-pt2.html#outcome-vs.-predictors",
    "title": "Cross validation application",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#analysis-goal",
    "href": "slides/14-cross-validation-pt2.html#analysis-goal",
    "title": "Cross validation application",
    "section": "Analysis goal",
    "text": "Analysis goal\n\nUse cross validation to evaluate and select a model to predict the tip amount\n\n\nv-fold cross validation ‚Äì commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#modeling-workflow",
    "href": "slides/14-cross-validation-pt2.html#modeling-workflow",
    "title": "Cross validation application",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nSplit data into training and test sets.\nUse cross validation on the training set to fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\nRefit the model using the entire training set and do ‚Äúfinal‚Äù evaluation on the test set (make sure you have not overfit the model).\n\nAdjust as needed if there is evidence of overfit.\n\nUse model fit on training set for inference and prediction.\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/anova-table.html",
    "href": "slides/anova-table.html",
    "title": "ANOVA Output in R",
    "section": "",
    "text": "We will use the Tips data set for this example.\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nThe variables of interest in this analysis are"
  },
  {
    "objectID": "slides/anova-table.html#model-fit",
    "href": "slides/anova-table.html#model-fit",
    "title": "ANOVA Output in R",
    "section": "Model fit",
    "text": "Model fit\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.17\n0.37\n-0.46\n0.64\n\n\nParty\n1.84\n0.12\n14.76\n0.00\n\n\nAgeMiddle\n1.01\n0.41\n2.47\n0.01\n\n\nAgeSenCit\n1.39\n0.48\n2.86\n0.00"
  },
  {
    "objectID": "slides/anova-table.html#anova",
    "href": "slides/anova-table.html#anova",
    "title": "ANOVA Output in R",
    "section": "ANOVA",
    "text": "ANOVA\nBelow is the ANOVA output for the model fit above.\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA\n\n\n\n\n\nWe will focus on the sum of squares in this document. The sum of squares are as follows:\n$$\n\\[\\begin{aligned}\n&SS_{Party} = 1188.64 \\\\\n&SS_{Age|Party} = 38.03 \\\\\n&SS_{Error} = SS_{Residuals} = 686.44 \\\\\n&SS_{Total} = 1913.11\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/anova-table.html#sum-of-squares-in-anova-table",
    "href": "slides/anova-table.html#sum-of-squares-in-anova-table",
    "title": "ANOVA Output in R",
    "section": "Sum of squares in ANOVA table",
    "text": "Sum of squares in ANOVA table\nR uses a sequential method to calculate sum of squares for the variables in the model. This means that the sum of squares attributed to each variable is the variation in the response explained by that variable after accounting for the total variation explained by the other variables already in the model.\nThe order of the sequence is determined by the order of the variables in the model fit code. This order is reflected in the order the variables appear in the ANOVA output. The sequential sum of squares attributed to each variable will change if the order of the variables in the model changes; however, the sum of squares attributed to the model overall will not change, regardless of the order of the variables.\nLet‚Äôs take a look at the sum of squares for Party, the first variable in the model. This value is calculated as the total variation in Tips explained by Party only. We can calculate this value by looking at the ANOVA table for simple linear regression model where Party is the only predictor.\n\nparty_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party, data = tips)\n\nanova(party_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n274\n0\n\n\nResiduals\n167\n724.47\n4.34\nNA\nNA\n\n\n\n\n\nNotice that the sum of squares in this table is the value of \\(SS_{Party}\\) above.\nNext, let‚Äôs add Age to the model. The sum of squares associated with Age is the additional variation in Tips explained by Age after accounting for variation explained by Party. This can be understood as the additional model variation in the model with Party and Age compared to a model that only includes Party. We can calculate this additional variation as follows:\n\nanova(party_fit$fit, tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nTip ~ Party\n167\n724.47\nNA\nNA\nNA\nNA\n\n\nTip ~ Party + Age\n165\n686.44\n2\n38.03\n4.57\n0.01\n\n\n\n\n\nNotice that the sum of squares in this table is the value of \\(SS_{Age|Party}\\) above.\n\n\n\n\n\n\nNote\n\n\n\nWhen we input two model is the anova() function, e.g., anova(Model 1, Model 2), the output produced is the additional sum of squares accounted for by the new variable(s) in Model 2 after accounting for the variables in Model 1. In this case, it is the additional sum of squares accounted for by Age after accounting for Party.\n\n\nWhen we use the ANOVA table, we are most interested in the variation in the response explained by the entire model, not the contribution from each variable. Therefore, we will primarily consider the \\(SS_{Model}\\), Sum of Squares Model. Because sum of squares are additive, it can be calculated as\n\\[\n\\begin{aligned}\nSS_{Model} &= SS_{Total} - SS_{Error} \\\\\n&= 1913.11 - 686.44 \\\\\n& = 1226.67\n\\end{aligned}\n\\]It can also be calculated as\n\\[\n\\begin{aligned}\nSS_{Model} &= SS_{Party} + SS_{Age | Party} \\\\\n& = 1188.64 + 38.03 \\\\\n& = 1226.67\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/19-logistic-compare.html#announcements",
    "href": "slides/19-logistic-compare.html#announcements",
    "title": "Logistic Regression: Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nProject draft due in your GitHub repo at 9am on\n\nNovember 14 (Tuesday labs)\nNovember 16 (Thursday labs)\nWill do peer review in lab those days\n\nTeam Feedback #1 due Friday, November 10 at 11:5pm\n\nReceived email from teammates\n\nHW 04 due Wednesday, November 15 at 11:59pm\n\nReleased later today"
  },
  {
    "objectID": "slides/19-logistic-compare.html#topics",
    "href": "slides/19-logistic-compare.html#topics",
    "title": "Logistic Regression: Model comparison",
    "section": "Topics",
    "text": "Topics\nComparing logistic regression models using\n\nDrop-in-deviance test\nAIC\nBIC"
  },
  {
    "objectID": "slides/19-logistic-compare.html#computational-setup",
    "href": "slides/19-logistic-compare.html#computational-setup",
    "title": "Logistic Regression: Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/19-logistic-compare.html#risk-of-coronary-heart-disease",
    "href": "slides/19-logistic-compare.html#risk-of-coronary-heart-disease",
    "title": "Logistic Regression: Model comparison",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/19-logistic-compare.html#data-prep",
    "href": "slides/19-logistic-compare.html#data-prep",
    "title": "Logistic Regression: Model comparison",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(here::here(\"slides\", \"data/framingham.csv\")) |&gt;\n  select(age, education, TenYearCHD, totChol, currentSmoker) |&gt;\n  drop_na() |&gt; #consider the limitations of doing this\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n\n# A tibble: 4,086 √ó 6\n     age education TenYearCHD totChol currentSmoker high_risk\n   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;    \n 1    39 4                  0     195 0             0        \n 2    46 2                  0     250 0             0        \n 3    48 1                  0     245 1             0        \n 4    61 3                  1     225 1             1        \n 5    46 3                  0     285 1             0        \n 6    43 2                  0     228 0             0        \n 7    63 1                  1     205 0             1        \n 8    45 2                  0     313 1             0        \n 9    52 1                  0     260 0             0        \n10    43 1                  0     225 1             0        \n# ‚Ñπ 4,076 more rows"
  },
  {
    "objectID": "slides/19-logistic-compare.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/19-logistic-compare.html#modeling-risk-of-coronary-heart-disease",
    "title": "Logistic Regression: Model comparison",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nUsing age and education:\n\nrisk_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/19-logistic-compare.html#model-output",
    "href": "slides/19-logistic-compare.html#model-output",
    "title": "Logistic Regression: Model comparison",
    "section": "Model output",
    "text": "Model output\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.508 + 0.076 ~ \\text{age} - 0.245 ~ \\text{ed2} - 0.236 ~ \\text{ed3} - 0.024 ~ \\text{ed4}}\n\\]\n\nShould we add currentSmoker to this model?"
  },
  {
    "objectID": "slides/19-logistic-compare.html#log-likelihood",
    "href": "slides/19-logistic-compare.html#log-likelihood",
    "title": "Logistic Regression: Model comparison",
    "section": "Log likelihood",
    "text": "Log likelihood\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\nDeviance = \\(-2 \\log L\\)\n\n\\(-2 \\log L\\) follows a \\(\\chi^2\\) distribution with \\(n - p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/19-logistic-compare.html#comparing-nested-models",
    "href": "slides/19-logistic-compare.html#comparing-nested-models",
    "title": "Logistic Regression: Model comparison",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes predictors \\(x_1, \\ldots, x_q\\)\nFull Model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least one }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\nTo do so, we will use the Drop-in-deviance test, also known as the Nested Likelihood Ratio test"
  },
  {
    "objectID": "slides/19-logistic-compare.html#drop-in-deviance-test",
    "href": "slides/19-logistic-compare.html#drop-in-deviance-test",
    "title": "Logistic Regression: Model comparison",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\n\n\nP-value: \\(P(\\chi^2 &gt; G)\\), calculated using a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models"
  },
  {
    "objectID": "slides/19-logistic-compare.html#chi2-distribution",
    "href": "slides/19-logistic-compare.html#chi2-distribution",
    "title": "Logistic Regression: Model comparison",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model",
    "href": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model",
    "title": "Logistic Regression: Model comparison",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nFirst model, reduced:\n\nrisk_fit_reduced &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\n\n\nSecond model, full:\n\nrisk_fit_full &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-1",
    "href": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-1",
    "title": "Logistic Regression: Model comparison",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate deviance for each model:\n\n(dev_reduced &lt;- glance(risk_fit_reduced)$deviance)\n\n[1] 3244.187\n\n(dev_full &lt;- glance(risk_fit_full)$deviance)\n\n[1] 3221.901\n\n\n\nDrop-in-deviance test statistic:\n\n(test_stat &lt;- dev_reduced - dev_full)\n\n[1] 22.2863"
  },
  {
    "objectID": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-2",
    "href": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-2",
    "title": "Logistic Regression: Model comparison",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate the p-value using a pchisq(), with degrees of freedom equal to the number of new model terms in the second model:\n\npchisq(test_stat, 1, lower.tail = FALSE) \n\n[1] 2.348761e-06\n\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that the coefficient of currentSmoker is not equal to 0. Therefore, we should add it to the model."
  },
  {
    "objectID": "slides/19-logistic-compare.html#drop-in-deviance-test-in-r",
    "href": "slides/19-logistic-compare.html#drop-in-deviance-test-in-r",
    "title": "Logistic Regression: Model comparison",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced$fit, risk_fit_full$fit, test = \"Chisq\") |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\ndf.residual\nresidual.deviance\ndf\ndeviance\np.value\n\n\n\n\nhigh_risk ~ age + education\n4081\n3244.187\nNA\nNA\nNA\n\n\nhigh_risk ~ age + education + currentSmoker\n4080\n3221.901\n1\n22.286\n0"
  },
  {
    "objectID": "slides/19-logistic-compare.html#model-selection",
    "href": "slides/19-logistic-compare.html#model-selection",
    "title": "Logistic Regression: Model comparison",
    "section": "Model selection",
    "text": "Model selection\nUse AIC or BIC for model selection\n\\[\n\\begin{align}\n&AIC = - 2 * \\log L - {\\color{purple}n\\log(n)}+ 2(p +1)\\\\[5pt]\n&BIC =- 2 * \\log L - {\\color{purple}n\\log(n)} + log(n)\\times(p+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/19-logistic-compare.html#aic-from-the-glance-function",
    "href": "slides/19-logistic-compare.html#aic-from-the-glance-function",
    "title": "Logistic Regression: Model comparison",
    "section": "AIC from the glance() function",
    "text": "AIC from the glance() function\nLet‚Äôs look at the AIC for the model that includes age, education, and currentSmoker\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nCalculating AIC\n\n- 2 * glance(risk_fit_full)$logLik + 2 * (5 + 1)\n\n[1] 3233.901"
  },
  {
    "objectID": "slides/19-logistic-compare.html#comparing-the-models-using-aic",
    "href": "slides/19-logistic-compare.html#comparing-the-models-using-aic",
    "title": "Logistic Regression: Model comparison",
    "section": "Comparing the models using AIC",
    "text": "Comparing the models using AIC\nLet‚Äôs compare the full and reduced models using AIC.\n\nglance(risk_fit_reduced)$AIC\n\n[1] 3254.187\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\n\nBased on AIC, which model would you choose?"
  },
  {
    "objectID": "slides/19-logistic-compare.html#comparing-the-models-using-bic",
    "href": "slides/19-logistic-compare.html#comparing-the-models-using-bic",
    "title": "Logistic Regression: Model comparison",
    "section": "Comparing the models using BIC",
    "text": "Comparing the models using BIC\nLet‚Äôs compare the full and reduced models using BIC\n\nglance(risk_fit_reduced)$BIC\n\n[1] 3285.764\n\nglance(risk_fit_full)$BIC\n\n[1] 3271.793\n\n\n\n\nBased on BIC, which model would you choose?"
  },
  {
    "objectID": "slides/19-logistic-compare.html#application-exercise",
    "href": "slides/19-logistic-compare.html#application-exercise",
    "title": "Logistic Regression: Model comparison",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 14: Comparing logistic regression models\n\n\nSit with your lab groups.\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/16-logistic-intro.html#announcements",
    "href": "slides/16-logistic-intro.html#announcements",
    "title": "Logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nHW 03 due Wed, Nov 1 at 11:59pm\nSee Ed Discussion for\n\nCode to reformat Theme variable in HW 03 data set\nExplanation on interpreting models with quadratic terms"
  },
  {
    "objectID": "slides/16-logistic-intro.html#spring-2024-statistics-courses",
    "href": "slides/16-logistic-intro.html#spring-2024-statistics-courses",
    "title": "Logistic regression",
    "section": "Spring 2024 statistics courses",
    "text": "Spring 2024 statistics courses\n\nSTA 211: Mathematics of Regression\n\nPrereqs: MATH 216/218/221, STA 210\n\nSTA 230 or STA 240: Probability\n\nPrereqs: MATH 22/112/122/202/212/219/222\n\nSTA 310: Generalized Linear Models\n\nPrereqs: STA 210 and STA 230/240\n\nSTA 313: Advanced Data Visualization\n\nPrereqs: STA 198 or STA 199 or STA 210\n\nSTA 323: Statistical Computing\n\nPrereqs: STA 210 and STA 230/240\n\nSTA 360: Bayesian Inference and Modern Statistical Methods\n\nPrereqs: STA 210 and STA 230/240 and MATH 202/212/219/222 and CS 101/102/201 and MATH 216/218/221, 211 (co-req)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#interpreting-models-with-log-transformed-variables",
    "href": "slides/16-logistic-intro.html#interpreting-models-with-log-transformed-variables",
    "title": "Logistic regression",
    "section": "Interpreting models with log-transformed variables",
    "text": "Interpreting models with log-transformed variables\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/16-logistic-intro.html#topics",
    "href": "slides/16-logistic-intro.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUse logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/16-logistic-intro.html#computational-setup",
    "href": "slides/16-logistic-intro.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data) #contains data set\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/16-logistic-intro.html#types-of-outcome-variables",
    "href": "slides/16-logistic-intro.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house in Duke Forest\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcome variable:\n\nIndicator of being high risk of getting coronary heart disease in the next 10 years\nModel: Probability an adult is high risk of heart disease in the next 10 years given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/16-logistic-intro.html#models-for-categorical-outcomes",
    "href": "slides/16-logistic-intro.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n1: Yes, 0: No\n\nMultinomial logistic regression\n3+ Outcomes\n1: Democrat, 2: Republican, 3: Independent"
  },
  {
    "objectID": "slides/16-logistic-intro.html#election-forecasts",
    "href": "slides/16-logistic-intro.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2022 election forecasts",
    "text": "2022 election forecasts\n\nSource: FiveThirtyEight 2022 Election Forecasts"
  },
  {
    "objectID": "slides/16-logistic-intro.html#nba-finals-predictions",
    "href": "slides/16-logistic-intro.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "2020 NBA finals predictions",
    "text": "2020 NBA finals predictions\n\nSource: FiveThirtyEight 2019-20 NBA Predictions"
  },
  {
    "objectID": "slides/16-logistic-intro.html#do-teenagers-get-7-hours-of-sleep",
    "href": "slides/16-logistic-intro.html#do-teenagers-get-7-hours-of-sleep",
    "title": "Logistic regression",
    "section": "Do teenagers get 7+ hours of sleep?",
    "text": "Do teenagers get 7+ hours of sleep?\n\n\nStudents in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.\nSleep7\n1: yes\n0: no\n\n\ndata(YouthRisk2009) #from Stat2Data package\nsleep &lt;- YouthRisk2009 |&gt;\n  as_tibble() |&gt;\n  filter(!is.na(Age), !is.na(Sleep7))\nsleep |&gt;\n  relocate(Age, Sleep7)\n\n# A tibble: 446 √ó 6\n     Age Sleep7 Sleep           SmokeLife SmokeDaily MarijuaEver\n   &lt;int&gt;  &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;     &lt;fct&gt;            &lt;int&gt;\n 1    16      1 8 hours         Yes       Yes                  1\n 2    17      0 5 hours         Yes       Yes                  1\n 3    18      0 5 hours         Yes       Yes                  1\n 4    17      1 7 hours         Yes       No                   1\n 5    15      0 4 or less hours No        No                   0\n 6    17      0 6 hours         No        No                   0\n 7    17      1 7 hours         No        No                   0\n 8    16      1 8 hours         Yes       No                   0\n 9    16      1 8 hours         No        No                   0\n10    18      0 4 or less hours Yes       Yes                  1\n# ‚Ñπ 436 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#plot-the-data",
    "href": "slides/16-logistic-intro.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data\n\nggplot(sleep, aes(x = Age, y = Sleep7)) +\n  geom_point() + \n  labs(y = \"Getting 7+ hours of sleep\")"
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-fit-a-linear-regression-model",
    "href": "slides/16-logistic-intro.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs fit a linear regression model",
    "text": "Let‚Äôs fit a linear regression model\nOutcome: \\(Y\\) = 1: yes, 0: no"
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-use-proportions",
    "href": "slides/16-logistic-intro.html#lets-use-proportions",
    "title": "Logistic regression",
    "section": "Let‚Äôs use proportions",
    "text": "Let‚Äôs use proportions\nOutcome: Probability of getting 7+ hours of sleep"
  },
  {
    "objectID": "slides/16-logistic-intro.html#what-happens-if-we-zoom-out",
    "href": "slides/16-logistic-intro.html#what-happens-if-we-zoom-out",
    "title": "Logistic regression",
    "section": "What happens if we zoom out?",
    "text": "What happens if we zoom out?\nOutcome: Probability of getting 7+ hours of sleep\n\nüõë This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-try-another-model",
    "href": "slides/16-logistic-intro.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs try another model",
    "text": "Let‚Äôs try another model\n\n‚úÖ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/16-logistic-intro.html#the-code",
    "href": "slides/16-logistic-intro.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nggplot(sleep_age, aes(x = Age, y = prop)) +\n  geom_point() + \n  geom_hline(yintercept = c(0,1), lty = 2) + \n  stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE) +\n  labs(y = \"P(7+ hours of sleep)\") +\n  xlim(1, 40) +\n  ylim(-0.5, 1.5)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#different-types-of-models",
    "href": "slides/16-logistic-intro.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLinear regression (transform Y)\nQuantitative\n\\(\\log(Y) = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#linear-vs.-logistic-regression",
    "href": "slides/16-logistic-intro.html#linear-vs.-logistic-regression",
    "title": "Logistic regression",
    "section": "Linear vs.¬†logistic regression",
    "text": "Linear vs.¬†logistic regression\n\nState whether a linear regression model or logistic regression model is more appropriate for each scenario.\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie‚Äôs total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year.\n\nSubmit your responses on Ed Discussion [10:05am][1:25pm]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#binary-response-variable",
    "href": "slides/16-logistic-intro.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\n\\(Y = 1: \\text{ yes}, 0: \\text{ no}\\)\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/16-logistic-intro.html#odds",
    "href": "slides/16-logistic-intro.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\n\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won‚Äôt rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#are-teenagers-getting-enough-sleep",
    "href": "slides/16-logistic-intro.html#are-teenagers-getting-enough-sleep",
    "title": "Logistic regression",
    "section": "Are teenagers getting enough sleep?",
    "text": "Are teenagers getting enough sleep?\n\nsleep |&gt;\n  count(Sleep7) |&gt;\n  mutate(p = round(n / sum(n), 3))\n\n# A tibble: 2 √ó 3\n  Sleep7     n     p\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1      0   150 0.336\n2      1   296 0.664\n\n\n\n\\(P(\\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664\\)\n\n\n\\(P(\\text{&lt; 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336\\)\n\n\n\\(P(\\text{odds of 7+ hours of sleep}) = \\frac{0.664}{0.336} = 1.976\\)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#from-odds-to-probabilities",
    "href": "slides/16-logistic-intro.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nodds\n\\[\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{\\omega}{1 + \\omega}\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#from-odds-to-probabilities-1",
    "href": "slides/16-logistic-intro.html#from-odds-to-probabilities-1",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\nLogistic model: log odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#logistic-regression-model",
    "href": "slides/16-logistic-intro.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\n\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\n\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#risk-of-coronary-heart-disease",
    "href": "slides/16-logistic-intro.html#risk-of-coronary-heart-disease",
    "title": "Logistic regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use age to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.\n\nhigh_risk:\n\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\n\nage: Age at exam time (in years)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#data-heart_disease",
    "href": "slides/16-logistic-intro.html#data-heart_disease",
    "title": "Logistic regression",
    "section": "Data: heart_disease",
    "text": "Data: heart_disease\n\n\n# A tibble: 4,240 √ó 2\n     age high_risk\n   &lt;dbl&gt; &lt;fct&gt;    \n 1    39 0        \n 2    46 0        \n 3    48 0        \n 4    61 1        \n 5    46 0        \n 6    43 0        \n 7    63 1        \n 8    45 0        \n 9    52 0        \n10    43 0        \n# ‚Ñπ 4,230 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#high-risk-vs.-age",
    "href": "slides/16-logistic-intro.html#high-risk-vs.-age",
    "title": "Logistic regression",
    "section": "High risk vs.¬†age",
    "text": "High risk vs.¬†age\n\nggplot(heart_disease, aes(x = high_risk, y = age)) +\n  geom_boxplot(fill = \"steelblue\") +\n  labs(x = \"High risk - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. High risk of heart disease\")"
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-fit-the-model",
    "href": "slides/16-logistic-intro.html#lets-fit-the-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs fit the model",
    "text": "Let‚Äôs fit the model\n\nheart_disease_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0"
  },
  {
    "objectID": "slides/16-logistic-intro.html#the-model",
    "href": "slides/16-logistic-intro.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0\n\n\n\n\n\n\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.075 \\times \\text{age}\\] where \\(\\hat{\\pi}\\) is the predicted probability of being high risk of having heart disease in the next 10 years"
  },
  {
    "objectID": "slides/16-logistic-intro.html#predicted-log-odds",
    "href": "slides/16-logistic-intro.html#predicted-log-odds",
    "title": "Logistic regression",
    "section": "Predicted log odds",
    "text": "Predicted log odds\n\naugment(heart_disease_fit$fit) |&gt; select(.fitted, .resid)\n\n# A tibble: 4,240 √ó 2\n   .fitted .resid\n     &lt;dbl&gt;  &lt;dbl&gt;\n 1  -2.65  -0.370\n 2  -2.13  -0.475\n 3  -1.98  -0.509\n 4  -1.01   1.62 \n 5  -2.13  -0.475\n 6  -2.35  -0.427\n 7  -0.858  1.56 \n 8  -2.20  -0.458\n 9  -1.68  -0.585\n10  -2.35  -0.427\n# ‚Ñπ 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted odds} = \\hat{\\omega} = \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp\\{-2.650\\} = 0.071\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#predicted-probabilities",
    "href": "slides/16-logistic-intro.html#predicted-probabilities",
    "title": "Logistic regression",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"prob\")\n\n# A tibble: 4,240 √ó 2\n   .pred_0 .pred_1\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1   0.934  0.0660\n 2   0.894  0.106 \n 3   0.878  0.122 \n 4   0.733  0.267 \n 5   0.894  0.106 \n 6   0.913  0.0870\n 7   0.702  0.298 \n 8   0.900  0.0996\n 9   0.843  0.157 \n10   0.913  0.0870\n# ‚Ñπ 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted probability} = \\hat{\\pi} = \\frac{\\exp\\{-2.650\\}}{1 + \\exp\\{-2.650\\}} = 0.066\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#predicted-classes",
    "href": "slides/16-logistic-intro.html#predicted-classes",
    "title": "Logistic regression",
    "section": "Predicted classes",
    "text": "Predicted classes\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"class\")\n\n# A tibble: 4,240 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ‚Ñπ 4,230 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#default-prediction",
    "href": "slides/16-logistic-intro.html#default-prediction",
    "title": "Logistic regression",
    "section": "Default prediction",
    "text": "Default prediction\nFor a logistic regression, the default prediction is the class.\n\npredict(heart_disease_fit, new_data = heart_disease)\n\n# A tibble: 4,240 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ‚Ñπ 4,230 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#observed-vs.-predicted",
    "href": "slides/16-logistic-intro.html#observed-vs.-predicted",
    "title": "Logistic regression",
    "section": "Observed vs.¬†predicted",
    "text": "Observed vs.¬†predicted\n\nWhat does the following table show?\n\n\npredict(heart_disease_fit, new_data = heart_disease) |&gt;\n  bind_cols(heart_disease) |&gt;\n  count(high_risk, .pred_class)\n\n# A tibble: 2 √ó 3\n  high_risk .pred_class     n\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt;\n1 0         0            3596\n2 1         0             644\n\n\n\n\nThe .pred_class is the class with the highest predicted probability. What is a limitation to using this method to determine the predicted class?"
  },
  {
    "objectID": "slides/16-logistic-intro.html#recap",
    "href": "slides/16-logistic-intro.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced logistic regression for binary response variable\nDescribed relationship between odds and probabilities\nUsed logistic regression model to calculate predicted odds and probabilities\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/17-odds-ratios.html#announcements",
    "href": "slides/17-odds-ratios.html#announcements",
    "title": "Odds ratios",
    "section": "Announcements",
    "text": "Announcements\n\nHW 03 TODAY at 11:59pm\nLab 06 due\n\nFriday, November 03 at 11:59pm (Tuesday labs)\nSunday, November 05 at 11:59pm (Thursday labs)"
  },
  {
    "objectID": "slides/17-odds-ratios.html#application-exercise",
    "href": "slides/17-odds-ratios.html#application-exercise",
    "title": "Odds ratios",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 13: Logistic regression (continuation of AE 12)"
  },
  {
    "objectID": "slides/17-odds-ratios.html#topics",
    "href": "slides/17-odds-ratios.html#topics",
    "title": "Odds ratios",
    "section": "Topics",
    "text": "Topics\n\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/17-odds-ratios.html#computational-setup",
    "href": "slides/17-odds-ratios.html#computational-setup",
    "title": "Odds ratios",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/17-odds-ratios.html#risk-of-coronary-heart-disease",
    "href": "slides/17-odds-ratios.html#risk-of-coronary-heart-disease",
    "title": "Odds ratios",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\n\n\nRows: 4240 Columns: 16\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (16): male, age, education, currentSmoker, cigsPerDay, BPMeds, prevalent...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "slides/17-odds-ratios.html#high-risk-vs.-education",
    "href": "slides/17-odds-ratios.html#high-risk-vs.-education",
    "title": "Odds ratios",
    "section": "High risk vs.¬†education",
    "text": "High risk vs.¬†education\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups",
    "href": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups",
    "title": "Odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\n\nWe want to compare the risk of heart disease for those with a High School diploma/GED and those with a college degree.\nWe‚Äôll use the odds to compare the two groups\n\n\\[\n\\text{odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{\\text{# of successes}}{\\text{# of failures}}\n\\]"
  },
  {
    "objectID": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups-1",
    "href": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups-1",
    "title": "Odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nOdds of being high risk for the High school or GED group: \\(\\frac{147}{1106} = 0.133\\)\nOdds of being high risk for the College group: \\(\\frac{70}{403} = 0.174\\)\nBased on this, we see those with a college degree had higher odds of being high risk for heart disease than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/17-odds-ratios.html#odds-ratio-or",
    "href": "slides/17-odds-ratios.html#odds-ratio-or",
    "title": "Odds ratios",
    "section": "Odds ratio (OR)",
    "text": "Odds ratio (OR)\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\nLet‚Äôs summarize the relationship between the two groups. To do so, we‚Äôll use the odds ratio (OR).\n\\[\nOR = \\frac{\\text{odds}_1}{\\text{odds}_2} = \\frac{\\omega_1}{\\omega_2}\n\\]"
  },
  {
    "objectID": "slides/17-odds-ratios.html#or-college-vs.-high-school-or-ged",
    "href": "slides/17-odds-ratios.html#or-college-vs.-high-school-or-ged",
    "title": "Odds ratios",
    "section": "OR: College vs.¬†High school or GED",
    "text": "OR: College vs.¬†High school or GED\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{HS}} = \\frac{0.174}{0.133} = \\mathbf{1.308}\\]\n\nThe odds of being high risk for heart disease are 1.30 times higher for those with a college degree than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/17-odds-ratios.html#or-college-vs.-some-high-school",
    "href": "slides/17-odds-ratios.html#or-college-vs.-some-high-school",
    "title": "Odds ratios",
    "section": "OR: College vs.¬†Some high school",
    "text": "OR: College vs.¬†Some high school\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{Some HS}} = \\frac{70/403}{323/1397} = 0.751\\]\n\nThe odds of being high risk for having heart disease for those with a college degree are 0.751 times the odds of being high risk for heart disease for those with some high school."
  },
  {
    "objectID": "slides/17-odds-ratios.html#more-natural-interpretation",
    "href": "slides/17-odds-ratios.html#more-natural-interpretation",
    "title": "Odds ratios",
    "section": "More natural interpretation",
    "text": "More natural interpretation\n\nIt‚Äôs more natural to interpret the odds ratio with a statement with the odds ratio greater than 1.\nThe odds of being high risk for heart disease are 1.33 times higher for those with some high school than those with a college degree."
  },
  {
    "objectID": "slides/17-odds-ratios.html#making-the-table-1",
    "href": "slides/17-odds-ratios.html#making-the-table-1",
    "title": "Odds ratios",
    "section": "Making the table 1",
    "text": "Making the table 1\nFirst, rename the levels of the categorical variables:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(\n    high_risk_names = if_else(high_risk == \"1\", \"High risk\", \"Not high risk\"),\n    education_names = case_when(\n      education == \"1\" ~ \"Some high school\",\n      education == \"2\" ~ \"High school or GED\",\n      education == \"3\" ~ \"Some college or vocational school\",\n      education == \"4\" ~ \"College\"\n    ),\n    education_names = fct_relevel(education_names, \"Some high school\", \"High school or GED\", \"Some college or vocational school\", \"College\")\n  )"
  },
  {
    "objectID": "slides/17-odds-ratios.html#making-the-table-2",
    "href": "slides/17-odds-ratios.html#making-the-table-2",
    "title": "Odds ratios",
    "section": "Making the table 2",
    "text": "Making the table 2\nThen, make the table:\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names)\n\n# A tibble: 8 √ó 3\n  education_names                   high_risk_names     n\n  &lt;fct&gt;                             &lt;chr&gt;           &lt;int&gt;\n1 Some high school                  High risk         323\n2 Some high school                  Not high risk    1397\n3 High school or GED                High risk         147\n4 High school or GED                Not high risk    1106\n5 Some college or vocational school High risk          88\n6 Some college or vocational school Not high risk     601\n7 College                           High risk          70\n8 College                           Not high risk     403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code-1",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code-1",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n)\n\n# A tibble: 4 √ó 3\n  education_names                   `High risk` `Not high risk`\n  &lt;fct&gt;                                   &lt;int&gt;           &lt;int&gt;\n1 Some high school                          323            1397\n2 High school or GED                        147            1106\n3 Some college or vocational school          88             601\n4 College                                    70             403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code-2",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code-2",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable()\n\n\n\n\neducation_names\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code-3",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code-3",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#categorical-predictor",
    "href": "slides/17-odds-ratios.html#categorical-predictor",
    "title": "Odds ratios",
    "section": "Categorical predictor",
    "text": "Categorical predictor\nRecall: Education - 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\nheart_edu_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ education, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046"
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-education4-log-odds",
    "href": "slides/17-odds-ratios.html#interpreting-education4-log-odds",
    "title": "Odds ratios",
    "section": "Interpreting education4: log-odds",
    "text": "Interpreting education4: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe log-odds of being high risk for heart disease are expected to be 0.286 less for those with a college degree compared to those with some high school (the baseline group).\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-education4-odds",
    "href": "slides/17-odds-ratios.html#interpreting-education4-odds",
    "title": "Odds ratios",
    "section": "Interpreting education4: odds",
    "text": "Interpreting education4: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe odds of being high risk for heart disease for those with a college degree are expected to be 0.751 (exp{-0.286}) times the odds for those with some high school."
  },
  {
    "objectID": "slides/17-odds-ratios.html#coefficients-odds-ratios",
    "href": "slides/17-odds-ratios.html#coefficients-odds-ratios",
    "title": "Odds ratios",
    "section": "Coefficients + odds ratios",
    "text": "Coefficients + odds ratios\nThe model coefficient, -0.286, is the expected change in the log-odds when going from the Some high school group to the College group.\n\nTherefore, \\(e^{-0.286}\\) = 0.751 is the expected change in the odds when going from the Some high school group to the College group.\n\n\n\\[\nOR  = e^{\\hat{\\beta}_j} = \\exp\\{\\hat{\\beta}_j\\}\n\\]"
  },
  {
    "objectID": "slides/17-odds-ratios.html#quantitative-predictor",
    "href": "slides/17-odds-ratios.html#quantitative-predictor",
    "title": "Odds ratios",
    "section": "Quantitative predictor",
    "text": "Quantitative predictor\n\nheart_age_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_age_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0"
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-age-log-odds",
    "href": "slides/17-odds-ratios.html#interpreting-age-log-odds",
    "title": "Odds ratios",
    "section": "Interpreting age: log-odds",
    "text": "Interpreting age: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\nFor each additional year in age, the log-odds of being high risk for heart disease are expected to increase by 0.076.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-age-odds",
    "href": "slides/17-odds-ratios.html#interpreting-age-odds",
    "title": "Odds ratios",
    "section": "Interpreting age: odds",
    "text": "Interpreting age: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\n\nFor each additional year in age, the odds of being high risk for heart disease are expected to multiply by a factor of 1.08 (exp{0.076}).\nAlternate interpretation: For each additional year in age, the odds of being high risk for heart disease are expected to increase by 8%."
  },
  {
    "objectID": "slides/17-odds-ratios.html#multiple-predictors",
    "href": "slides/17-odds-ratios.html#multiple-predictors",
    "title": "Odds ratios",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nheart_edu_age_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ education + age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_age_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000"
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds",
    "title": "Odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The log-odds of being high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, holding age constant.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds-1",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds-1",
    "title": "Odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the log-odds of being high risk for heart disease are expected to increase by 0.073, holding education level constant.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds",
    "title": "Odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The odds of being high risk for heart disease for those with a college degree are expected to be 0.98 (exp{-0.020}) times the odds for those with some high school, holding age constant.\n\n\n\n\n\n\nNote\n\n\nIn logistic regression with 2+ predictors, \\(exp\\{\\hat{\\beta}_j\\}\\) is often called the adjusted odds ratio (AOR)."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds-1",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds-1",
    "title": "Odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the odds being high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.073)), holding education level constant.\n\n\n\n\n\n\nNote\n\n\nIn logistic regression with 2+ predictors, \\(exp\\{\\hat{\\beta}_j\\}\\) is often called the adjusted odds ratio (AOR)."
  },
  {
    "objectID": "slides/17-odds-ratios.html#recap",
    "href": "slides/17-odds-ratios.html#recap",
    "title": "Odds ratios",
    "section": "Recap",
    "text": "Recap\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/13-comparison-cv.html#announcements",
    "href": "slides/13-comparison-cv.html#announcements",
    "title": "Model comparison + cross validation",
    "section": "Announcements",
    "text": "Announcements\n\nSee Ed Discussion for upcoming events and internship opportunities\nStatistics Experience due Mon, Nov 20 at 11:59pm\nPlease submit mid-semester feedback by Friday\nProf.¬†Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester"
  },
  {
    "objectID": "slides/13-comparison-cv.html#topics",
    "href": "slides/13-comparison-cv.html#topics",
    "title": "Model comparison + cross validation",
    "section": "Topics",
    "text": "Topics\n\n\nANOVA for multiple linear regression and sum of squares\nComparing models with \\(R^2\\) vs.¬†\\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam‚Äôs razor and parsimony\nCross validation"
  },
  {
    "objectID": "slides/13-comparison-cv.html#computational-setup",
    "href": "slides/13-comparison-cv.html#computational-setup",
    "title": "Model comparison + cross validation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/13-comparison-cv.html#data-restaurant-tips",
    "href": "slides/13-comparison-cv.html#data-restaurant-tips",
    "title": "Model comparison + cross validation",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 √ó 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ‚Ñπ 159 more rows"
  },
  {
    "objectID": "slides/13-comparison-cv.html#variables",
    "href": "slides/13-comparison-cv.html#variables",
    "title": "Model comparison + cross validation",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/13-comparison-cv.html#outcome-tip",
    "href": "slides/13-comparison-cv.html#outcome-tip",
    "title": "Model comparison + cross validation",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/13-comparison-cv.html#predictors",
    "href": "slides/13-comparison-cv.html#predictors",
    "title": "Model comparison + cross validation",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/13-comparison-cv.html#relevel-categorical-predictors",
    "href": "slides/13-comparison-cv.html#relevel-categorical-predictors",
    "title": "Model comparison + cross validation",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/13-comparison-cv.html#predictors-again",
    "href": "slides/13-comparison-cv.html#predictors-again",
    "title": "Model comparison + cross validation",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/13-comparison-cv.html#outcome-vs.-predictors",
    "href": "slides/13-comparison-cv.html#outcome-vs.-predictors",
    "title": "Model comparison + cross validation",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/13-comparison-cv.html#fit-and-summarize-model",
    "href": "slides/13-comparison-cv.html#fit-and-summarize-model",
    "title": "Model comparison + cross validation",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n\n\n\n\n\nIs this the best model to explain variation in tips?"
  },
  {
    "objectID": "slides/13-comparison-cv.html#another-model-summary",
    "href": "slides/13-comparison-cv.html#another-model-summary",
    "title": "Model comparison + cross validation",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/13-comparison-cv.html#analysis-of-variance-anova-1",
    "href": "slides/13-comparison-cv.html#analysis-of-variance-anova-1",
    "title": "Model comparison + cross validation",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#anova",
    "href": "slides/13-comparison-cv.html#anova",
    "title": "Model comparison + cross validation",
    "section": "ANOVA",
    "text": "ANOVA\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can‚Äôt be explained by the model (left in the residuals)\n\nIf the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be ‚Äúvaluable‚Äù (at least one of the \\(\\beta\\)s not equal to 0)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#anova-output-in-r1",
    "href": "slides/13-comparison-cv.html#anova-output-in-r1",
    "title": "Model comparison + cross validation",
    "section": "ANOVA output in R1",
    "text": "ANOVA output in R1\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA\n\n\n\n\n\n\n\nClick here for explanation about the way R calculates sum of squares for each variable."
  },
  {
    "objectID": "slides/13-comparison-cv.html#anova-output-with-totals",
    "href": "slides/13-comparison-cv.html#anova-output-with-totals",
    "title": "Model comparison + cross validation",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares",
    "href": "slides/13-comparison-cv.html#sum-of-squares",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares-ss_total",
    "href": "slides/13-comparison-cv.html#sum-of-squares-ss_total",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares: \\(SS_{Total}\\)",
    "text": "Sum of squares: \\(SS_{Total}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome\n\n\\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) = 1913.11"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares-ss_error",
    "href": "slides/13-comparison-cv.html#sum-of-squares-ss_error",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares: \\(SS_{Error}\\)",
    "text": "Sum of squares: \\(SS_{Error}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals\n\n\\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) = 686.44"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares-ss_model",
    "href": "slides/13-comparison-cv.html#sum-of-squares-ss_model",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares: \\(SS_{Model}\\)",
    "text": "Sum of squares: \\(SS_{Model}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Model}\\): Variability explained by the model\n\n\\(SS_{Total} - SS_{Error}\\) = 1226.67"
  },
  {
    "objectID": "slides/13-comparison-cv.html#r-squared-r2",
    "href": "slides/13-comparison-cv.html#r-squared-r2",
    "title": "Model comparison + cross validation",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]\n\n\n\nglance(tip_fit)$r.squared\n\n[1] 0.6411891"
  },
  {
    "objectID": "slides/13-comparison-cv.html#r-squared-r2-1",
    "href": "slides/13-comparison-cv.html#r-squared-r2-1",
    "title": "Model comparison + cross validation",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/13-comparison-cv.html#adjusted-r2",
    "href": "slides/13-comparison-cv.html#adjusted-r2",
    "title": "Model comparison + cross validation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares (sumsq/df) rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/13-comparison-cv.html#r2-and-adjusted-r2",
    "href": "slides/13-comparison-cv.html#r2-and-adjusted-r2",
    "title": "Model comparison + cross validation",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/13-comparison-cv.html#using-r2-and-adjusted-r2",
    "href": "slides/13-comparison-cv.html#using-r2-and-adjusted-r2",
    "title": "Model comparison + cross validation",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/13-comparison-cv.html#comparing-models-with-r2_adj",
    "href": "slides/13-comparison-cv.html#comparing-models-with-r2_adj",
    "title": "Model comparison + cross validation",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n    data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.674         0.664\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.683         0.662\n\n\n\n\n\n\nWhy did we not use the full recipe() workflow to fit Model 1 or Model 2?\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?\n\nVote on Ed Discussion [10:05am lecture][1:25pm lecture]"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic",
    "href": "slides/13-comparison-cv.html#aic-bic",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike‚Äôs Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz‚Äôs Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic-1",
    "href": "slides/13-comparison-cv.html#aic-bic-1",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic-2",
    "href": "slides/13-comparison-cv.html#aic-bic-2",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic-3",
    "href": "slides/13-comparison-cv.html#aic-bic-3",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/13-comparison-cv.html#using-aic-bic",
    "href": "slides/13-comparison-cv.html#using-aic-bic",
    "title": "Model comparison + cross validation",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e.¬†models with fewer terms)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#comparing-models-with-aic-and-bic",
    "href": "slides/13-comparison-cv.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison + cross validation",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 √ó 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  714.  736.\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 √ó 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  720.  757.\n\n\n\n\n\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "slides/13-comparison-cv.html#commonalities-between-criteria",
    "href": "slides/13-comparison-cv.html#commonalities-between-criteria",
    "title": "Model comparison + cross validation",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/13-comparison-cv.html#parsimony-and-occams-razor",
    "href": "slides/13-comparison-cv.html#parsimony-and-occams-razor",
    "title": "Model comparison + cross validation",
    "section": "Parsimony and Occam‚Äôs razor",
    "text": "Parsimony and Occam‚Äôs razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam‚Äôs razor because he ‚Äúshaved‚Äù his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/13-comparison-cv.html#in-pursuit-of-occams-razor",
    "href": "slides/13-comparison-cv.html#in-pursuit-of-occams-razor",
    "title": "Model comparison + cross validation",
    "section": "In pursuit of Occam‚Äôs razor",
    "text": "In pursuit of Occam‚Äôs razor\n\nOccam‚Äôs razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e.¬†parsimonious model"
  },
  {
    "objectID": "slides/13-comparison-cv.html#alternate-views",
    "href": "slides/13-comparison-cv.html#alternate-views",
    "title": "Model comparison + cross validation",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/13-comparison-cv.html#other-concerns-with-our-approach",
    "href": "slides/13-comparison-cv.html#other-concerns-with-our-approach",
    "title": "Model comparison + cross validation",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we‚Äôre making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n‚Äútrain‚Äù the model on the training data and pick a few models we‚Äôre genuinely considering as potentially good models\ntest those models on the testing set\n‚Ä¶and repeat this process multiple times"
  },
  {
    "objectID": "slides/13-comparison-cv.html#spending-our-data",
    "href": "slides/13-comparison-cv.html#spending-our-data",
    "title": "Model comparison + cross validation",
    "section": "Spending our data",
    "text": "Spending our data\n\nWe have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\nHowever, we usually need to understand the effectiveness of the model before using the test set.\nTypically we can‚Äôt decide on which final model to take to the test set without making model assessments.\nRemedy: Resampling to make model assessments on training data in a way that can generalize to new data."
  },
  {
    "objectID": "slides/13-comparison-cv.html#resampling-for-model-assessment",
    "href": "slides/13-comparison-cv.html#resampling-for-model-assessment",
    "title": "Model comparison + cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set. Model fit statistics such as \\(R^2_{Adj}\\), AIC, and BIC are calculated based on this fit.\nThe model is evaluated with the assessment set."
  },
  {
    "objectID": "slides/13-comparison-cv.html#resampling-for-model-assessment-1",
    "href": "slides/13-comparison-cv.html#resampling-for-model-assessment-1",
    "title": "Model comparison + cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\nImage source: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/13-comparison-cv.html#analysis-and-assessment-sets",
    "href": "slides/13-comparison-cv.html#analysis-and-assessment-sets",
    "title": "Model comparison + cross validation",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-1",
    "href": "slides/13-comparison-cv.html#cross-validation-1",
    "title": "Model comparison + cross validation",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation ‚Äì commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\nRepeat v times, updating which partition is used for assessment each time\n\n\nLet‚Äôs give an example where v = 3‚Ä¶"
  },
  {
    "objectID": "slides/13-comparison-cv.html#to-get-started",
    "href": "slides/13-comparison-cv.html#to-get-started",
    "title": "Model comparison + cross validation",
    "section": "To get started‚Ä¶",
    "text": "To get started‚Ä¶\nSplit data into training and test sets\n\nset.seed(345)\n\ntips_split &lt;- initial_split(tips)\ntips_train &lt;- training(tips_split)\ntips_test &lt;- testing(tips_split)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#to-get-started-1",
    "href": "slides/13-comparison-cv.html#to-get-started-1",
    "title": "Model comparison + cross validation",
    "section": "To get started‚Ä¶",
    "text": "To get started‚Ä¶\nCreate recipe\n\ntips_rec1 &lt;- recipe(Tip ~ Party + Age + Meal, \n                    data = tips_train)\n\ntips_rec1\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3"
  },
  {
    "objectID": "slides/13-comparison-cv.html#to-get-started-2",
    "href": "slides/13-comparison-cv.html#to-get-started-2",
    "title": "Model comparison + cross validation",
    "section": "To get started‚Ä¶",
    "text": "To get started‚Ä¶\nSpecify model\n\ntips_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nCreate workflow\n\ntips_wflow1 &lt;- workflow() |&gt;\n  add_model(tips_spec) |&gt;\n  add_recipe(tips_rec1)\n\ntips_wflow1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n0 Recipe Steps\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-step-1",
    "href": "slides/13-comparison-cv.html#cross-validation-step-1",
    "title": "Model comparison + cross validation",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/13-comparison-cv.html#tips-split-training-data",
    "href": "slides/13-comparison-cv.html#tips-split-training-data",
    "title": "Model comparison + cross validation",
    "section": "Tips: Split training data",
    "text": "Tips: Split training data\n\nfolds &lt;- vfold_cv(tips_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 √ó 2\n  splits          id   \n  &lt;list&gt;          &lt;chr&gt;\n1 &lt;split [84/42]&gt; Fold1\n2 &lt;split [84/42]&gt; Fold2\n3 &lt;split [84/42]&gt; Fold3"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-steps-2-and-3",
    "href": "slides/13-comparison-cv.html#cross-validation-steps-2-and-3",
    "title": "Model comparison + cross validation",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\n\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/13-comparison-cv.html#tips-fit-resamples",
    "href": "slides/13-comparison-cv.html#tips-fit-resamples",
    "title": "Model comparison + cross validation",
    "section": "Tips: Fit resamples",
    "text": "Tips: Fit resamples\n\ntips_fit_rs1 &lt;- tips_wflow1 |&gt;\n  fit_resamples(folds)\n\ntips_fit_rs1 \n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 √ó 4\n  splits          id    .metrics         .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt;"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-now-what",
    "href": "slides/13-comparison-cv.html#cross-validation-now-what",
    "title": "Model comparison + cross validation",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe‚Äôve fit a bunch of models\nNow it‚Äôs time to use them to collect metrics (e.g., $R^2$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/13-comparison-cv.html#collect-r2-and-rmse-from-cv",
    "href": "slides/13-comparison-cv.html#collect-r2-and-rmse-from-cv",
    "title": "Model comparison + cross validation",
    "section": "Collect \\(R^2\\) and RMSE from CV",
    "text": "Collect \\(R^2\\) and RMSE from CV\n\n# Produces summary across all CV\ncollect_metrics(tips_fit_rs1)\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.10      3  0.243  Preprocessor1_Model1\n2 rsq     standard   0.591     3  0.0728 Preprocessor1_Model1\n\n\n\nNote: These are calculated using the assessment data"
  },
  {
    "objectID": "slides/13-comparison-cv.html#deeper-look-into-r2-and-rmse",
    "href": "slides/13-comparison-cv.html#deeper-look-into-r2-and-rmse",
    "title": "Model comparison + cross validation",
    "section": "Deeper look into \\(R^2\\) and RMSE",
    "text": "Deeper look into \\(R^2\\) and RMSE\n\ncv_metrics1 &lt;- collect_metrics(tips_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 √ó 5\n  id    .metric .estimator .estimate .config             \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Fold1 rmse    standard       2.04  Preprocessor1_Model1\n2 Fold1 rsq     standard       0.736 Preprocessor1_Model1\n3 Fold2 rmse    standard       1.71  Preprocessor1_Model1\n4 Fold2 rsq     standard       0.509 Preprocessor1_Model1\n5 Fold3 rmse    standard       2.54  Preprocessor1_Model1\n6 Fold3 rsq     standard       0.528 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/13-comparison-cv.html#better-tabulation-of-r2-and-rmse-from-cv",
    "href": "slides/13-comparison-cv.html#better-tabulation-of-r2-and-rmse-from-cv",
    "title": "Model comparison + cross validation",
    "section": "Better tabulation of \\(R^2\\) and RMSE from CV",
    "text": "Better tabulation of \\(R^2\\) and RMSE from CV\n\ncv_metrics1 |&gt;\n  mutate(.estimate = round(.estimate, 3)) |&gt;\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |&gt;\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n2.036\n0.736\n\n\nFold2\n1.709\n0.509\n\n\nFold3\n2.545\n0.528"
  },
  {
    "objectID": "slides/13-comparison-cv.html#how-does-rmse-compare-to-y",
    "href": "slides/13-comparison-cv.html#how-does-rmse-compare-to-y",
    "title": "Model comparison + cross validation",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\n\n\nCross validation RMSE stats:\n\ncv_metrics1 |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 √ó 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.71  2.54  2.10 0.421\n\n\n\nTraining data tips:\n\ntips_train |&gt;\n  summarise(\n    min = min(Tip),\n    max = max(Tip),\n    mean = mean(Tip),\n    sd = sd(Tip)\n  )\n\n# A tibble: 1 √ó 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  19.5  4.87  3.37"
  },
  {
    "objectID": "slides/13-comparison-cv.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "href": "slides/13-comparison-cv.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "title": "Model comparison + cross validation",
    "section": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold",
    "text": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold\n\n# Function to get Adj R-sq, AIC, BIC\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}\n\n# Fit model and calculate statistics for each fold\ntips_fit_rs1 &lt;-tips_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n\ntips_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 √ó 5\n  splits          id    .metrics         .notes           .extracts       \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;"
  },
  {
    "objectID": "slides/13-comparison-cv.html#collect-r2_adj-aic-bic-from-cv",
    "href": "slides/13-comparison-cv.html#collect-r2_adj-aic-bic-from-cv",
    "title": "Model comparison + cross validation",
    "section": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV",
    "text": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV\n\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  bind_cols(Fold = tips_fit_rs1$id)\n\n# A tibble: 3 √ó 4\n  adj.r.squared   AIC   BIC Fold \n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1         0.550  369.  386. Fold1\n2         0.659  377.  394. Fold2\n3         0.718  337.  354. Fold3\n\n\n\nNote: These are based on the model fit from the analysis data"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-in-practice",
    "href": "slides/13-comparison-cv.html#cross-validation-in-practice",
    "title": "Model comparison + cross validation",
    "section": "Cross validation in practice",
    "text": "Cross validation in practice\n\n\nTo illustrate how CV works, we used v = 3:\n\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/13-comparison-cv.html#recap",
    "href": "slides/13-comparison-cv.html#recap",
    "title": "Model comparison + cross validation",
    "section": "Recap",
    "text": "Recap\n\nANOVA for multiple linear regression and sum of squares\nComparing models with\n\n\\(R^2\\) vs.¬†\\(R^2_{Adj}\\)\nAIC and BIC\n\nOccam‚Äôs razor and parsimony\n\n\n\nCross validation for\n\nmodel evaluation\nmodel comparison\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#announcements",
    "href": "slides/04-slr-evaluation.html#announcements",
    "title": "SLR: Prediction + model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours have started. Click here for full schedule.\nAccept the email invitation to join the sta210-fa22 GitHub organization by today at 11:59pm.\n\nYou may also go to the course organization and click to accept on the banner at the top of the page.\nIf you don‚Äôt see the email or banner invitation, please email Prof.¬†Tackett (maria.tackett@duke.edu).\n\nLab 01 this week - will need access to RStudio and to be a member of the course GitHub organization.\nSee Week 02 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/04-slr-evaluation.html#topics",
    "href": "slides/04-slr-evaluation.html#topics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Topics",
    "text": "Topics\n\nMotivate the importance of model evaluation\nDescribe how \\(R^2\\) and RMSE are used to evaluate models\nAssess model‚Äôs predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#computational-setup",
    "href": "slides/04-slr-evaluation.html#computational-setup",
    "title": "SLR: Prediction + model evaluation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#application-exercise",
    "href": "slides/04-slr-evaluation.html#application-exercise",
    "title": "SLR: Prediction + model evaluation",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#data-source",
    "href": "slides/04-slr-evaluation.html#data-source",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data source",
    "text": "Data source\n\nThe data come from usdata::county_2019\nThese data have been compiled from the 2019 American Community Survey\n\n\n\nJoining with `by = join_by(region, subregion)`"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#uninsurance-rate",
    "href": "slides/04-slr-evaluation.html#uninsurance-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance rate",
    "text": "Uninsurance rate"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#high-school-graduation-rate",
    "href": "slides/04-slr-evaluation.html#high-school-graduation-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "High school graduation rate",
    "text": "High school graduation rate"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#examining-the-relationship",
    "href": "slides/04-slr-evaluation.html#examining-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Examining the relationship",
    "text": "Examining the relationship\n\n\nThe NC Labor and Economic Analysis Division (LEAD) ‚Äúcollects data, conducts research and analysis and publishes reports about the state‚Äôs economy and labor market. Information and data produced by LEAD help stakeholders make more informed decisions on business recruitment, education and workforce policies and career development, as well as gain a more extensive view of North Carolina‚Äôs economy.‚Äù\nSuppose that an analyst working for LEAD is interested in the relationship between uninsurance and high school graduation rates in NC counties.\n\n\n\n\nWhat type of visualization should the analyst make to examine the relationship between these two variables?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#data-prep",
    "href": "slides/04-slr-evaluation.html#data-prep",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data prep",
    "text": "Data prep\n\ncounty_2019_nc &lt;- county_2019 |&gt;\n  as_tibble() |&gt;\n  filter(state == \"North Carolina\") |&gt;\n  select(name, hs_grad, uninsured)\n\ncounty_2019_nc\n\n# A tibble: 100 √ó 3\n   name             hs_grad uninsured\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n 1 Alamance County     86.3      11.2\n 2 Alexander County    82.4       8.9\n 3 Alleghany County    77.5      11.3\n 4 Anson County        80.7      11.1\n 5 Ashe County         85.1      12.6\n 6 Avery County        83.6      15.9\n 7 Beaufort County     87.7      12  \n 8 Bertie County       78.4      11.9\n 9 Bladen County       81.3      12.9\n10 Brunswick County    91.3       9.8\n# ‚Ñπ 90 more rows"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/04-slr-evaluation.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance vs.¬†HS graduation rates",
    "text": "Uninsurance vs.¬†HS graduation rates\n\n\nCode\nggplot(county_2019_nc,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  ) +\n  geom_point(data = county_2019_nc |&gt; filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured), shape = \"circle open\", color = \"#8F2D56\", size = 4, stroke = 2) +\n  geom_text(data = county_2019_nc |&gt; filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured, label = name), color = \"#8F2D56\", fontface = \"bold\", nudge_y = 3, nudge_x = 2)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#modeling-the-relationship",
    "href": "slides/04-slr-evaluation.html#modeling-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Modeling the relationship",
    "text": "Modeling the relationship\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#fitting-the-model",
    "href": "slides/04-slr-evaluation.html#fitting-the-model",
    "title": "SLR: Prediction + model evaluation",
    "section": "Fitting the model",
    "text": "Fitting the model\nWith fit():\n\nnc_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#augmenting-the-data",
    "href": "slides/04-slr-evaluation.html#augmenting-the-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug &lt;- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 √ó 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# ‚Ñπ 90 more rows"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-i",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-i",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model I",
    "text": "Visualizing the model I\n\n\n\n\nBlack circles: Observed values (y = uninsured)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-ii",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-ii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model II",
    "text": "Visualizing the model II\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-iii",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-iii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model III",
    "text": "Visualizing the model III\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-iv",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-iv",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model IV",
    "text": "Visualizing the model IV\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\nGray dashed lines: Residuals\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#evaluating-the-model-fit",
    "href": "slides/04-slr-evaluation.html#evaluating-the-model-fit",
    "title": "SLR: Prediction + model evaluation",
    "section": "Evaluating the model fit",
    "text": "Evaluating the model fit\n\nHow can we evaluate whether the model for predicting uninsurance rate from high school graduation rate for NC counties is a good fit?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#two-statistics",
    "href": "slides/04-slr-evaluation.html#two-statistics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#r2",
    "href": "slides/04-slr-evaluation.html#r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.243"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#interpreting-r2",
    "href": "slides/04-slr-evaluation.html#interpreting-r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\nüó≥Ô∏è Vote on Ed Discussion\n\n\nThe \\(R^2\\) of the model for predicting uninsurance rate from high school graduation rate for NC counties is 24.3%. Which of the following is the correct interpretation of this value?\n\n\nHigh school graduation rates correctly predict 24.3% of uninsurance rates in NC counties.\n24.3% of the variability in uninsurance rates in NC counties can be explained by high school graduation rates.\n24.3% of the variability in high school graduation rates in NC counties can be explained by uninsurance rates.\n24.3% of the time uninsurance rates in NC counties can be predicted by high school graduation rates.\n\n\n\nVote - Section 001\nVote - Section 002"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#alternative-approach-for-r2",
    "href": "slides/04-slr-evaluation.html#alternative-approach-for-r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(nc_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.243         0.235  2.09      31.5 0.000000188     1  -214.  435.  443.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nglance(nc_fit)$r.squared\n\n[1] 0.2430694"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#rmse",
    "href": "slides/04-slr-evaluation.html#rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        2.07\n\n\nThe value of RMSE is not very meaningful on its own, but it‚Äôs useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#obtaining-r2-and-rmse",
    "href": "slides/04-slr-evaluation.html#obtaining-r2-and-rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#purpose-of-model-evaluation",
    "href": "slides/04-slr-evaluation.html#purpose-of-model-evaluation",
    "title": "SLR: Prediction + model evaluation",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e.¬†out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#spending-our-data",
    "href": "slides/04-slr-evaluation.html#spending-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Spending our data",
    "text": "Spending our data\n\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we‚Äôve done so far)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#simulation-data-splitting",
    "href": "slides/04-slr-evaluation.html#simulation-data-splitting",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#predictive-performance",
    "href": "slides/04-slr-evaluation.html#predictive-performance",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#bootstrapping-our-data",
    "href": "slides/04-slr-evaluation.html#bootstrapping-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/04-slr-evaluation.html#simulation-bootstrapping",
    "href": "slides/04-slr-evaluation.html#simulation-bootstrapping",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\n\nTake a bootstrap sample ‚Äì sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#predictive-performance-1",
    "href": "slides/04-slr-evaluation.html#predictive-performance-1",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#recap",
    "href": "slides/04-slr-evaluation.html#recap",
    "title": "SLR: Prediction + model evaluation",
    "section": "Recap",
    "text": "Recap\n\nMotivated the importance of model evaluation\nDescribed how \\(R^2\\) and RMSE are used to evaluate models\nAssessed model‚Äôs predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#next-week",
    "href": "slides/04-slr-evaluation.html#next-week",
    "title": "SLR: Prediction + model evaluation",
    "section": "Next week",
    "text": "Next week\nInference on the slope using\n\nSimulation-based methods\nMathematical models\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/02-big-picture.html#announcements",
    "href": "slides/02-big-picture.html#announcements",
    "title": "The big picture",
    "section": "Announcements",
    "text": "Announcements\n\nResources for extra R review\n\nLearn R: An interactive introduction to data analysis R (focus on Chapters 4 - 6)\nDuke Library Center for Data and Visualization Sciences workshops\n\nR for Lunch: data wrangling with dplyr (Fri, Sep 1, 12:30 - 1:30)\nR for Lunch: visualization with ggplot2 (Fri, Sep 8, 12:30 - 1:30)\nSee the CDVS website for more information and to register.\n\n\nLast day of in-person work for this class is Dec 7\nLecture recordings request policy\nReadings for next week will be posted later this week"
  },
  {
    "objectID": "slides/02-big-picture.html#topics",
    "href": "slides/02-big-picture.html#topics",
    "title": "The big picture",
    "section": "Topics",
    "text": "Topics\n\nData analysis life cycle\nReproducible data analysis\nAnalyzing multivariable relationships"
  },
  {
    "objectID": "slides/02-big-picture.html#reproducibility-checklist",
    "href": "slides/02-big-picture.html#reproducibility-checklist",
    "title": "The big picture",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Can the tables and figures be exactly reproduced from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/02-big-picture.html#why-is-reproducibility-important",
    "href": "slides/02-big-picture.html#why-is-reproducibility-important",
    "title": "The big picture",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "slides/02-big-picture.html#when-things-go-wrong",
    "href": "slides/02-big-picture.html#when-things-go-wrong",
    "title": "The big picture",
    "section": "When things go wrong",
    "text": "When things go wrong\n\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al.¬†2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al.¬†2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nSource: Ostblom and Timbers (2022)"
  },
  {
    "objectID": "slides/02-big-picture.html#toolkit",
    "href": "slides/02-big-picture.html#toolkit",
    "title": "The big picture",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\n\n\n\n\n\n\nNote\n\n\nYou will start using these computing tools in Lab 01."
  },
  {
    "objectID": "slides/02-big-picture.html#r-and-rstudio",
    "href": "slides/02-big-picture.html#r-and-rstudio",
    "title": "The big picture",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\nSource: Statistical Inference via Data Science"
  },
  {
    "objectID": "slides/02-big-picture.html#rstudio-ide",
    "href": "slides/02-big-picture.html#rstudio-ide",
    "title": "The big picture",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/02-big-picture.html#quarto",
    "href": "slides/02-big-picture.html#quarto",
    "title": "The big picture",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports ‚Äì the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/02-big-picture.html#quarto-1",
    "href": "slides/02-big-picture.html#quarto-1",
    "title": "The big picture",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/02-big-picture.html#how-will-we-use-quarto",
    "href": "slides/02-big-picture.html#how-will-we-use-quarto",
    "title": "The big picture",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou‚Äôll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/02-big-picture.html#what-is-versioning",
    "href": "slides/02-big-picture.html#what-is-versioning",
    "title": "The big picture",
    "section": "What is versioning?",
    "text": "What is versioning?"
  },
  {
    "objectID": "slides/02-big-picture.html#what-is-versioning-1",
    "href": "slides/02-big-picture.html#what-is-versioning-1",
    "title": "The big picture",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages"
  },
  {
    "objectID": "slides/02-big-picture.html#why-do-we-need-version-control",
    "href": "slides/02-big-picture.html#why-do-we-need-version-control",
    "title": "The big picture",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\n\n\n\n\n\nProvides a clear record of how the analysis methods evolved. This makes analysis auditable and thus more trustworthy and reliable. (Ostblom and Timbers 2022)"
  },
  {
    "objectID": "slides/02-big-picture.html#git-and-github",
    "href": "slides/02-big-picture.html#git-and-github",
    "title": "The big picture",
    "section": "git and GitHub",
    "text": "git and GitHub\n\n\ngit is a version control system ‚Äì like ‚ÄúTrack Changes‚Äù features from Microsoft Word.\nGitHub is the home for your git-based projects on the internet (like DropBox but much better).\nThere are a lot of git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull."
  },
  {
    "objectID": "slides/02-big-picture.html#carbohydrates-in-starbucks-food",
    "href": "slides/02-big-picture.html#carbohydrates-in-starbucks-food",
    "title": "The big picture",
    "section": "Carbohydrates in Starbucks food",
    "text": "Carbohydrates in Starbucks food\n\nStarbucks often displays the total calories in their food items but not the other nutritional information.\nCarbohydrates are a body‚Äôs main fuel source. The Dietary Guidelines for America recommend that carbohydrates make up 45% to 65% of total daily calories.1\nOur goal is to understand the relationship between the amount of carbohydrates and calories in Starbucks food items. We‚Äôd also like to assess if the relationship differs based on the type of food item (bakery, salad, sandwich, etc.)\n\nSource: Mayo Clinic"
  },
  {
    "objectID": "slides/02-big-picture.html#starbucks-data",
    "href": "slides/02-big-picture.html#starbucks-data",
    "title": "The big picture",
    "section": "Starbucks data",
    "text": "Starbucks data\n\n\nObservations: 77 Starbucks food items\nVariables:\n\ncarb: Total carbohydrates (in grams)\ncalories: Total calories\nbakery: 1: bakery food item, 0: other food type"
  },
  {
    "objectID": "slides/02-big-picture.html#terminology",
    "href": "slides/02-big-picture.html#terminology",
    "title": "The big picture",
    "section": "Terminology",
    "text": "Terminology\n\ncarb is the response variable\n\nvariable whose variation we want to understand / variable we wish to predict\nalso known as outcome or dependent variable\n\n\n\n\ncalories, bakery are the predictor variables\n\nvariables used to account for variation in the response\nalso known as explanatory, independent, or input variables"
  },
  {
    "objectID": "slides/02-big-picture.html#univariate-exploratory-data-analysis",
    "href": "slides/02-big-picture.html#univariate-exploratory-data-analysis",
    "title": "The big picture",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides/02-big-picture.html#bivariate-exploratory-data-analysis",
    "href": "slides/02-big-picture.html#bivariate-exploratory-data-analysis",
    "title": "The big picture",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/02-big-picture.html#function-between-response-and-predictors",
    "href": "slides/02-big-picture.html#function-between-response-and-predictors",
    "title": "The big picture",
    "section": "Function between response and predictors",
    "text": "Function between response and predictors\n\n\\[\\text{carb} = f(\\text{calories}, \\text{bakery}) + \\epsilon\\]\n\n\nGoal: Determine \\(f\\)\nHow do we determine \\(f\\)?\n\nMake an assumption about the functional form \\(f\\) (parametric model)\nUse the data to fit a model based on that form"
  },
  {
    "objectID": "slides/02-big-picture.html#determine-f",
    "href": "slides/02-big-picture.html#determine-f",
    "title": "The big picture",
    "section": "Determine \\(f\\)",
    "text": "Determine \\(f\\)\n\nChoose the functional form of \\(f\\), i.e., choose the appropriate model given the response variable\n\n\nSuppose \\(f\\) takes the form of a linear model\n\\[y = f(\\mathbf{X}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p + \\epsilon\\]\n\n\n\nUse the data to fit (or train) the model, i.e, estimate the model parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)"
  },
  {
    "objectID": "slides/02-big-picture.html#carb-vs.-calories",
    "href": "slides/02-big-picture.html#carb-vs.-calories",
    "title": "The big picture",
    "section": "Carb vs.¬†Calories",
    "text": "Carb vs.¬†Calories\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\epsilon\\]"
  },
  {
    "objectID": "slides/02-big-picture.html#carb-vs.-calories-bakery",
    "href": "slides/02-big-picture.html#carb-vs.-calories-bakery",
    "title": "The big picture",
    "section": "Carb vs.¬†Calories + Bakery",
    "text": "Carb vs.¬†Calories + Bakery\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\epsilon\\]"
  },
  {
    "objectID": "slides/02-big-picture.html#carb-vs.-calories-bakery-with-interaction",
    "href": "slides/02-big-picture.html#carb-vs.-calories-bakery-with-interaction",
    "title": "The big picture",
    "section": "Carb vs.¬†Calories + Bakery (with interaction)",
    "text": "Carb vs.¬†Calories + Bakery (with interaction)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]"
  },
  {
    "objectID": "slides/02-big-picture.html#statistical-model-vs.-regression-equation",
    "href": "slides/02-big-picture.html#statistical-model-vs.-regression-equation",
    "title": "The big picture",
    "section": "Statistical model vs.¬†regression equation",
    "text": "Statistical model vs.¬†regression equation\nStatistical model (also known as data-generating model)\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]\nModels the process for generating values of the response in the population (function + error)\n\n\nRegression equation\nEstimate of the function using the sample data\n\\[{\\small \\hat{\\text{carb}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~\\text{calories} + \\hat{\\beta}_2 ~\\text{bakery} + \\hat{\\beta}_3 ~ \\text{calories} \\times \\text{bakery}}\\]"
  },
  {
    "objectID": "slides/02-big-picture.html#why-fit-a-model",
    "href": "slides/02-big-picture.html#why-fit-a-model",
    "title": "The big picture",
    "section": "Why fit a model?",
    "text": "Why fit a model?\n\nPrediction: Expected value of the response variable for given values of the predictor variables\nInference: Conclusion about the relationship between the response and predictor variables\n\n\n\nWhat is an example of a prediction question that can be answered using the model of carb vs.¬†calories and bakery?\nWhat is an example of an inference question that can be answered using the model of carb vs.¬†calories and bakery?"
  },
  {
    "objectID": "slides/02-big-picture.html#recap",
    "href": "slides/02-big-picture.html#recap",
    "title": "The big picture",
    "section": "Recap",
    "text": "Recap\n\nReproducibility\n\nIt is best practice conduct all data analysis in a reproducible way\nWe will implement a reproducible workflow using R, Quarto, and git/GitHub\n\n\n\n\nMultivariable relationships\n\nWe can use exploratory data analysis to describe the relationship between two variables\nWe make an assumption about the relationship between variables when doing linear regression\nThe two main objectives for fitting a linear regression model are (1) prediction and (2) inference"
  },
  {
    "objectID": "slides/02-big-picture.html#references",
    "href": "slides/02-big-picture.html#references",
    "title": "The big picture",
    "section": "References",
    "text": "References\n\n\n\nüîó STA 210 - Fall 2023 - Schedule\n\n\n\nAlexander, Rohan. 2023. ‚ÄúTelling Stories with Data,‚Äù June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. ‚ÄúOpinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.‚Äù Journal of Statistics and Data Science Education 30 (3): 241‚Äì50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "slides/09-mlr.html#announcements",
    "href": "slides/09-mlr.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due Mon, Oct 2 at 11:59pm.\nAll lecture recordings available until Wed, Oct 4 at 9am.\n\nClick here for link to videos. You can also find the link in the navigation bar of the course website.\n\nLab groups start this week. You will get your assigned group when you go to lab.\nSubmit questions about SLR by Thu, Sep 28. These questions will be used to make the Exam Review. Click here for more info.\nExam 01: Wed, Oct 4 (in-class + take-home)\nExam 01 review: Mon, Oct 2"
  },
  {
    "objectID": "slides/09-mlr.html#computational-setup",
    "href": "slides/09-mlr.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/09-mlr.html#data-peer-to-peer-lender",
    "href": "slides/09-mlr.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday‚Äôs data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 √ó 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ‚Ñπ 40 more rows"
  },
  {
    "objectID": "slides/09-mlr.html#variables",
    "href": "slides/09-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/09-mlr.html#outcome-interest_rate",
    "href": "slides/09-mlr.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/09-mlr.html#predictors",
    "href": "slides/09-mlr.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-1-rescale-income",
    "href": "slides/09-mlr.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/09-mlr.html#outcome-vs.-predictors",
    "href": "slides/09-mlr.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/09-mlr.html#single-vs.-multiple-predictors",
    "href": "slides/09-mlr.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs.¬†multiple predictors",
    "text": "Single vs.¬†multiple predictors\nSo far we‚Äôve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/09-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/09-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{interest_rate}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{debt_to_income} \\\\ & + \\hat{\\beta}_2 \\text{verified_income} \\\\ &+ \\hat{\\beta}_3 \\text{annual_income_th}\n\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values interest_rate follow a Normal distribution."
  },
  {
    "objectID": "slides/09-mlr.html#multiple-linear-regression-1",
    "href": "slides/09-mlr.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/09-mlr.html#multiple-linear-regression-2",
    "href": "slides/09-mlr.html#multiple-linear-regression-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/09-mlr.html#model-fit",
    "href": "slides/09-mlr.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078"
  },
  {
    "objectID": "slides/09-mlr.html#model-equation",
    "href": "slides/09-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  10.726 &+0.671 \\times \\text{debt_to_income}\\\\\n&+ 2.211 \\times \\text{source_verified}\\\\  \n&+ 6.880 \\times \\text{verified}\\\\\n& -0.021 \\times \\text{annual_income_th}\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\nWe will talk about why there are two terms in the model for verified_income shortly!"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-hatbeta_j",
    "href": "slides/09-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for debt_to_income is 0.671. This means for each point in an borrower‚Äôs debt to income ratio, the interest rate on the loan is expected to be greater by 0.671%, holding annual income and income verification constant."
  },
  {
    "objectID": "slides/09-mlr.html#prediction",
    "href": "slides/09-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted interest rate for an borrower with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000?\n\n\n\n10.726 + 0.671 * 0.558 + 2.211 * 0 + 6.880 * 0 - 0.021 * 59\n\n[1] 9.861418\n\n\n\nThe predicted interest rate for an borrower with with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000 is 9.86%."
  },
  {
    "objectID": "slides/09-mlr.html#prediction-revisited",
    "href": "slides/09-mlr.html#prediction-revisited",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisited",
    "text": "Prediction, revisited\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_borrower &lt;- tibble(\n  debt_to_income  = 0.558, \n  verified_income = \"Not Verified\", \n  annual_income_th = 59\n)\n\npredict(int_fit, new_borrower)\n\n# A tibble: 1 √ó 1\n  .pred\n  &lt;dbl&gt;\n1  9.89\n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/09-mlr.html#confidence-interval-for-hatmu_y",
    "href": "slides/09-mlr.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 90% confidence interval for the estimated mean interest rate for borrowers with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, type = \"conf_int\", level = 0.90)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        8.16        11.6"
  },
  {
    "objectID": "slides/09-mlr.html#prediction-interval-for-haty",
    "href": "slides/09-mlr.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 90% confidence interval for the predicted interest rate for an individual appllicant with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, type = \"pred_int\", level = 0.90)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        2.18        17.6"
  },
  {
    "objectID": "slides/09-mlr.html#cautions",
    "href": "slides/09-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-results",
    "href": "slides/09-mlr.html#interpreting-results",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/09-mlr.html#mean-centering",
    "href": "slides/09-mlr.html#mean-centering",
    "title": "Multiple linear regression (MLR)",
    "section": "Mean-centering",
    "text": "Mean-centering\nIf we are interested in interpreting the intercept, we can mean-center the quantitative predictors in the model.\nWe can mean-center a quantitative predictor \\(X_j\\) using the following:\n\\[X_{j_{Cent}} = X_{j}- \\bar{X}_{j}\\]\n\nIf we mean-center all quantitative variables, then the intercept is interpreted as the expected value of the response variable when all quantitative variables are at their mean value."
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/09-mlr.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/09-mlr.html#visualize-mean-centered-predictors",
    "href": "slides/09-mlr.html#visualize-mean-centered-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualize mean-centered predictors",
    "text": "Visualize mean-centered predictors"
  },
  {
    "objectID": "slides/09-mlr.html#using-mean-centered-variables-in-the-model",
    "href": "slides/09-mlr.html#using-mean-centered-variables-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Using mean-centered variables in the model",
    "text": "Using mean-centered variables in the model\n\nHow do you expect the model to change if we use the debt_inc_cent and annual_income_cent in the model?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002"
  },
  {
    "objectID": "slides/09-mlr.html#original-vs.-mean-centered-model",
    "href": "slides/09-mlr.html#original-vs.-mean-centered-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Original vs.¬†mean-centered model",
    "text": "Original vs.¬†mean-centered model\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n10.726\n\n\ndebt_to_income\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th\n-0.021\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n9.444\n\n\ndebt_inc_cent\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th_cent\n-0.021"
  },
  {
    "objectID": "slides/09-mlr.html#indicator-variables-1",
    "href": "slides/09-mlr.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/09-mlr.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 √ó 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/09-mlr.html#indicators-in-the-model",
    "href": "slides/09-mlr.html#indicators-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn‚Äôt have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 √ó 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-verified_income",
    "href": "slides/09-mlr.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nThe baseline category is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n\n\n\nInterpret the coefficient of Verified in the context of the data."
  },
  {
    "objectID": "slides/09-mlr.html#interaction-terms-1",
    "href": "slides/09-mlr.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/09-mlr.html#interest-rate-vs.-annual-income",
    "href": "slides/09-mlr.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs.¬†annual income",
    "text": "Interest rate vs.¬†annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/09-mlr.html#interaction-term-in-model",
    "href": "slides/09-mlr.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent + verified_income + annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-interaction-terms",
    "href": "slides/09-mlr.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/09-mlr.html#data-manipulation-4-create-interaction-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, ‚Ä¶\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.‚Ä¶\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, ‚Ä¶\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, ‚Ä¶\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, ‚Ä¶\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0‚Ä¶\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11‚Ä¶"
  },
  {
    "objectID": "slides/09-mlr.html#recap",
    "href": "slides/09-mlr.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted coefficients in the multiple linear regression model\nCalculated predictions and associated intervals for multiple linear regression models\nMean-centered quantitative predictors\nUsed indicator variables for categorical predictors\nUsed interaction terms"
  },
  {
    "objectID": "slides/09-mlr.html#looking-backward",
    "href": "slides/09-mlr.html#looking-backward",
    "title": "Multiple linear regression (MLR)",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):\n\nloan50 |&gt;\n  select(interest_rate, annual_income, debt_to_income, verified_income) |&gt;\n  mutate(\n    # 1. rescale income\n    annual_income_th = annual_income / 1000,\n    # 2. mean-center quantitative predictors\n    debt_inc_cent = debt_to_income - mean(debt_to_income),\n    annual_income_th_cent = annual_income_th - mean(annual_income_th),\n    # 3. create dummy variables for verified_income\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0),\n    # 4. create interaction variables\n    `annual_income_th_cent:verified_incomeSource Verified` = annual_income_th_cent * source_verified,\n    `annual_income_th_cent:verified_incomeVerified` = annual_income_th_cent * verified\n  )"
  },
  {
    "objectID": "slides/09-mlr.html#looking-forward-after-exam-01",
    "href": "slides/09-mlr.html#looking-forward-after-exam-01",
    "title": "Multiple linear regression (MLR)",
    "section": "Looking forward (after Exam 01)",
    "text": "Looking forward (after Exam 01)\nFeature engineering, with recipes (from tidymodels):\n\nloan_rec &lt;- recipe( ~ ., data = loan50) |&gt;\n  # 1. rescale income\n  step_mutate(annual_income_th = annual_income / 1000) |&gt;\n  # 2. mean-center quantitative predictors\n  step_center(all_numeric_predictors()) |&gt;\n  # 3. create dummy variables for verified_income\n  step_dummy(verified_income) |&gt;\n  # 4. create interaction variables\n  step_interact(terms = ~ annual_income_th:verified_income)"
  },
  {
    "objectID": "slides/09-mlr.html#recipe",
    "href": "slides/09-mlr.html#recipe",
    "title": "Multiple linear regression (MLR)",
    "section": "Recipe",
    "text": "Recipe\n\nloan_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 24\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: annual_income / 1000\n\n\n‚Ä¢ Centering for: all_numeric_predictors()\n\n\n‚Ä¢ Dummy variables from: verified_income\n\n\n‚Ä¢ Interactions with: annual_income_th:verified_income\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/06-slr-math-models.html#announcements",
    "href": "slides/06-slr-math-models.html#announcements",
    "title": "SLR: Mathematical models for inference",
    "section": "Announcements",
    "text": "Announcements\n\nHW 01: due Wed, Sep 20 at 11:59pm\n\nQuestions?\n\nStatistics experience - due Mon, Nov 20 at 11:59pm"
  },
  {
    "objectID": "slides/06-slr-math-models.html#course-policy-reminders",
    "href": "slides/06-slr-math-models.html#course-policy-reminders",
    "title": "SLR: Mathematical models for inference",
    "section": "Course policy reminders",
    "text": "Course policy reminders\n\nLate work\n\nHW and labs accepted up to 2 days late.\n5% deduction for each 24-hour period the assignment is late.\n\nOne time late waiver\n\nCan use on HW and individual labs\n\nLowest HW and lowest lab grade dropped at the end of the semester."
  },
  {
    "objectID": "slides/06-slr-math-models.html#course-policy-reminders-1",
    "href": "slides/06-slr-math-models.html#course-policy-reminders-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Course policy reminders",
    "text": "Course policy reminders\n\nRegrade requests\n\nOpened 1 day after assignment is returned and due within 1 week\nOnly submit regrade if there is an error in the grading not to dispute points\nProf.¬†Tackett or Sam the Head TA will regrade the entire exercise being disputed, which could result in a lower grade\n\n\n\n\n\n\n\nTip\n\n\nSee the posted solutions and ask during office hours before submitting a regrade request"
  },
  {
    "objectID": "slides/06-slr-math-models.html#statistician-of-the-day-regina-nuzzo",
    "href": "slides/06-slr-math-models.html#statistician-of-the-day-regina-nuzzo",
    "title": "SLR: Mathematical models for inference",
    "section": "Statistician of the day: Regina Nuzzo",
    "text": "Statistician of the day: Regina Nuzzo\n\n\n\n\n\n\n\n\nDr.¬†Nuzzo received her PhD in Statistics from Stanford University and is now Professor of Science, Technology, & Mathematics at Gallaudet University. Dr.¬†Nuzzo teaches statistics using American Sign Language.\nHer work has appeared in Nature, Los Angeles Times, New York Times, Reader‚Äôs Digest, New Scientist, and Scientific American. Primarily, she works to help lay-audiences understand science and statistics in particular. She earned the American Statistical Association‚Äôs 2014 Excellence in Statistical Reporting Award for her article on p-values in Nature. Her work led to the ASA‚Äôs statement on p-values.\nSource: hardin47.github.io/CURV/scholars/nuzzo"
  },
  {
    "objectID": "slides/06-slr-math-models.html#regina-nuzzo",
    "href": "slides/06-slr-math-models.html#regina-nuzzo",
    "title": "SLR: Mathematical models for inference",
    "section": "Regina Nuzzo",
    "text": "Regina Nuzzo\nFrom ‚ÄúTips for Communicating Statistical Significance.‚Äù\n‚ÄúThat we use p-values to alert us to surprising data results, not to give a final answer on anything. (Or at least that‚Äôs what we should be doing). And that results can get flagged as‚Äùstatistically surprising‚Äù with a small p-value for a number of reasons‚Äù\n\nThere was a fluke.\nSomething was violated.\nThere was a real but tiny relationship.\nThere was a relationship that is worth more study.\n\nOr any combination of the above."
  },
  {
    "objectID": "slides/06-slr-math-models.html#topics",
    "href": "slides/06-slr-math-models.html#topics",
    "title": "SLR: Mathematical models for inference",
    "section": "Topics",
    "text": "Topics\n\nDefine mathematical models to conduct inference for the slope\nUse mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope"
  },
  {
    "objectID": "slides/06-slr-math-models.html#computational-setup",
    "href": "slides/06-slr-math-models.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/06-slr-math-models.html#the-regression-model-revisited",
    "href": "slides/06-slr-math-models.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/06-slr-math-models.html#inference-revisited",
    "href": "slides/06-slr-math-models.html#inference-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we‚Äôll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/06-slr-math-models.html#mathematical-representation-of-the-model",
    "href": "slides/06-slr-math-models.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn‚Äôt tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/06-slr-math-models.html#mathematical-representation-visualized",
    "href": "slides/06-slr-math-models.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#regression-standard-error",
    "href": "slides/06-slr-math-models.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error, the average distance between the observed values and the regression line\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#standard-error-of-hatbeta_1",
    "href": "slides/06-slr-math-models.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\nThe standard error of \\(\\hat{\\beta}_1\\) quantifies the sampling variability in the estimated slopes\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-for-the-slope",
    "href": "slides/06-slr-math-models.html#hypothesis-test-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test for the slope",
    "text": "Hypothesis test for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-test-statistic",
    "href": "slides/06-slr-math-models.html#hypothesis-test-test-statistic",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Test statistic",
    "text": "Hypothesis test: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{159.48 - 0}{18.17} = 8.78\n\\]\n\nSelect the best interpretation of the test statistic on Ed Discussion.\n[10:05am lecture] [1:25pm lecture]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-p-value",
    "href": "slides/06-slr-math-models.html#hypothesis-test-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-p-value-1",
    "href": "slides/06-slr-math-models.html#hypothesis-test-p-value-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\nA more exact p-value\n\n2 * pt(q = 8.78, df = 96, lower.tail = FALSE)\n\n[1] 6.19602e-14\n\n\n\nSelect the meaning of the p-value on Ed Discussion.\n[10:05am lecture][1:25pm lecture]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#understanding-the-p-value",
    "href": "slides/06-slr-math-models.html#understanding-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "href": "slides/06-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest."
  },
  {
    "objectID": "slides/06-slr-math-models.html#confidence-interval-for-the-slope",
    "href": "slides/06-slr-math-models.html#confidence-interval-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/06-slr-math-models.html#confidence-interval-critical-value",
    "href": "slides/06-slr-math-models.html#confidence-interval-critical-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(duke_forest) - 2)\n\n[1] 1.984984\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(duke_forest) - 2)\n\n[1] 1.660881\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(duke_forest) - 2)\n\n[1] 2.628016"
  },
  {
    "objectID": "slides/06-slr-math-models.html#ci-for-the-slope-calculation",
    "href": "slides/06-slr-math-models.html#ci-for-the-slope-calculation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 159.48 \\hspace{15mm} t^* = 1.98 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 18.17\\]\n\n\\[\n159.48 \\pm 1.98 \\times 18.17 = (123.50, 195.46)\n\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#ci-for-the-slope-computation",
    "href": "slides/06-slr-math-models.html#ci-for-the-slope-computation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(df_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n10847.77\n222456.88\n\n\narea\n159.48\n18.17\n8.78\n0.00\n123.41\n195.55"
  },
  {
    "objectID": "slides/06-slr-math-models.html#intervals-for-predictions-1",
    "href": "slides/06-slr-math-models.html#intervals-for-predictions-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nSuppose we want to answer the question ‚ÄúWhat is the predicted sale price of a Duke Forest house that is 2,800 square feet?‚Äù\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/06-slr-math-models.html#two-types-of-predictions",
    "href": "slides/06-slr-math-models.html#two-types-of-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: ‚ÄúWhat is the average predicted sale price of Duke Forest houses that are 2,800 square feet?‚Äù\nPrediction for an individual observation: ‚ÄúWhat is the predicted sale price of a Duke Forest house that is 2,800 square feet?‚Äù\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#uncertainty-in-predictions",
    "href": "slides/06-slr-math-models.html#uncertainty-in-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#standard-errors",
    "href": "slides/06-slr-math-models.html#standard-errors",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#standard-errors-1",
    "href": "slides/06-slr-math-models.html#standard-errors-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#confidence-interval",
    "href": "slides/06-slr-math-models.html#confidence-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_house &lt;- tibble(area = 2800)\n\npredict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95) |&gt;\n  kable()\n\n\n\n\n.pred_lower\n.pred_upper\n\n\n\n\n529351\n597060.1\n\n\n\n\n\n\n\n\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between $529,351 and $597,060."
  },
  {
    "objectID": "slides/06-slr-math-models.html#prediction-interval",
    "href": "slides/06-slr-math-models.html#prediction-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95) |&gt;\n  kable()\n\n\n\n\n.pred_lower\n.pred_upper\n\n\n\n\n226438.3\n899972.7\n\n\n\n\n\n\n\n\nWe are 95% confident that predicted sale price of a Duke Forest house that is 2,800 square feet is between $226,438 and $899,973."
  },
  {
    "objectID": "slides/06-slr-math-models.html#comparing-intervals",
    "href": "slides/06-slr-math-models.html#comparing-intervals",
    "title": "SLR: Mathematical models for inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/06-slr-math-models.html#extrapolation",
    "href": "slides/06-slr-math-models.html#extrapolation",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the sale price of a ‚Äútiny house‚Äù in Duke Forest that is 225 square feet.\n\n\n\n\n\n\n\n\n\n\n\nNo, thanks!"
  },
  {
    "objectID": "slides/06-slr-math-models.html#extrapolation-1",
    "href": "slides/06-slr-math-models.html#extrapolation-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-05.html#team-workflow",
    "href": "slides/lab-05.html#team-workflow",
    "title": "Lab 05",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are no markers in today‚Äôs lab, but you should use a similar workflow as Lab 04.\n\nEvery team member should still be engaged in discussion for all questions, even if it‚Äôs not your turn type.\n\nDon‚Äôt forget to pull to get your teammates‚Äô updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Carefully read the submission instructions at the end of the lab instructions."
  },
  {
    "objectID": "slides/lab-05.html#tips-for-working-on-a-team",
    "href": "slides/lab-05.html#tips-for-working-on-a-team",
    "title": "Lab 05",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other."
  },
  {
    "objectID": "slides/lab-05.html#sta-210-mid-semester-survey",
    "href": "slides/lab-05.html#sta-210-mid-semester-survey",
    "title": "Lab 05",
    "section": "STA 210 Mid Semester Survey",
    "text": "STA 210 Mid Semester Survey\nPlease take a few minutes to complete the STA 210 Mid Semester Survey.\n\nClick here to access the survey."
  },
  {
    "objectID": "slides/lab-05.html#resources-for-lab-04",
    "href": "slides/lab-05.html#resources-for-lab-04",
    "title": "Lab 05",
    "section": "Resources for Lab 04",
    "text": "Resources for Lab 04\n\nLecture notes:\n\nFeature engineering\nFeature engineering: Model workflow\n\nRecipes package function reference\nTidy Modeling in R - Chapter 8: Feature engineering with recipes\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-00.html#meet-your-ta",
    "href": "slides/lab-00.html#meet-your-ta",
    "title": "Welcome to STA 210 Labs!",
    "section": "Meet your TA!",
    "text": "Meet your TA!"
  },
  {
    "objectID": "slides/lab-00.html#meet-each-other",
    "href": "slides/lab-00.html#meet-each-other",
    "title": "Welcome to STA 210 Labs!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\nGet into groups of 4 - 5.\nIntroduce yourself - Name, year, major\nChoose a reporter\n\nNeed help choosing? Person with birthday closest to today‚Äôs date.\n\nIdentify 8 things everyone in the group has in common\n\nNot being a Duke student\nNot clothes (e.g., we‚Äôre all wearing socks)\nNot body parts (e.g., we all have a nose)\n\nReporter will share list with the class.\n\n\n\n\n\n‚àí+\n06:00"
  },
  {
    "objectID": "slides/lab-00.html#what-to-expect-in-lab",
    "href": "slides/lab-00.html#what-to-expect-in-lab",
    "title": "Welcome to STA 210 Labs!",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nWork on the lab assignment (individual for the first few weeks, but in teams for the rest of the semester)\nLab instructions will be posted on the course website\nStart each lab by finding your assignment repo in the course GitHub organization\n\nMore on the computing tools during lecture"
  },
  {
    "objectID": "slides/lab-00.html#todays-lab",
    "href": "slides/lab-00.html#todays-lab",
    "title": "Welcome to STA 210 Labs!",
    "section": "Today‚Äôs lab",
    "text": "Today‚Äôs lab\nThe rest of the today‚Äôs lab is focused on setting up the computing for the course and completing the class survey. Click the link below for the Lab 00 instructions.\nüîó sta210-fa23.netlify.app/labs/lab-00.html\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#statistics-experience-assignment",
    "href": "slides/04-slr-bootstrap.html#statistics-experience-assignment",
    "title": "SLR: Simulation-based inference",
    "section": "Statistics experience assignment",
    "text": "Statistics experience assignment\nGoal: Engage with statistics / data science outside the classroom and connect your experience with what you‚Äôre learning in the course.\nWhat: Have a statistics experience + create a slide reflecting on the experience. Counts as a homework grade.\nWhen: Must do the activity this semester. Reflection due Monday, November 20 at 11:59pm\nFor more info: sta210-fa23.netlify.app/stats-experience"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#statistician-of-the-day",
    "href": "slides/04-slr-bootstrap.html#statistician-of-the-day",
    "title": "SLR: Simulation-based inference",
    "section": "Statistician of the day",
    "text": "Statistician of the day\nWhat: Each week, we will highlight a statisticians, data scientists, or other scholars from groups who have been historically marginalized in the field and whose work has made a significant impact.\nGoal: Learn about scholars you may not see in traditional textbooks and see the breadth of past and current work in the field.\nWho: Scholars are from the CURV (connecting, uplifting, and recognizing voices) database started by Dr.¬†Jo Hardin at Pomona College\nParticipate: Present a Statistician of the Day or contribute to the CURV data base as your statistics experience"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#w.e.b.-du-bois",
    "href": "slides/04-slr-bootstrap.html#w.e.b.-du-bois",
    "title": "SLR: Simulation-based inference",
    "section": "W.E.B. Du Bois",
    "text": "W.E.B. Du Bois\n\n\n\n\nDu Bois (1868 - 1963) was a sociologist who contributed to the field of data visualization through infographics related to the African American in the early twentieth century.\nIn 1900 Du Bois contributed approximately 60 data visualizations to an exhibit at the Exposition Universelle in Paris, an exhibit designed to illustrate the progress made by African Americans since the end of slavery (only 37 years prior, in 1863).\n\nSource: hardin47.github.io/CURV/scholars/dubois.html"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#w.e.b.-du-bois-1",
    "href": "slides/04-slr-bootstrap.html#w.e.b.-du-bois-1",
    "title": "SLR: Simulation-based inference",
    "section": "W.E.B. Du Bois",
    "text": "W.E.B. Du Bois\n\n\nThe set of visualizations demonstrate how powerfully a picture can tell 1000 words, as the information Du Bois used was primarily available from public records (e.g., census and other government reports).\nSource: hardin47.github.io/CURV/scholars/dubois.html\n\n\n\n\nVisualization by W.E.B. Du Bois"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#ae-03-follow-up-1",
    "href": "slides/04-slr-bootstrap.html#ae-03-follow-up-1",
    "title": "SLR: Simulation-based inference",
    "section": "AE 03 Follow-up",
    "text": "AE 03 Follow-up\nGoal: Use simple linear regression to model the relationship between temperature and daily bike rentals in the winter season"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#ae-03-follow-up-2",
    "href": "slides/04-slr-bootstrap.html#ae-03-follow-up-2",
    "title": "SLR: Simulation-based inference",
    "section": "AE 03 Follow-up",
    "text": "AE 03 Follow-up\nStatistical Model:\n\\[count = \\beta_0 +\\beta_1 ~ temp\\_orig + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n\n\nwinter_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(count ~ temp_orig, data = winter)\n\ntidy(winter_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.038\n238.312\n-0.466\n0.642\n\n\ntemp_orig\n222.416\n18.459\n12.049\n0.000"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#ae-03-follow-up-3",
    "href": "slides/04-slr-bootstrap.html#ae-03-follow-up-3",
    "title": "SLR: Simulation-based inference",
    "section": "AE 03 Follow-up",
    "text": "AE 03 Follow-up\nUse the output to write out the estimated regression equation.\n\\[\n\\hat{count} =  -111.038 + 222.416 ~temp\\_orig\n\\]\n\n\n\n\n\n\n\nYour turn!\n\n\n\nInterpret the slope in the context of the data.\nWhy is there no error term in the regression equation?\n\n\n\n\n\nLaTex:\n\\$\\$\\hat{count} = -111.038 + 222.416 ~ temp\\_orig\\$\\$"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#topics",
    "href": "slides/04-slr-bootstrap.html#topics",
    "title": "SLR: Simulation-based inference",
    "section": "Topics",
    "text": "Topics\n\nFind range of plausible values for the slope using bootstrap confidence intervals"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#computational-setup",
    "href": "slides/04-slr-bootstrap.html#computational-setup",
    "title": "SLR: Simulation-based inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#data-houses-in-duke-forest",
    "href": "slides/04-slr-bootstrap.html#data-houses-in-duke-forest",
    "title": "SLR: Simulation-based inference",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#exploratory-data-analysis",
    "href": "slides/04-slr-bootstrap.html#exploratory-data-analysis",
    "title": "SLR: Simulation-based inference",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nCode\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#modeling",
    "href": "slides/04-slr-bootstrap.html#modeling",
    "title": "SLR: Simulation-based inference",
    "section": "Modeling",
    "text": "Modeling\n\ndf_fit &lt;- linear_reg() |&gt;\n  #set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2) #neatly format table to 2 digits\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, for $116,652, on average.\n\nIs this interpretation useful?\n\nSlope: For each additional square foot, we expect the sale price of Duke Forest houses to be higher by $159, on average."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#from-sample-to-population",
    "href": "slides/04-slr-bootstrap.html#from-sample-to-population",
    "title": "SLR: Simulation-based inference",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor each additional square foot, we expect the sale price of Duke Forest houses to be higher by $159, on average.\n\n\n\nThis estimate is valid for the single sample of 98 houses.\nBut what if we‚Äôre not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all houses in Duke Forest?"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#statistical-inference",
    "href": "slides/04-slr-bootstrap.html#statistical-inference",
    "title": "SLR: Simulation-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference provide methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we‚Äôre interested in"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#inference-for-simple-linear-regression",
    "href": "slides/04-slr-bootstrap.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation-based inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope,\\(\\beta_1\\)"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#confidence-interval",
    "href": "slides/04-slr-bootstrap.html#confidence-interval",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#confidence-interval-for-the-slope-1",
    "href": "slides/04-slr-bootstrap.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like ‚ÄúFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus X dollars.‚Äù\n\n\nShould X be $10? $100? $1000?\nIf we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by $10? $100? $1000?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#quantify-the-variability-of-the-slope",
    "href": "slides/04-slr-bootstrap.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\n\nTwo approaches:\n\nVia simulation (what we‚Äôll do today)\nVia mathematical models (what we‚Äôll do in the next class)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-sample-1",
    "href": "slides/04-slr-bootstrap.html#bootstrap-sample-1",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-sample-2",
    "href": "slides/04-slr-bootstrap.html#bootstrap-sample-2",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-sample-3",
    "href": "slides/04-slr-bootstrap.html#bootstrap-sample-3",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-sample-4",
    "href": "slides/04-slr-bootstrap.html#bootstrap-sample-4",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-sample-5",
    "href": "slides/04-slr-bootstrap.html#bootstrap-sample-5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth‚Ä¶"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-samples-1---5",
    "href": "slides/04-slr-bootstrap.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#bootstrap-samples-1---100",
    "href": "slides/04-slr-bootstrap.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#slopes-of-bootstrap-samples",
    "href": "slides/04-slr-bootstrap.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus ___ dollars."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#slopes-of-bootstrap-samples-1",
    "href": "slides/04-slr-bootstrap.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, we expect the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus ___ dollars."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#confidence-level",
    "href": "slides/04-slr-bootstrap.html#confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between $0 and $250? How about $150 and $170? How about $90 and $210?"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#confidence-interval-1",
    "href": "slides/04-slr-bootstrap.html#confidence-interval-1",
    "title": "SLR: Simulation-based inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#computing-the-ci-for-the-slope-i",
    "href": "slides/04-slr-bootstrap.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/04-slr-bootstrap.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\nset.seed(1120)\n\nboot_fits &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n# A tibble: 200 √ó 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# ‚Ñπ 190 more rows"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/04-slr-bootstrap.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\" #default method\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#precision-vs.-accuracy",
    "href": "slides/04-slr-bootstrap.html#precision-vs.-accuracy",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs.¬†accuracy",
    "text": "Precision vs.¬†accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#precision-vs.-accuracy-1",
    "href": "slides/04-slr-bootstrap.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs.¬†accuracy",
    "text": "Precision vs.¬†accuracy\n\nHow can we get best of both worlds ‚Äì high precision and high accuracy?"
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#changing-confidence-level",
    "href": "slides/04-slr-bootstrap.html#changing-confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#changing-confidence-level-1",
    "href": "slides/04-slr-bootstrap.html#changing-confidence-level-1",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          104.     212.\n2 intercept  -24380.  256730.\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          56.3     226.\n2 intercept -61950.   370395."
  },
  {
    "objectID": "slides/04-slr-bootstrap.html#recap",
    "href": "slides/04-slr-bootstrap.html#recap",
    "title": "SLR: Simulation-based inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can‚Äôt continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-03.html#reminders",
    "href": "slides/lab-03.html#reminders",
    "title": "Lab 03",
    "section": "Reminders",
    "text": "Reminders\nMake sure to do the following as you complete the assignment:\n\nWrite all narrative in complete sentences.\nUse informative axis titles and labels on all graphs.\nImplement version control in your reproducible workflow.\n\nThroughout the assignment periodically render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\nBenchmark: Push changes to GitHub at least three times as you work on the assignment."
  },
  {
    "objectID": "slides/lab-03.html#todays-lab",
    "href": "slides/lab-03.html#todays-lab",
    "title": "Lab 03",
    "section": "Today‚Äôs lab",
    "text": "Today‚Äôs lab\n\nFocused on mathematical inference for simple linear regression and checking model conditions.\n\nWe will talk about model conditions in the September 20 lecture.\n\nRemember to mark all pages in your Gradescope submission. The first page should be marked for the ‚ÄúWorkflow & formatting‚Äù section.\nUse lectures and application exercises as reference as you complete the lab.\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 210: Regression Analysis",
    "section": "",
    "text": "Learn approaches for analyzing multivariate data sets, emphasizing analysis of variance, linear regression, and logistic regression. Learn techniques for checking the appropriateness of proposed models, such as residual analyses and case influence diagnostics, and techniques for selecting models. Gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory."
  },
  {
    "objectID": "index.html#teaching-assistants",
    "href": "index.html#teaching-assistants",
    "title": "STA 210: Regression Analysis",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\n\n\nClick here for schedule of office hours.\n\n\n\n\n\n\nName\nRole\n\n\n\n\n\nSam Rosen\nHead TA\nLab 01: Tue 10:05 - 11:20am\n\n\nBethany Astor\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nJon Campbell\nTA\n\n\n\nDonald Cayton\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nAllison Li\nTA\n\n\n\nMitchelle Mojekwu\nTA\nLab 04: Thu 11:45am - 1pm\n\n\nBen Thorpe\nTA\n\n\n\nLinxuan Wang\nTA\nLab 03: Thu 10:05 - 11:20am\n\n\nXiaojun Zheng\nTA\nLab 04: Thu 11:45am - 1pm"
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge"
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge"
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\nTutorials most relevant to this course:\n\nThe Basics\nWork with Data\nVisualize Data\nTidy your Data\n\nClick here for full list of tutorials."
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences."
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help."
  },
  {
    "objectID": "support.html#lectures-and-labs",
    "href": "support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "support.html#ed-discussion",
    "href": "support.html#ed-discussion",
    "title": "Course support",
    "section": "Ed Discussion",
    "text": "Ed Discussion\nOutside of class and office hours, any general questions about course content or assignments should be posted on Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g.¬†illness, accommodations, etc.), you may email Professor Tackett at maria.tackett@duke.edu. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nIf you have questions about assignment extensions, please email sta210@duke.edu. This email is accessible to the Head TA Sam Rosen and Professor Tackett."
  },
  {
    "objectID": "support.html#academic-support",
    "href": "support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "support.html#mental-health-and-wellness",
    "href": "support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. Go to studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS): CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000 or students.duke.edu/wellness/caps\nTimelyCare (formerly known as Blue Devils Care): An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu"
  },
  {
    "objectID": "support.html#technology-accommodations",
    "href": "support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nHighly aided students who have limited access to computers may request loaner laptops through the DukeLIFE Technology Assistance Program. Please note that supplies are limited.\nNote that we will be using Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "support.html#course-materials-costs",
    "href": "support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "support.html#assistance-with-zoom-or-sakai",
    "href": "support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "ae/ae-01-movies.html",
    "href": "ae/ae-01-movies.html",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "",
    "text": "Important\n\n\n\nFor this AE, you will discuss the questions in groups and submit answers on Ed Discussion. This AE does not count towards the Application Exercise grade.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(DT)"
  },
  {
    "objectID": "ae/ae-01-movies.html#data",
    "href": "ae/ae-01-movies.html#data",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\")\n\nView the first 10 rows of data.\n\nmovies |&gt;\n  slice(1:10)\n\n# A tibble: 10 √ó 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S‚Ä¶ R      Drama  1980 June 13‚Ä¶   8.4 9.27e5 Stanley‚Ä¶ Steph‚Ä¶ Jack‚Ä¶ United‚Ä¶\n 2 The B‚Ä¶ R      Adve‚Ä¶  1980 July 2,‚Ä¶   5.8 6.5 e4 Randal ‚Ä¶ Henry‚Ä¶ Broo‚Ä¶ United‚Ä¶\n 3 Star ‚Ä¶ PG     Acti‚Ä¶  1980 June 20‚Ä¶   8.7 1.20e6 Irvin K‚Ä¶ Leigh‚Ä¶ Mark‚Ä¶ United‚Ä¶\n 4 Airpl‚Ä¶ PG     Come‚Ä¶  1980 July 2,‚Ä¶   7.7 2.21e5 Jim Abr‚Ä¶ Jim A‚Ä¶ Robe‚Ä¶ United‚Ä¶\n 5 Caddy‚Ä¶ R      Come‚Ä¶  1980 July 25‚Ä¶   7.3 1.08e5 Harold ‚Ä¶ Brian‚Ä¶ Chev‚Ä¶ United‚Ä¶\n 6 Frida‚Ä¶ R      Horr‚Ä¶  1980 May 9, ‚Ä¶   6.4 1.23e5 Sean S.‚Ä¶ Victo‚Ä¶ Bets‚Ä¶ United‚Ä¶\n 7 The B‚Ä¶ R      Acti‚Ä¶  1980 June 20‚Ä¶   7.9 1.88e5 John La‚Ä¶ Dan A‚Ä¶ John‚Ä¶ United‚Ä¶\n 8 Ragin‚Ä¶ R      Biog‚Ä¶  1980 Decembe‚Ä¶   8.2 3.30e5 Martin ‚Ä¶ Jake ‚Ä¶ Robe‚Ä¶ United‚Ä¶\n 9 Super‚Ä¶ PG     Acti‚Ä¶  1980 June 19‚Ä¶   6.8 1.01e5 Richard‚Ä¶ Jerry‚Ä¶ Gene‚Ä¶ United‚Ä¶\n10 The L‚Ä¶ R      Biog‚Ä¶  1980 May 16,‚Ä¶   7   1   e4 Walter ‚Ä¶ Bill ‚Ä¶ Davi‚Ä¶ United‚Ä¶\n# ‚Ñπ 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;, company &lt;chr&gt;, runtime &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-01-movies.html#analysis",
    "href": "ae/ae-01-movies.html#analysis",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Analysis",
    "text": "Analysis\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\n\nmovies |&gt;\n  filter(genre %in% genre_list) |&gt; \n  group_by(genre,year) |&gt;\n  summarise(avg_gross = mean(gross)) |&gt;\n  ggplot(mapping = aes(x = year, y = avg_gross, color=genre)) +\n    geom_point() + \n    geom_line() +\n    ylab(\"Average Gross Revenue (in US Dollars)\") +\n    ggtitle(\"Gross Revenue Over Time\") +\n    scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\nWhat do you observe from the plot?\n\n\n\nNext, let‚Äôs see the relationship between a movie‚Äôs budget and its gross revenue.\n\nmovies |&gt;\n  filter(genre %in% genre_list, budget &gt; 0) |&gt; \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point() +\n  geom_smooth(method=\"lm\",se=FALSE) + \n  xlab(\"Log-transformed Budget\")+\n  ylab(\"Log-transformed Gross Revenue\") +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d()"
  },
  {
    "objectID": "ae/ae-01-movies.html#exercises",
    "href": "ae/ae-01-movies.html#exercises",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nPost your response on ED Discussion.\n\nSection 001 (10:05am lecture)\nSection 002 (1:25pm lecture)\n\n[Time permitting] Discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)? Post your group‚Äôs response in the AE 01 Movie Budgets comments on Ed Discussion."
  },
  {
    "objectID": "ae/ae-01-movies.html#references",
    "href": "ae/ae-01-movies.html#references",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "References",
    "text": "References\n\ngithub.com/danielgrijalva/movie-stats\nInternet Movie Database"
  },
  {
    "objectID": "ae/ae-01-movies.html#appendix",
    "href": "ae/ae-01-movies.html#appendix",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies |&gt; \n  arrange(genre) |&gt; \n  select(genre) |&gt;\n  distinct() |&gt;\n  datatable()"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html",
    "href": "ae/ae-12-logistic-intro.html",
    "title": "AE 12: Logistic regression introduction",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-12 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, November 2 at 11:59pm."
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#packages",
    "href": "ae/ae-12-logistic-intro.html#packages",
    "title": "AE 12: Logistic regression introduction",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(\"data/framingham.csv\") |&gt;\n  select(totChol, TenYearCHD) |&gt;\n  drop_na() |&gt;\n  mutate(high_risk = as.factor(TenYearCHD)) |&gt;\n  select(totChol, high_risk)\n\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\nheart_disease |&gt;\n  count(high_risk)\n\n# A tibble: 2 √ó 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#calculating-probability-and-odds",
    "href": "ae/ae-12-logistic-intro.html#calculating-probability-and-odds",
    "title": "AE 12: Logistic regression introduction",
    "section": "Calculating probability and odds",
    "text": "Calculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease in the next 10 years?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease in the next 10 years?"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#logistic-regression-model",
    "href": "ae/ae-12-logistic-intro.html#logistic-regression-model",
    "title": "AE 12: Logistic regression introduction",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet \\(\\pi\\) be the probability an adult is high risk of heart disease. The statistical model is\n\\[\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\\]\n\nheart_disease_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits."
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#calculating-log-odds-odds-and-probabilities",
    "href": "ae/ae-12-logistic-intro.html#calculating-log-odds-odds-and-probabilities",
    "title": "AE 12: Logistic regression introduction",
    "section": "Calculating log-odds, odds and probabilities",
    "text": "Calculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease in the next 10 years?\nWhat are the odds they are high risk for heart disease in the next 10 years?\nWhat is the probability they are high risk for heart disease in the next 10 years? Use the odds to calculate your answer."
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#comparing-observations",
    "href": "ae/ae-12-logistic-intro.html#comparing-observations",
    "title": "AE 12: Logistic regression introduction",
    "section": "Comparing observations",
    "text": "Comparing observations\nSuppose a person‚Äôs cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease in the next 10 years to change?\nHow do you expect the odds that this person is high risk for heart disease in the next 10 years to change?"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#submission",
    "href": "ae/ae-12-logistic-intro.html#submission",
    "title": "AE 12: Logistic regression introduction",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-12 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-03-slr.html",
    "href": "ae/ae-03-slr.html",
    "title": "AE 03: Bike rentals in Washington, DC",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-03 repo to get started. If you do not see an ae-03 repo, click here to create one.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, September 9 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-03-slr.html#data",
    "href": "ae/ae-03-slr.html#data",
    "title": "AE 03: Bike rentals in Washington, DC",
    "section": "Data",
    "text": "Data\nOur data set contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-03-slr.html#exercises",
    "href": "ae/ae-03-slr.html#exercises",
    "title": "AE 03: Bike rentals in Washington, DC",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nBelow are visualizations of the distributions of daily bike rentals and temperature as well as the relationship between these two variables.\n\np1 &lt;- ggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250) + \n  labs(x = \"Daily bike rentals\")\n\np2 &lt;- ggplot(bikeshare, aes(x = temp_orig)) +\n  geom_histogram() + \n  labs(x = \"Temperature (Celsius)\")\n\np3 &lt;- ggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point() + \n  labs(x = \"Temperature (Celsius)\", \n       y = \"Daily bike rentals\")\n\n(p1 | p2) / p3\n\n\n\n\n\n\n\n\n\n\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n\n\n\n[Add your answer here]\n\n\nExercise 2\n\n\n\n\n\n\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 3\n\n\n\n\n\n\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs.¬†temperature faceted by season.\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\n\n\n\n\n\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n\n\n\n[Add your answer here]\n\n\nExercise 5\n\n\n\n\n\n\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 6\n\n\n\n\n\n\nUsing the subset of the data from Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 7\n\n\n\n\n\n\nUse the output to write out the estimated regression equation.\n\n\n\n[Add your answer here]\n\n\nExercise 8\n\n\n\n\n\n\nInterpret the slope in the context of the data.\n\n\n\n[Add your answer here]\n\n\nExercise 9\n\n\n\n\n\n\nInterpret the intercept in the context of the data.\n\n\n\n[Add your answer here]\n\n\nExercise 10\n\n\n\n\n\n\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2024. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n\n\n\n[Add your answer here]\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-03- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html",
    "href": "ae/ae-14-logistic-compare.html",
    "title": "AE 14: Comparing logistic regression models",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-14 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, November 11 at 11:59pm."
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#packages",
    "href": "ae/ae-14-logistic-compare.html#packages",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#response-to-leukemia-treatment",
    "href": "ae/ae-14-logistic-compare.html#response-to-leukemia-treatment",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Response to Leukemia treatment",
    "text": "Response to Leukemia treatment\nToday‚Äôs data is from a study where 51 untreated adult patients with Acute Myeloid Leukemia who were given a course of treatment, and they were assessed as to their response to the treatment.1\nThe goal of today‚Äôs analysis is to use pre-treatment factors to predict how likely it is a patient will respond to the treatment.\nWe will use the following variables:\n\nAge: Age at diagnosis (in years)\nSmear: Differential percentage of blasts\nInfil: Percentage of absolute marrow leukemia infiltrate\nIndex: Percentage labeling index of the bone marrow leukemia cells\nBlasts: Absolute number of blasts, in thousands\nTemp: Highest temperature of the patient prior to treatment, in degrees Fahrenheit\nResp: 1 = responded to treatment or 0 = failed to respond\n\n\nleukemia &lt;- read_csv(\"data/leukemia.csv\") |&gt;\n  mutate(Resp = factor(Resp))"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#comparing-models",
    "href": "ae/ae-14-logistic-compare.html#comparing-models",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Comparing models",
    "text": "Comparing models\n\nConsider a model with all the pre-treatment variables: Age, Smear, Infil, Index, Blasts and Temp. Fit a model using these six variables to predict whether a patient responded to the treatment. Call the model full_model. Display the model.\n\n\n# add code\n\n\nBased on the model, which pre-treatment variables are statistically significant using a threshold of \\(\\alpha = 0.05\\)? (We will talk more about inference for logistic regression coefficients in an upcoming lecture.)\nFit a model that only includes the statistically significant predictors. Call the model reduced_model.\n\n\n# add code\n\n\nUse a drop-in-deviance test to compare a model that includes only the significant predictors to the full model. Which model do you choose based on the results of this test?\n\n\n# add code\n\n\nIs your choice based on AIC consistent with your choice from the previous exercise? What about a choice based on BIC?\n\n\n# add code"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#submission",
    "href": "ae/ae-14-logistic-compare.html#submission",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-14 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#footnotes",
    "href": "ae/ae-14-logistic-compare.html#footnotes",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data set is from the Stat2Data R package. This AE is adapted from exercises in Stat 2.‚Ü©Ô∏é"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html",
    "href": "ae/ae-09-feature-engineering.html",
    "title": "AE 09: Feature engineering with recipes",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-09 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 12 at 11:59pm."
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#packages-data",
    "href": "ae/ae-09-feature-engineering.html#packages-data",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\nThe data for this AE is from the loan50 data set in the openintro R package. We will focus on the following variables:\nPredictors\n\nannual_income: Annual income (in US dollars)\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nResponse\n\ninterest_rate: Interest rate for the loan (0- 100)"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#analysis-goal",
    "href": "ae/ae-09-feature-engineering.html#analysis-goal",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis are to build a recipe to fit a linear regression model on the training data that has the following features:\n\nannual_income rescaled to thousands of dollars\nMean-centered quantitative variables\nIndicator (dummy) variables for the categorical predictor\nInteraction term between rescaled annual_income and verified_income\n\nand (2) use prep() and bake() to check the recipe"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#testtrain-split",
    "href": "ae/ae-09-feature-engineering.html#testtrain-split",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Test/train split",
    "text": "Test/train split\nFill in the code to split the data into 90% training, 10% testing.\n\n\n\n\n\n\nImportant\n\n\n\nRemove #| eval: false from the code chunk.\n\n\n\nset.seed(123)\n\nloans_split &lt;- initial_split(loan50, prop = _____)\nloan_train &lt;- training(_____)\nloan_test  &lt;- _____(loan_split)"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#build-a-recipe",
    "href": "ae/ae-09-feature-engineering.html#build-a-recipe",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Build a recipe",
    "text": "Build a recipe\n\nUse step_mutate() to create a new variable annual_income_th that is annual_income rescaled to thousands of dollars\nUse step_center() to mean-center quantitative variables\nUse step_dummy() to create indicator variables for the categorical predictor\nUse step_interact() to create interaction between annual_income_th and verified_income\n\n\n\n\n\n\n\nImportant\n\n\n\nRemove #| eval: false from the code chunk.\n\n\n\n# use original variables when specifying recipe\nloan_rec &lt;-  recipe(interest_rate ~ annaul_income + debt_to_income + verified_income, \n                    data = loan_train) |&gt;\n  # add recipe steps\n\n\nloan_rec"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#check-recipe-using-prep-and-bake",
    "href": "ae/ae-09-feature-engineering.html#check-recipe-using-prep-and-bake",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Check recipe using prep() and bake()",
    "text": "Check recipe using prep() and bake()\nRemove #| eval: false from the code chunk\n\n# determine required parameters to be estimated\nloan_rec_trained &lt;- prep(loan_rec)\n\n# apply recipe computations to data\nbake(loan_rec_trained, loan_train) |&gt;\n  glimpse()"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#to-submit-the-ae",
    "href": "ae/ae-09-feature-engineering.html#to-submit-the-ae",
    "title": "AE 09: Feature engineering with recipes",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-09 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-04-bootstrap.html",
    "href": "ae/ae-04-bootstrap.html",
    "title": "AE 04: Bootstrap confidence intervals",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-04 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, September 14 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-04-bootstrap.html#data",
    "href": "ae/ae-04-bootstrap.html#data",
    "title": "AE 04: Bootstrap confidence intervals",
    "section": "Data",
    "text": "Data\nThe data are on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. It was originally scraped from Zillow, and can be found in the duke_forest data set in the openintro R package.\n\nglimpse(duke_forest)\n\nRows: 98\nColumns: 13\n$ address    &lt;chr&gt; \"1 Learned Pl, Durham, NC 27705\", \"1616 Pinecrest Rd, Durha‚Ä¶\n$ price      &lt;dbl&gt; 1520000, 1030000, 420000, 680000, 428500, 456000, 1270000, ‚Ä¶\n$ bed        &lt;dbl&gt; 3, 5, 2, 4, 4, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 5, 3, 4, 4, 3,‚Ä¶\n$ bath       &lt;dbl&gt; 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 3.0,‚Ä¶\n$ area       &lt;dbl&gt; 6040, 4475, 1745, 2091, 1772, 1950, 3909, 2841, 3924, 2173,‚Ä¶\n$ type       &lt;chr&gt; \"Single Family\", \"Single Family\", \"Single Family\", \"Single ‚Ä¶\n$ year_built &lt;dbl&gt; 1972, 1969, 1959, 1961, 2020, 2014, 1968, 1973, 1972, 1964,‚Ä¶\n$ heating    &lt;chr&gt; \"Other, Gas\", \"Forced air, Gas\", \"Forced air, Gas\", \"Heat p‚Ä¶\n$ cooling    &lt;fct&gt; central, central, central, central, central, central, centr‚Ä¶\n$ parking    &lt;chr&gt; \"0 spaces\", \"Carport, Covered\", \"Garage - Attached, Covered‚Ä¶\n$ lot        &lt;dbl&gt; 0.97, 1.38, 0.51, 0.84, 0.16, 0.45, 0.94, 0.79, 0.53, 0.73,‚Ä¶\n$ hoa        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ url        &lt;chr&gt; \"https://www.zillow.com/homedetails/1-Learned-Pl-Durham-NC-‚Ä¶"
  },
  {
    "objectID": "ae/ae-04-bootstrap.html#exploratory-data-analysis",
    "href": "ae/ae-04-bootstrap.html#exploratory-data-analysis",
    "title": "AE 04: Bootstrap confidence intervals",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "ae/ae-04-bootstrap.html#model",
    "href": "ae/ae-04-bootstrap.html#model",
    "title": "AE 04: Bootstrap confidence intervals",
    "section": "Model",
    "text": "Model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-04-bootstrap.html#bootstrap-confidence-interval",
    "href": "ae/ae-04-bootstrap.html#bootstrap-confidence-interval",
    "title": "AE 04: Bootstrap confidence intervals",
    "section": "Bootstrap confidence interval",
    "text": "Bootstrap confidence interval\n\n1. Calculate the observed fit (slope)\n\nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159.\n\n\n\n\n2. Take n bootstrap samples and fit models to each one.\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(091222)\n\nboot_fits &lt;- ______ |&gt;\n  specify(______) |&gt;\n  generate(reps = ____, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n\nWhy do we set a seed before taking the bootstrap samples?\nMake a histogram of the bootstrap samples to visualize the bootstrap distribution.\n\n# Code for histogram\n\n\n\n\n3. Compute the 95% confidence interval as the middle 95% of the bootstrap distribution\nFill in the code, then set eval: true .\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = _____, \n  level = ____,\n  type = \"percentile\"\n)"
  },
  {
    "objectID": "ae/ae-04-bootstrap.html#changing-confidence-level",
    "href": "ae/ae-04-bootstrap.html#changing-confidence-level",
    "title": "AE 04: Bootstrap confidence intervals",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nModify the code from Step 3 to create a 90% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\n\nModify the code from Step 3 to create a 99% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\nWhich confidence level produces the most accurate confidence interval (90%, 95%, 99%)? Explain\nWhich confidence level produces the most precise confidence interval (90%, 95%, 99%)? Explain\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-04 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "prepare/week-02.html",
    "href": "prepare/week-02.html",
    "title": "September 06 lecture",
    "section": "",
    "text": "üìñ STA 210 Notes: Ch 1: Simple linear regression"
  },
  {
    "objectID": "prepare/week-02.html#assigned-reading",
    "href": "prepare/week-02.html#assigned-reading",
    "title": "September 06 lecture",
    "section": "",
    "text": "üìñ STA 210 Notes: Ch 1: Simple linear regression"
  },
  {
    "objectID": "prepare/week-02.html#additional-resources-optional",
    "href": "prepare/week-02.html#additional-resources-optional",
    "title": "September 06 lecture",
    "section": "Additional resources (optional)",
    "text": "Additional resources (optional)\nüìñ Introduction to Modern Statistics (IMS) Ch 7: Linear regression with a single predictor"
  },
  {
    "objectID": "prepare/sep-11.html",
    "href": "prepare/sep-11.html",
    "title": "September 11 lecture",
    "section": "",
    "text": "üìñ IMS, Sec 24.2: Randomization test for the slope\nüìñ IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "prepare/sep-11.html#assigned-reading",
    "href": "prepare/sep-11.html#assigned-reading",
    "title": "September 11 lecture",
    "section": "",
    "text": "üìñ IMS, Sec 24.2: Randomization test for the slope\nüìñ IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02: Palmer penguins",
    "section": "",
    "text": "In this analysis you will use simple and multiple linear regression to analyze relationships between variables in three different scenarios."
  },
  {
    "objectID": "hw/hw-02.html#exercise-1",
    "href": "hw/hw-02.html#exercise-1",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 1",
    "text": "Exercise 1\nOur eventual goal is to fit a model predicting body mass (which is more difficult to measure) from island, bill length, bill depth, flipper length, species, and sex.\nWe will start by preparing the data.\n\nUse the drop_na() function to remove any observations from the penguins data frame that has missing values. Your resulting data frame should have 333 observations.\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of this assignment, we will treat the missingness as random, i.e., this is still a representative sample even though we removed observations with missing values."
  },
  {
    "objectID": "hw/hw-02.html#exercise-2",
    "href": "hw/hw-02.html#exercise-2",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nUse the data to fit a model predicting body mass (which can be difficult to measure) from bill length.\nNeatly display the model using 3 digits.\nWrite the estimated regression equation. Use the variable names in your equation.\nInterpret the slope in the context of the data.\nCalculate the \\(R^2\\) value."
  },
  {
    "objectID": "hw/hw-02.html#exercise-3",
    "href": "hw/hw-02.html#exercise-3",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nPlot the fitted values vs.¬†the residual values for the model in the previous exercise.\nMake another plot of the histogram of the residuals.\nIs inference for linear regression appropriate? Why or why not?"
  },
  {
    "objectID": "hw/hw-02.html#exercise-4",
    "href": "hw/hw-02.html#exercise-4",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nThe p-value in Exercise 2 indicates statistical significance. Does this mean the model is a good fit for the data?"
  },
  {
    "objectID": "hw/hw-02.html#exercise-5",
    "href": "hw/hw-02.html#exercise-5",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 5",
    "text": "Exercise 5\n\nNow, use the data to fit a model predicting body mass (which can be difficult to measure) from bill length, bill depth, flipper length, island, species, and sex. Only include main effects, i.e., no interaction terms, in this model.\nNeatly display the model using 3 digits.\nWrite the estimated regression equation. Use the variable names in your equation.\nCalculate the \\(R^2\\) value. Why is it larger than the previous one? Interpret this value in context of the data and the model.\n\n\n\n\n\n\n\nTip\n\n\n\nThe code for fitting multiple linear regression models is the same as simple linear regression. Use + to add multiple predictor variables to the model."
  },
  {
    "objectID": "hw/hw-02.html#exercise-6",
    "href": "hw/hw-02.html#exercise-6",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nPlot the fitted values vs.¬†the residual values for the model in the previous exercise.\nMake another plot of the histogram of the residuals.\nIs inference for linear regression appropriate? Why or why not?"
  },
  {
    "objectID": "hw/hw-02.html#exercise-7",
    "href": "hw/hw-02.html#exercise-7",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nInterpret the intercept in the context of the data.\nInterpret the coefficient of sex in the context of the data.\nInterpret coefficient (slope) of bill length in the context of the data.\nIs the coefficient of bill length the same as in Exercise 2. Briefly explain why or why not?"
  },
  {
    "objectID": "hw/hw-02.html#exercise-8",
    "href": "hw/hw-02.html#exercise-8",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nCalculate the residual for a female Gentoo penguin on the Biscoe island that weighs 4450 grams with the following body measurements: bill_length_mm = 43.2, bill_depth_mm = 14.5, flipper_length_mm = 208.\nDoes the model overpredict or underpredict this penguin‚Äôs mass?"
  },
  {
    "objectID": "hw/hw-02.html#exercise-9",
    "href": "hw/hw-02.html#exercise-9",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 9",
    "text": "Exercise 9\nFill in the starter code below to write the function predict_boots which takes a bootstrap sample, calculates \\(R^2\\) terms for the two models listed below. Set the seed to 29.\n\nModel 1: The model from Exercise 5. Model 2: The model from Exercise 5, without the island term.\nThe last line of the code runs the function predict_boots for 1000 iterations and saves the output as the object r_squared_diffs. Create a new variable that calculates the difference between the \\(R^2\\) values for the two models.\nMake a histogram of the differences.\nWhat do you notice about the histogram?\nWhich model is best for predicting body mass of a penguin? Briefly explain.\n\n\nset.seed(29)\n\npredict_boots &lt;- function(i) {\n  # take a bootstrap sample\n  boot &lt;- penguins |&gt;\n    slice_sample(n = nrow(penguins), replace = TRUE) |&gt;\n    mutate(boot_samp = i)\n  \n  #fit the model with island to the bootstrap sample\n  fit_island &lt;- linear_reg() |&gt;\n    fit(_____, data = boot)\n  \n  # fit the model without island to the bootstrap sample\n  fit_no_island &lt;- linear_reg() |&gt;\n    fit(_______, data = boot)\n  \n  #save the rsq values for each model\n  tibble(\n    island_r2 = glance(fit_island)$r.squared,\n    no_island_r2 = glance(fit_no_island)$r.squared\n  )\n}\n\n# run the function 1000 times and save the differences between\nr_squared_diffs &lt;- map_df(1:1000, predict_boots)"
  },
  {
    "objectID": "hw/hw-02.html#exercise-10",
    "href": "hw/hw-02.html#exercise-10",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nUse the model from the previous exercise without island, and set the seed to 210.\nFit the model to 1000 bootstrap samples.\nMake a scatterplot of the estimated coefficients for flipper length (y-axis) vs.¬†bill length (x-axis).\nCalculate a 95% confidence interval for the difference between these coefficient values. Is there evidence that either bill length or flipper length has a larger influence on the model? Briefly explain.\n\n\n\n\n\n\n\nTip\n\n\n\nThe code for bootstrapping in multiple linear regression is the same as the code for simple linear regression."
  },
  {
    "objectID": "hw/hw-02.html#footnotes",
    "href": "hw/hw-02.html#footnotes",
    "title": "HW 02: Palmer penguins",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Education & median income in US Counties",
    "section": "",
    "text": "In this assignment, you will use simple linear regression to examine the association between between the percent of adults with a bachelor‚Äôs degree and the median household income for counties in the United States."
  },
  {
    "objectID": "hw/hw-01.html#part-1-exploratory-data-analysis",
    "href": "hw/hw-01.html#part-1-exploratory-data-analysis",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 1: Exploratory data analysis",
    "text": "Part 1: Exploratory data analysis\n\nExercise 1\nCreate a histogram of the distribution of the predictor variable bachelorsand calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\n\n\nExercise 2\nLet‚Äôs view the data in another way. Use the code below to make a map of the United States with the color of the counties filled in based on the percent of residents 25 years old and older who have a Bachelor‚Äôs degree. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 observations you have from the map?\nWhat is a feature that is apparent in the map that wasn‚Äôt as easily apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not as easily apparent from the map?\n\n\ncounty_map_data &lt;- left_join(county_data_sample, map_data_sample)\n\nggplot(data = map_data_all) +\n  geom_polygon(aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = county_map_data, aes(x = long, y = lat, group = group,\n    fill = bachelors)\n    ) +\n  labs(\n    x = \"Longitude\",\n    y = \"Latitude\",\n    fill = \"_____\",\n    title = \"_____\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\n\nExercise 3\nCreate a visualization of the relationship between bachelors and median_household_income and calculate the correlation. Use the visualization and correlation to describe the relationship between the two variables.\n\n\n\n\n\n\nTip\n\n\n\nRecall the analysis objective stated at the beginning of the Exercises section.\n\n\n\nIf you haven‚Äôt yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#part-2-modeling",
    "href": "hw/hw-01.html#part-2-modeling",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 2: Modeling",
    "text": "Part 2: Modeling\n\nExercise 4\nWe will use a linear regression model to better quantify the relationship between bachelors and median_household_income.\nWrite the form of the statistical model we will use for this task using mathematical notation. Use variable names (bachelors and median_household_income) in the equation for your model1.\n\n\nExercise 5\n\nFit the regression line corresponding to the statistical model in the previous exercise. Neatly display the model output using 3 digits.\nWrite the equation of the fitted model using mathematical notation. Use variable names (bachelors and median_household_income) in the equation.\n\n\n\nExercise 6\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nIs it useful to interpret the intercept for this data? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#part-3-inference-for-the-u.s.",
    "href": "hw/hw-01.html#part-3-inference-for-the-u.s.",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 3: Inference for the U.S.",
    "text": "Part 3: Inference for the U.S.\nWe want to use the data from these 600 randomly selected counties to draw conclusions about the relationship between the percent of adults age 25 and older with a bachelor‚Äôs degree and median household income for the over 3,000 counties in the United States.\n\nExercise 7\n\nWhat is the population of interest? What is the sample?\nIs it reasonable to treat the sample in this analysis as representative of the population? Briefly explain why or why not.\n\n\n\nExercise 8\nConduct a hypothesis test for the slope to assess whether there is sufficient evidence of a linear relationship between the percent of adults age 25 and older with a bachelor‚Äôs degree and the median household income in a county. Use a randomization (permutation) test. In your response:\n\nState the null and alternative hypotheses in words and mathematical notation\nShow all relevant code and output used to conduct the test. Use set.seed(2023) and 1000 iterations to construct the appropriate distribution.\nState the conclusion in the context of the data.\n\n\n\nExercise 9\nNext, construct a 95% confidence interval for the slope using bootstrapping with set.seed(2023) and 1000 iterations.\n\nShow all relevant code and output used to calculate the interval.\nInterpret the confidence interval in the context of the data.\nIs the confidence interval consistent with the results of the test from the previous exercise? Briefly explain why or why not.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#reproducibility",
    "href": "hw/hw-01.html#reproducibility",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nExercise 10\nYou are asked to use a reproducible workflow for all of your work in the class, and the goal of this question to is better understand potential real-world implications of doing (or not) doing so. Below are some real-life examples in which having a non-reproducible workflow resulted in errors that impacted research and public records.\n\nSource: Ostblom and Timbers (2022)\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al.¬†2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al.¬†2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nChoose one of the scenarios from the table and read the linked article discussing what went wrong. Then,\n\nBriefly describe what went wrong, i.e., what part of the process of was not reproducible and what error or impact that had.\nThen, describe how the researchers could make the process reproducible.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#submission",
    "href": "hw/hw-01.html#submission",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ‚û°Ô∏è Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "hw/hw-01.html#grading-50-points",
    "href": "hw/hw-01.html#grading-50-points",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Grading (50 points)",
    "text": "Grading (50 points)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n5\n\n\nEx 2\n5\n\n\nEx 3\n4\n\n\nEx 4\n3\n\n\nEx 5\n4\n\n\nEx 6\n4\n\n\nEx 7\n3\n\n\nEx 8\n7\n\n\nEx 9\n7\n\n\nEx 10\n5\n\n\nWorkflow & formatting\n32"
  },
  {
    "objectID": "hw/hw-01.html#footnotes",
    "href": "hw/hw-01.html#footnotes",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClick here for a guide on writing mathematical symbols using LaTex.‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-05.html",
    "href": "labs/lab-05.html",
    "title": "Lab 05: Candy Competition",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, October 20, 11:59pm (Tuesday labs)\nFriday, October 20, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-05.html#introduction",
    "href": "labs/lab-05.html#introduction",
    "title": "Lab 05: Candy Competition",
    "section": "Introduction",
    "text": "Introduction\nIn today‚Äôs lab you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\nLearning goals\nBy the end of the lab you will be able to\n\ndescribe the components of a recipe\nfit a model using recipes\ncompare models\ncontinue developing a collaborative workflow with your teammates"
  },
  {
    "objectID": "labs/lab-05.html#getting-started",
    "href": "labs/lab-05.html#getting-started",
    "title": "Lab 05: Candy Competition",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-05. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-05.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-05.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 05: Candy Competition",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group‚Äôs .qmd file at a time. Once that person has finished typing the group‚Äôs responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-05.html#packages",
    "href": "labs/lab-05.html#packages",
    "title": "Lab 05: Candy Competition",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fivethirtyeight)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-05.html#data-candy",
    "href": "labs/lab-05.html#data-candy",
    "title": "Lab 05: Candy Competition",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data from this lab comes from the the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g.¬†Reeses vs.¬†Skittles). Click here to check out some of the match ups.\nThe data set contains the characteristics and win percentage from 85 candies in the experiment. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nchocolate\nDoes it contain chocolate?\n\n\nfruity\nIs it fruit flavored?\n\n\ncaramel\nIs there caramel in the candy?\n\n\npeanutalmondy\nDoes it contain peanuts, peanut butter or almonds?\n\n\nnougat\nDoes it contain nougat?\n\n\ncrispedricewafer\nDoes it contain crisped rice, wafers, or a cookie component?\n\n\nhard\nIs it a hard candy?\n\n\nbar\nIs it a candy bar?\n\n\npluribus\nIs it one of many candies in a bag or box?\n\n\nsugarpercent\nThe percentile of sugar it falls under within the data set. Values 0 - 1.\n\n\npricepercent\nThe unit price percentile compared to the rest of the set. Values 0 - 1.\n\n\nwinpercent\nThe overall win percentage according to 269,000 matchups. Values 0 - 100.\n\n\n\nUse the code below to get a glimpse of the candy_rankings data frame in the fivethirtyeight R package.\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter‚Ä¶\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F‚Ä¶\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE‚Ä¶\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,‚Ä¶\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, ‚Ä¶\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,‚Ä¶\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS‚Ä¶\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F‚Ä¶\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE‚Ä¶\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31‚Ä¶\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51‚Ä¶\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.‚Ä¶"
  },
  {
    "objectID": "labs/lab-05.html#exercises",
    "href": "labs/lab-05.html#exercises",
    "title": "Lab 05: Candy Competition",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use multiple linear regression to understand the factors that make a good candy.\n\nExercise 1\n\nUse ggplot to make a graph of your choice exploring the relationship between at least 3 variables in the data set.\nWrite two observations from your graph.\n\n\n\nExercise 2\nSplit the data into training (80%) and testing sets (20%). Call the training set candy_train and the testing set candy_test. Use a seed of 2.\n\n\nExercise 3\nBelow is a recipe for a model that uses the characteristics of candy to understand variability in the win percentage. The lines of the recipe code are labeled Line 1 - Line 9. Describe what each line of code does. The explanation should be written comprehensively and specifically enough that someone could replicate the data manipulation steps based on your description.\nFor example, if a line of code is step_center(X), a comprehensive and specific explanation something similar to the following: ‚ÄúThis line of code mean centers the variable \\(X\\) by subtracting \\(\\bar{X}\\) from each value of \\(X\\) in the training data.‚Äù\n\n\n\n\n\n\nTip\n\n\n\nUse the Recipes Function Reference page as a resource to learn more about the step_ functions.\n\n\n\n#Line 1\ncandy_rec &lt;- recipe(winpercent ~ ., data = candy_train) |&gt; \n#Line 2\n  update_role(competitorname, new_role = \"ID\") |&gt; \n# Line 3\n  step_cut(sugarpercent, breaks = c(0, 0.25, 0.5, 0.75, 1)) |&gt; \n#Line 4\n  step_mutate(pricepercent = pricepercent * 100) |&gt; \n#Line 5\n  step_dummy(all_nominal_predictors()) |&gt; \n#Line 6\n  step_interact(terms =~ pricepercent:chocolate) |&gt; \n#Line 7\n  step_interact(terms =~ peanutyalmondy:chocolate) |&gt; \n#Line 8\n  step_rm(fruity, caramel, hard, pluribus, bar, nougat, crispedricewafer) |&gt; \n#Line 9\n  step_zv(all_predictors()) \n\n\n\nExercise 4\nFill in the code to use prep and bake for a preview of what will happen when the recipe in Exercise 3 is applied.\n\ncandy_rec |&gt;\n  prep() |&gt;\n  bake(_____) |&gt;\n  glimpse()\n\nHow many terms (not including the intercept) will be in the model produced by this recipe?\n\n\nExercise 5\nFill in the code to specify model, build the model workflow using the recipe in Exercise 2, and fit the model to the training data. Then, neatly display the model using 3 digits.\n\n#specify the model\ncandy_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#build model workflow\ncandy_workflow &lt;- workflow() |&gt;\n  add_model(_____) |&gt;\n  add_recipe(_____) \n\n# fit the model \ncandy_fit &lt;- candy_workflow |&gt;\n  fit(data = _____)\n\n\n\nExercise 6\nInterpret the following in the context of the data:\n\nIntercept\nCoefficient of sugarpercent_X.0.75.1.\nCoefficient of pricepercent_x_chocolateTRUE\nEffect of pricepercent for chocolate candy\n\n\n\nExercise 7\nLet‚Äôs consider another model. Use the recipe workflow to fit a new model that meets the following criteria:\n\nIncludes variables chocolate, pricepercent, crispedricewafer, peanutyalmondy, sugarpercent\nUpdate pricepercent so it ranges from 0 to 100 (instead of 0 to 1)\nMakes sugarpercent a factor where the levels equal the four quartiles: 0 - 0.25, 0.25 - 0.50, 0.50 - 0.75, 0.75 - 1\nIncludes the interaction between pricepercent and peanutyalmondy\n\nNeatly display the model using 3 digits.\n\n\n\n\n\n\nTip\n\n\n\nSee the Function Reference page on recipes.tidymodels.org to find the appropriate recipe functions.\n\n\n\n\nExercise 8\n\nConsider the model from Exercise 5 ‚ÄúModel 1‚Äù and the model fit in Exercise 7 ‚ÄúModel 2‚Äù. Use the glance() function to calculate \\(R^2\\) for both of these models.\nWhich model would you choose based on \\(R^2\\)? Briefly explain your choice.\n\n\n\nExercise 9\nWe will use RMSE to evalulate the predictive performance of each model on the testing data.\n\nUse the code below to calculate predicted values and RMSE for Model 1 on the testing data. Then get the RMSE for Model 2 on the testing data.\n\n\npredict_test1 &lt;- predict(candy_fit, candy_test) |&gt; \n  bind_cols(candy_test)\n\nrmse1 &lt;- rmse(predict_test1, truth = winpercent, estimate = .pred)\n\n\nWhich model would you choose based on RMSE on the testing data? Briefly explain your choice.\n\n\n\nExercise 10\n\nUse the model you selected in Exercise 9 to describe what generally makes a good candy, i.e., one with a high win percentage."
  },
  {
    "objectID": "labs/lab-05.html#submission",
    "href": "labs/lab-05.html#submission",
    "title": "Lab 05: Candy Competition",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nSelect all team members‚Äô names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "labs/lab-05.html#grading",
    "href": "labs/lab-05.html#grading",
    "title": "Lab 05: Candy Competition",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n6\n\n\nEx 2\n3\n\n\nEx 3\n5\n\n\nEx 4\n1\n\n\nEx 5\n2\n\n\nEx 6\n8\n\n\nEx 7\n8\n\n\nEx 8\n4\n\n\nEx 9\n4\n\n\nEx 10\n4\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-05.html#footnotes",
    "href": "labs/lab-05.html#footnotes",
    "title": "Lab 05: Candy Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and collaboration. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-00.html",
    "href": "labs/lab-00.html",
    "title": "Lab 00: Getting started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today‚Äôs lab tasks before leaving lab today.\n\n\n\nReserve a STA 210 container\n\nMake sure to reserve the container titled ‚ÄúSTA210‚Äù not ‚ÄúRStudio‚Äù\nStart the container and confirm you see the RStudio interface\n\nSubmit your GitHub username and confirm you have access to the 210 container\n\nIf you do not have a GitHub username, click here for information on registering for a GitHub account and choosing a username\n\nComplete the STA 210 Student Survey"
  },
  {
    "objectID": "labs/lab-03.html",
    "href": "labs/lab-03.html",
    "title": "Lab 03: Coffee grades",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, September 22, 11:59pm (Tuesday labs)\nSunday, September 24, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-03.html#learning-goals",
    "href": "labs/lab-03.html#learning-goals",
    "title": "Lab 03: Coffee grades",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will‚Ä¶\n\nbe able to use mathematical models to conduct inference for the slope\nbe able to assess conditions for simple linear regression"
  },
  {
    "objectID": "labs/lab-03.html#grading-50-pts",
    "href": "labs/lab-03.html#grading-50-pts",
    "title": "Lab 03: Coffee grades",
    "section": "Grading (50 pts)",
    "text": "Grading (50 pts)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n4\n\n\nEx 2\n3\n\n\nEx 3\n4\n\n\nEx 4\n3\n\n\nEx 5\n8\n\n\nEx 6\n5\n\n\nEx 7\n8\n\n\nEx 8\n6\n\n\nEx 9\n4\n\n\nEx 10\n2\n\n\nWorkflow & formatting\n31"
  },
  {
    "objectID": "labs/lab-03.html#footnotes",
    "href": "labs/lab-03.html#footnotes",
    "title": "Lab 03: Coffee grades",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-02.html",
    "href": "labs/lab-02.html",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, September 15, 11:59pm (Tuesday labs)\nSunday, September 17, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-02.html#learning-goals",
    "href": "labs/lab-02.html#learning-goals",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will‚Ä¶\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use simulation-based inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-02.html#exercise-10",
    "href": "labs/lab-02.html#exercise-10",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nThere is a statistically significant linear relationship between average air temperature and average ice duration on Lake Monona \\((\\beta_1 \\neq 0)\\).\n\nDoes the confidence interval you calculated in the previous exercise support or refute this claim? Briefly explain.\n\nRender, commit, and push your final changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you‚Äôve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-02.html#footnotes",
    "href": "labs/lab-02.html#footnotes",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://lter.github.io/lterdatasampler/index.html‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "Lab 01: Park access",
    "section": "",
    "text": "Important\n\n\n\nDue\n\nFriday, September 8, 11:59pm (Tuesday labs)\nSunday, September 10, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-01.html#learning-goals",
    "href": "labs/lab-01.html#learning-goals",
    "title": "Lab 01: Park access",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2 and use those visualizations to describe distributions\nBe gain to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 01: Park access",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-01-. It contains the starter documents you need to complete the lab.\n\nIf you do not see your lab-01 repo, click here to create your repo. Then, click here to (re)submit your GitHub username.\n\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab-01.html#r-and-r-studio",
    "href": "labs/lab-01.html#r-and-r-studio",
    "title": "Lab 01: Park access",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of an Quarto (.Rmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-01.html#footnotes",
    "href": "labs/lab-01.html#footnotes",
    "title": "Lab 01: Park access",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-04.html",
    "href": "labs/lab-04.html",
    "title": "Lab 04: Alumni jobs",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, September 29, 11:59pm (Tuesday labs)\nSunday, October 1, 11:59pm (Thursday labs)\nIn today‚Äôs lab, you‚Äôll use simple linear regression to analyze the relationship between the median early career salary and percent of alumni who perceive their job as making the world a better place for colleges and universities in the United States. You will also start working with your lab teams, write a team agreement, and be introduced to using GitHub for collaboration."
  },
  {
    "objectID": "labs/lab-04.html#exercise-1",
    "href": "labs/lab-04.html#exercise-1",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a linear model that can be used to predict the typical (median) early career pay based on the percent of STEM degrees. Display the model output using 3 digits for numerical values."
  },
  {
    "objectID": "labs/lab-04.html#exercise-2",
    "href": "labs/lab-04.html#exercise-2",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate the predicted values and residuals from your model and save these results in a data frame. Print the first five rows of the new data frame.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It‚Äôs your turn! Type the team‚Äôs response to exercise 3."
  },
  {
    "objectID": "labs/lab-04.html#exercise-3",
    "href": "labs/lab-04.html#exercise-3",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 3",
    "text": "Exercise 3\nBefore using the model for prediction, let‚Äôs check the model conditions. To figure out how to do so, we type the following prompt into ChatGPT (‚Äúan AI powered language model‚Äù) ‚ÄúHow do you check the model conditions for simple linear regression?‚Äù\nThe response from ChatGPT shown below would receive partial credit. For each condition, state whether it is correct or incorrect/ incomplete. If it incorrect/incomplete, state what is wrong or missing, then write the correct response.\n\nIn simple linear regression, you‚Äôre trying to establish a linear relationship between two variables: a dependent variable (Y) and an independent variable (X). The model conditions or assumptions for simple linear regression are important to check to ensure that your model is valid and reliable. Here are the key assumptions and how to check them:\n1. Linearity Assumption:\n\nAssumption: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nHow to Check: Create a scatterplot of Y against X. If the points roughly form a straight line, the linearity assumption is likely met.\n\n2. Independence Assumption:\n\nAssumption: The residuals (the differences between observed Y and predicted Y) are independent of each other.\nHow to Check: Plot the residuals against the independent variable (X) or the predicted values. Look for patterns or trends in the residuals. There should be no clear structure or autocorrelation in the residuals.\n\n3. Homoscedasticity [Constant variance] Assumption:\n\nAssumption: The variance of the residuals is constant across all levels of the independent variable.\nHow to Check: Plot the residuals against the predicted values or the independent variable (X). The spread of the residuals should remain roughly constant across the range of X.\n\n4. Normality Assumption:\n\nAssumption: The residuals follow a normal distribution.\nHow to Check: Create a histogram or a normal probability plot (Q-Q plot) of the residuals. If the residuals are approximately normally distributed, they should follow a bell-shaped curve in the histogram and lie along a straight line in the Q-Q plot.\n\nIt‚Äôs important to note that these assumptions may not always hold perfectly in real-world data, and minor violations may not necessarily invalidate your model. However, if you detect significant violations of these assumptions, you may need to consider alternative modeling techniques or transformations of your data to address the issues.\n\n\n‚úÖ ‚¨ÜÔ∏è Team Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 3.\nTeam Member 3: It‚Äôs your turn! Type the team‚Äôs response to exercises 4 - 5.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can proceed assuming all the model conditions are satisfied."
  },
  {
    "objectID": "labs/lab-04.html#exercise-4",
    "href": "labs/lab-04.html#exercise-4",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n31 percent of the alumni at Johns Hopkins University received a STEM degree. Calculate the predicted early career pay at this university and the appropriate 95% interval. Interpret the interval in the context of the data.\nThe actual early career pay at this university is $67,200. Is this what you expected based on your analysis? Briefly explain your reasoning."
  },
  {
    "objectID": "labs/lab-04.html#exercise-5",
    "href": "labs/lab-04.html#exercise-5",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext, let‚Äôs consider how well the model fits the relationship between the early career pay and percent with a STEM degree.\n\nCalculate \\(R^2\\) and interpret it in the context of the data.\nCalculate RMSE and interpret it in the context of the data.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 4 - 5.\nTeam Member 4: It‚Äôs your turn! Type the team‚Äôs response to exercise 6."
  },
  {
    "objectID": "labs/lab-04.html#exercise-6",
    "href": "labs/lab-04.html#exercise-6",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 6",
    "text": "Exercise 6\nDo you think the model is useful for understanding and predicting the typical early career pay for alumni at a university? Briefly explain your reasoning.\n\nTeam Member 4: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 6.\nTeam Member 1: It‚Äôs your turn! Type the team‚Äôs response to exercise 7."
  },
  {
    "objectID": "labs/lab-04.html#exercise-7",
    "href": "labs/lab-04.html#exercise-7",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit another model which predicts the typical early career pay using only the percentage of alumni that believe their career will make the world a better place.\n\nNeatly display this model with 3 digits.\nWhich model do you think is better - this model or the model fit in Exercise 1? Explain your response showing any analysis used to make the decision.\n\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team‚Äôs completed lab!"
  },
  {
    "objectID": "labs/lab-04.html#footnotes",
    "href": "labs/lab-04.html#footnotes",
    "title": "Lab 04: Alumni jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon‚Äôt trust yourself to keep your hands off the keyboard? Put them in your pocket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and collaboration. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-06.html",
    "href": "labs/lab-06.html",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, November 03 , 11:59pm (Tuesday labs)\nSunday, November 05, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-06.html#introduction",
    "href": "labs/lab-06.html#introduction",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Introduction",
    "text": "Introduction\nIn this assignment, you‚Äôll get to put into practice the logistic regression skills you‚Äôve developed to analyze data about Palmer Penguins. You will also practice handling merge conflicts."
  },
  {
    "objectID": "labs/lab-06.html#learning-goals",
    "href": "labs/lab-06.html#learning-goals",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will be able to\n\nconduct exploratory data analysis for logistic regression\nfit logistic regression models and write the regression equation\nuse the model to calculate predicted probabilities\ncontinue developing a collaborative workflow with your teammates\ndevelop strategies for dealing with merge conflicts"
  },
  {
    "objectID": "labs/lab-06.html#merge-conflicts-uh-oh",
    "href": "labs/lab-06.html#merge-conflicts-uh-oh",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Merge Conflicts (uh oh)",
    "text": "Merge Conflicts (uh oh)\n\n\n\nCollaboration with no merge conflict\n\n\n\n\n\nCollaboration with a merge conflict\n\n\nYou may have seen something like the image above through the course of your collaboration. When two collaborators make changes to a file and push the file to their repository, git merges these two files.\nIf these two files have conflicting content on the same line, git will produce a merge conflict. Merge conflicts need to be resolved manually, as they require a human intervention:\n\n\n\nMerge conflict identifiers\n\n\nTo resolve the merge conflict, decide if you want to keep only your text, the text on GitHub, or incorporate changes from both texts. Delete the conflict markers &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt; and make the changes you want in the final merge.\nAssign numbers 1, 2, 3, and 4 to each of your team members (if only 3 team members, just number 1 through 3). Go through the following steps in detail, which simulate a merge conflict. Completing this exercise will be part of the lab grade.\n\nResolving a merge conflict\nStep 1: Everyone clone the merge-conflict assignment repo in RStudio and open file merge-conflict.Rmd.\nMember 4 should look at the group‚Äôs repo on GitHub.com to ensure that the other members‚Äô files are pushed to GitHub after every step.\nStep 2: Member 1 Change the team name to your team name. Render, commit, and push.\nStep 3: Member 2 Change the team name to something different (i.e., not your team name). Render, commit, and push.\nYou should get an error.\nPull and review the document with the merge conflict. Read the error to your teammates. You can also show them the error by sharing your screen. A merge conflict occurred because you edited the same part of the document as Member 1. Resolve the conflict with whichever name you want to keep, then render, commit and push again.\nStep 4: Member 3 Write some narrative in the space provided. You should get an error.\nThis time, no merge conflicts should occur, since you edited a different part of the document from Members 1 and 2. Read the error to your teammates. You can also show them the error by sharing your screen.\nClick to pull. Then render, commit, and push.\nPlease ask your TA if you have any questions about merge conflicts and collaborating in GitHub."
  },
  {
    "objectID": "labs/lab-06.html#getting-started",
    "href": "labs/lab-06.html#getting-started",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-05. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-06.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-06.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group‚Äôs .qmd file at a time. Once that person has finished typing the group‚Äôs responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-06.html#packages",
    "href": "labs/lab-06.html#packages",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-06.html#data-palmer-penguins",
    "href": "labs/lab-06.html#data-palmer-penguins",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nWe will go back to the Palmer penguins data used in HW 02.\nData were collected and made available by Dr.¬†Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\nThese data can be found in the palmerpenguins package. We‚Äôre going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica. Click here to see the codebook.\nWe will focus on the following variables:\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nbody_mass_g\ninteger\nBody mass in g\n\n\nflipper_length_mm\ninteger\nFlipper length in mm"
  },
  {
    "objectID": "labs/lab-06.html#exercises",
    "href": "labs/lab-06.html#exercises",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is considered large. Let‚Äôs assume a penguin is considered large if it has an above average body mass. First, remove missing values and create a new response variable to identify whether a penguin is large.\n\npenguins &lt;- penguins |&gt;\n  drop_na() |&gt;\n  mutate(large = factor(if_else(body_mass_g &gt; mean(body_mass_g, na.rm = TRUE), 1, 0)))\n\nAnd let‚Äôs make sure the new variable looks how we would expect before we continue with the analysis. Note the mean body mass for the penguins in the data is 4207.057 grams.\n\npenguins |&gt; \n  group_by(large) |&gt;\n  summarise(min = min(body_mass_g), mean = mean(body_mass_g), max = max(body_mass_g))\n\n# A tibble: 2 √ó 4\n  large   min  mean   max\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1 0      2700 3605.  4200\n2 1      4250 4988.  6300\n\n\n\nExercise 1\nLet‚Äôs start by examining the relationship between large and species.\nVisualize the relationship between large and species. Write something you observe about the relationship between these two variables based on the plot.\n\n\nExercise 2\n\nCalculate the probability a randomly selected penguin is large, given that it is from the Adelie species.\nCalculate the odds a randomly selected penguin is large, given it is from the Adelie speices.\n\n\n\nExercise 3\nSplit the data into a 75% training set, penguins_train, and 25% test set, penguins_test. Use a seed of 29. We will be using the training set throughout the assignment for modeling and the test set to assess the performance.\n\n\nExercise 4\nYou want to fit a model using species to predict the odds a penguins is large. Let \\(\\pi\\) be the probability a penguin is large. The model has the form shown below.\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1~ Chinstrap + \\beta_2 ~ Gentoo\n\\]\n\nFit the model using the training data and neatly display the model output using three digits.\nWhat are the predicted odds of a penguin being large if it is Gentoo?\nWhat are the predicted odds of a penguin being large if it is Adelie?\n\n\n\nExercise 5\nNext, we‚Äôd like to add flipper length to the model. We‚Äôll start by examining the relationship between these two variables.\n\nVisualize the relationship between flipper_length_mm and large. What is something you observe about the relationship between these two variables from the plot?\nAdd flipper length to the model from Exercise 4 so that there are two predictors, species and flipper_length_mm. Neatly display the model output using three digits.\nWrite the estimated regression equation.\n\n\n\nExercise 6\nUse the model from Exercise 5.\n\nHow do you expect the log-odds of being a large penguin to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins are of the Gentoo species.\nHow do you expect the odds of being a large penguin to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins are of the Gentoo species.\n\n\n\nExercise 7\nUse the model from Exercise 5.\n\nHow do you expect the log-odds of being a large penguin to change when going from a penguin with flipper length 195 mm recorded as Chinstrap to a penguin with flipper length 210 mm recorded as Adelie?\nHow do you expect the odds of being a large penguin to change when going from a penguin with flipper length 195 mm recorded as Chinstrap to a penguin with flipper length 210 mm recorded as Adelie?\n\n\n\nExercise 8\nNow let‚Äôs assess the performance of both models on the test set.\n\nUse predict() function to get the predicted class for the observations in the penguins_test based on the model from Exercise 4. Calculate the percentage of observations that were mislcassified.\nUse the predict() function to get the predicted class for the observations in penguins_test based on the model from Exercise 5. Calculate the percentage of observations that were mislcassified.\nBased on this analysis, which model would you select? Briefly explain."
  },
  {
    "objectID": "labs/lab-06.html#submission",
    "href": "labs/lab-06.html#submission",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nSelect all team members‚Äô names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "labs/lab-06.html#grading",
    "href": "labs/lab-06.html#grading",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 8\n40\n\n\nWorkflow & formatting\n51\n\n\nComplete merge conflict activity\n5"
  },
  {
    "objectID": "labs/lab-06.html#footnotes",
    "href": "labs/lab-06.html#footnotes",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and collaboration. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "",
    "text": "In this analysis you will use multiple linear regression to fit and evaluate models using characteristics of LEGO sets to understand variability in the price."
  },
  {
    "objectID": "hw/hw-03.html#exercise-1",
    "href": "hw/hw-03.html#exercise-1",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn this analysis, we dropped observations that have missing values for some of the relevant variables. What is a disadvantage of dropping observations that have missing values, instead of using a method to impute, i.e., fill in, the missing data? How might dropping these observations impact the generalizability of conclusions? What is the disadvantage of replacing the NAs in Minifigures to 0?"
  },
  {
    "objectID": "hw/hw-03.html#exercise-2",
    "href": "hw/hw-03.html#exercise-2",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distributions of the predictor variables Pieces, Size, Year, and Pages. Neatly arrange the plots using the patchwork package."
  },
  {
    "objectID": "hw/hw-03.html#exercise-3",
    "href": "hw/hw-03.html#exercise-3",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat are some feature engineering steps you might use to prepare the variables in the previous exercise for the model? Describe the steps and the function you would use. The list should incorporate at least three different step_ functions.\n\n\n\n\n\n\nTip\n\n\n\nUse the recipes reference page for a list of step_ functions."
  },
  {
    "objectID": "hw/hw-03.html#exercise-4",
    "href": "hw/hw-03.html#exercise-4",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe distribution of Theme is shown below. The bars are ordered by the frequency they occur in the data set.\n\nlegos |&gt;\n  count(Theme) |&gt;\nggplot(aes(x = fct_reorder(Theme, n), y = n)) +\n  geom_col() + \n    labs(title = \"Lego Set Theme\", \n         x = \"Theme\", \n         y = \"Number of LEGO sets\") + \n  coord_flip()\n\n\n\n\nWhat is one reason we should avoid putting the variable Theme in a model as is?"
  },
  {
    "objectID": "hw/hw-03.html#exercise-5",
    "href": "hw/hw-03.html#exercise-5",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 5",
    "text": "Exercise 5\nWe will use v-fold cross validation to compare two models. We‚Äôll start by preparing the data, creating the folds, and defining the model specification that will be used for both models.\n\nSplit the data into training (75%) and testing (25%) sets. Use a seed of 6.\nSplit the training data into 12 folds. Set the seed to 6 again.\nSpecify the model."
  },
  {
    "objectID": "hw/hw-03.html#exercise-6",
    "href": "hw/hw-03.html#exercise-6",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the training data to create a recipe for the first model. The model uses the variables Size, Theme, and Pages to predict Amazon_Price. Conduct the following feature engineering steps:\n\nUse step_other() to collapse Theme into fewer categories. Define the threshold such that any levels of Theme with fewer than 20 observations is defined as ‚ÄúOther‚Äù.\nMean-center Pages.\nMake dummy variables for all categorical predictors.\nRemove any predictors with zero variance.\n\nThen create the workflow that brings together this recipe and the model specification from the previous exercise."
  },
  {
    "objectID": "hw/hw-03.html#exercise-7",
    "href": "hw/hw-03.html#exercise-7",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 7",
    "text": "Exercise 7\nConduct 12-fold cross validation using the workflow from the previous exercise. Calculate and display mean RMSE across the 12 folds.\n\n\n\n\n\n\nNote\n\n\n\nWe will just use RMSE to compare models for this assignment; however, in practice, it is best to take into account multiple model fit statistics to get a more holistic evaluation and comparison of the models."
  },
  {
    "objectID": "hw/hw-03.html#exercise-8",
    "href": "hw/hw-03.html#exercise-8",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs consider a new model that includes all the variables used in model from Exercise 6 along with Year, Pieces and Minifigures.\n\nUse the training data to create a recipe that uses all the feature engineering steps in Exercise 6 with the addition of the following steps:\n\nCreate a new variable called since2018 that calculates the number of years since 2018.\nRemove Year as a potential predictor.\nMean-center Pieces.\n\nCreate the workflow that brings together this recipe and the model specification from Exercise 5.\nConduct 12-fold cross validation using this model workflow. Calculate and display mean RMSE across the 12 folds."
  },
  {
    "objectID": "hw/hw-03.html#exercise-9",
    "href": "hw/hw-03.html#exercise-9",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 9",
    "text": "Exercise 9\nCompare the cross validation results from Exercises 7 and 8. Which model do you select based on RMSE? Briefly explain your choice."
  },
  {
    "objectID": "hw/hw-03.html#exercise-10",
    "href": "hw/hw-03.html#exercise-10",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 10",
    "text": "Exercise 10\nRefit the selected model on the entire training data. Neatly display the model using 3 digits.\nThen, calculate VIF for the model and use it to comment on whether there are potential issues with multicollinearity.\n\n\n\n\n\n\nTip\n\n\n\nWhen we fit a model using recipe and workflow, we need to extract the model object before using augment or vif functions. Fill in the name of the selected model in both blanks in the code below to extract the model object and calculate VIF\n\nlegos_fit_model &lt;- extract_fit_parsnip(______)\nvif(legos_fit_model$fit)"
  },
  {
    "objectID": "hw/hw-03.html#exercise-11",
    "href": "hw/hw-03.html#exercise-11",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 11",
    "text": "Exercise 11\nCalculate RMSE on the training data and on the testing data. Use it to comment on how well the model performs on new data and whether there are signs of model overfit."
  },
  {
    "objectID": "hw/hw-03.html#exercise-12",
    "href": "hw/hw-03.html#exercise-12",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 12",
    "text": "Exercise 12\nThough we do not check the model conditions in this assignment, complete this exercise assuming the model conditions are met.\nDescribe the effect of Theme on the price of LEGO sets, including an indication of which levels are statistically significant. Use a threshold of 0.05 to determine significance.\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-03.html#exercise-13",
    "href": "hw/hw-03.html#exercise-13",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 13",
    "text": "Exercise 13\nThe World Bank collects ‚Äúworld development indicators‚Äù about the past and current development of countries. These data are made available on the World Bank‚Äôs website. It can be used to understand the relationships between these various factors and trends over time.\n\nFor this analysis, we focus on indicators from 2011 on 165 countries. The variables of interest are:\n\ngdp.per.capita: gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.\nsanit.access.factor: Population access to sanitation facilities (Low, High)\nedu.expend: Government expenditure on education, total (% of government expenditure)\nlife.expect: Life expectancy at birth (in years)\n\nYou fit a model using sanitation access, education expenditures, and life expectancy to understand variability in GDP. The model takes the form\n\\[\n\\begin{aligned}\\widehat{\\log(GDP)} = \\hat{\\beta}_0 &+ \\hat{\\beta}_1 ~ sanit.access.factor + \\hat{\\beta}_2 ~ edu.expend + \\hat{\\beta}_3 ~life.expect \\\\ &+ \\hat{\\beta}_4 ~ sanit.access.factor \\times life.expect\\end{aligned}\n\\]\nThe output from R is shown below.\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.491\n1.638\n2.742\n0.007\n\n\nsanit.access.factorhigh\n-6.993\n1.971\n-3.548\n0.001\n\n\nedu.expend\n0.097\n0.038\n2.550\n0.012\n\n\nlife.expect\n0.030\n0.029\n1.061\n0.291\n\n\nsanit.access.factorhigh:life.expect\n0.122\n0.032\n3.853\n0.000\n\n\n\n\n\n\nInterpret the coefficient of edu.expend in the context of the data.\nInterpret the coefficient of life.expect for countries with high sanitation access in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nWrite all interpretations in terms of the original units, not the log-transformed units."
  },
  {
    "objectID": "hw/hw-03.html#exercise-14",
    "href": "hw/hw-03.html#exercise-14",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 14",
    "text": "Exercise 14\nBelow are plots from the exploratory data analysis of the relationships between the predictor variables. Based on these plots, what appears to be a potential issue with the model from Exercise 13? Briefly explain your response."
  },
  {
    "objectID": "hw/hw-03.html#footnotes",
    "href": "hw/hw-03.html#footnotes",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.‚Ü©Ô∏é‚Ü©Ô∏é"
  },
  {
    "objectID": "prepare/oct-09.html",
    "href": "prepare/oct-09.html",
    "title": "October 09 & 11 lectures",
    "section": "",
    "text": "üìñ Tidy Modeling withr R, Ch 8:Feature engineering with recipes"
  },
  {
    "objectID": "prepare/oct-09.html#assigned-reading",
    "href": "prepare/oct-09.html#assigned-reading",
    "title": "October 09 & 11 lectures",
    "section": "",
    "text": "üìñ Tidy Modeling withr R, Ch 8:Feature engineering with recipes"
  },
  {
    "objectID": "prepare/sep-13.html",
    "href": "prepare/sep-13.html",
    "title": "September 13 lecture",
    "section": "",
    "text": "üìñ IMS, Sec 24.2: Randomization test for the slope\nüìñ IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "prepare/sep-13.html#assigned-reading",
    "href": "prepare/sep-13.html#assigned-reading",
    "title": "September 13 lecture",
    "section": "",
    "text": "üìñ IMS, Sec 24.2: Randomization test for the slope\nüìñ IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html",
    "href": "ae/ae-11-cross-validation.html",
    "title": "AE 11: Cross validation",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-11 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 26 at 11:59pm."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-statistics-function",
    "href": "ae/ae-11-cross-validation.html#model-statistics-function",
    "title": "AE 11: Cross validation",
    "section": "Model statistics function",
    "text": "Model statistics function\nYou will use this function to calculate \\(Adj. R^2\\), AIC, and BIC in the cross validation.\n\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#packages",
    "href": "ae/ae-11-cross-validation.html#packages",
    "title": "AE 11: Cross validation",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#load-data-and-relevel-factors",
    "href": "ae/ae-11-cross-validation.html#load-data-and-relevel-factors",
    "title": "AE 11: Cross validation",
    "section": "Load data and relevel factors",
    "text": "Load data and relevel factors\n\ntips &lt;- read_csv(\"data/tip-data.csv\")\n\ntips &lt;- tips |&gt;\n  mutate(Age = factor(Age, levels = c(\"Yadult\", \"Middle\", \"SenCit\")), \n         Meal = factor(Meal, levels = c(\"Lunch\", \"Dinner\", \"Late Night\"))\n  )"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#split-data-into-training-and-testing",
    "href": "ae/ae-11-cross-validation.html#split-data-into-training-and-testing",
    "title": "AE 11: Cross validation",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\nset.seed(10232023)\ntips_split &lt;- initial_split(tips)\ntips_train &lt;- training(tips_split)\ntips_test &lt;- testing(tips_split)"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#specify-model",
    "href": "ae/ae-11-cross-validation.html#specify-model",
    "title": "AE 11: Cross validation",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it tips_spec.\n\ntips_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\ntips_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#create-recipe",
    "href": "ae/ae-11-cross-validation.html#create-recipe",
    "title": "AE 11: Cross validation",
    "section": "Create recipe",
    "text": "Create recipe\nCreate a recipe to use Party, Age, and Meal to predict Tip. Call it tips_rec1.\n\ntips_rec1 &lt;- recipe(Tip ~ Party + Age + Meal,\n                    data = tips_train) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\ntips_rec1\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()\n\n\n‚Ä¢ Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#preview-recipe",
    "href": "ae/ae-11-cross-validation.html#preview-recipe",
    "title": "AE 11: Cross validation",
    "section": "Preview recipe",
    "text": "Preview recipe\n\nprep(tips_rec1) |&gt;\n  bake(tips_train) |&gt;\n  glimpse()\n\nRows: 126\nColumns: 6\n$ Party           &lt;dbl&gt; 3, 2, 2, 4, 2, 7, 4, 3, 2, 4, 1, 2, 2, 1, 2, 1, 2, 3, ‚Ä¶\n$ Tip             &lt;dbl&gt; 4.00, 4.92, 5.09, 8.84, 3.09, 15.00, 8.00, 4.00, 5.00,‚Ä¶\n$ Age_Middle      &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, ‚Ä¶\n$ Age_SenCit      &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ Meal_Dinner     &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, ‚Ä¶\n$ Meal_Late.Night &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, ‚Ä¶"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#create-workflow",
    "href": "ae/ae-11-cross-validation.html#create-workflow",
    "title": "AE 11: Cross validation",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it tips_wflow1.\n\ntips_wflow1 &lt;- workflow() |&gt;\n  add_model(tips_spec) |&gt;\n  add_recipe(tips_rec1)\n\ntips_wflow1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n2 Recipe Steps\n\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#create-folds",
    "href": "ae/ae-11-cross-validation.html#create-folds",
    "title": "AE 11: Cross validation",
    "section": "Create folds",
    "text": "Create folds\nCreate 5 folds.\n\n# make 10 folds\nset.seed(10232023)\nfolds &lt;- vfold_cv(tips_train, v = 5)"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#conduct-cross-validation",
    "href": "ae/ae-11-cross-validation.html#conduct-cross-validation",
    "title": "AE 11: Cross validation",
    "section": "Conduct cross validation",
    "text": "Conduct cross validation\nConduct cross validation on the 5 folds.\n\n# Fit model and performance statistics for each iteration\ntips_fit_rs1 &lt;- tips_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#take-a-look-at-tips_fit_rs1",
    "href": "ae/ae-11-cross-validation.html#take-a-look-at-tips_fit_rs1",
    "title": "AE 11: Cross validation",
    "section": "Take a look at tips_fit_rs1",
    "text": "Take a look at tips_fit_rs1\n\ntips_fit_rs1\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 √ó 5\n  splits           id    .metrics         .notes           .extracts       \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [100/26]&gt; Fold1 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n2 &lt;split [101/25]&gt; Fold2 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n3 &lt;split [101/25]&gt; Fold3 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n4 &lt;split [101/25]&gt; Fold4 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n5 &lt;split [101/25]&gt; Fold5 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#summarize-assessment-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#summarize-assessment-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Summarize assessment CV metrics",
    "text": "Summarize assessment CV metrics\nSummarize assessment metrics from your CV iterations These statistics are calculated using the assessment set.\n\ncollect_metrics(tips_fit_rs1, summarize = TRUE)\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.09      5  0.265  Preprocessor1_Model1\n2 rsq     standard   0.673     5  0.0519 Preprocessor1_Model1\n\n\n\n\n\n\n\n\nTip\n\n\n\nSet summarize = FALSE to see the individual \\(R^2\\) and RMSE for each iteration."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#summarize-model-fit-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#summarize-model-fit-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Summarize model fit CV metrics",
    "text": "Summarize model fit CV metrics\nSummarize model fit statistics from your CV iterations These statistics are calculated using the analysis set.\n\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  summarise(mean_adj_rsq = mean(adj.r.squared), \n            mean_aic = mean(AIC), \n            mean_bic = mean(BIC))\n\n# A tibble: 1 √ó 3\n  mean_adj_rsq mean_aic mean_bic\n         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        0.670     434.     453.\n\n\n\n\n\n\n\n\nTip\n\n\n\nRun the first line of code map_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) to see the individual \\(Adj. R^2\\), AIC, and BIC for each iteration."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-recipe",
    "href": "ae/ae-11-cross-validation.html#model-2-recipe",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Recipe",
    "text": "Model 2: Recipe\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-model-building-workflow",
    "href": "ae/ae-11-cross-validation.html#model-2-model-building-workflow",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Model building workflow",
    "text": "Model 2: Model building workflow\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-conduct-cv",
    "href": "ae/ae-11-cross-validation.html#model-2-conduct-cv",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Conduct CV",
    "text": "Model 2: Conduct CV\n\n\n\n\n\n\nNote\n\n\n\nWe will use the same folds as the ones used for Model 1. Why should we use the same folds to evaluate and compare both models?\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-summarize-assessment-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#model-2-summarize-assessment-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Summarize assessment CV metrics",
    "text": "Model 2: Summarize assessment CV metrics\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-summarize-model-fit-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#model-2-summarize-model-fit-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Summarize model fit CV metrics",
    "text": "Model 2: Summarize model fit CV metrics\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#fit-the-selected-model",
    "href": "ae/ae-11-cross-validation.html#fit-the-selected-model",
    "title": "AE 11: Cross validation",
    "section": "Fit the selected model",
    "text": "Fit the selected model\nFit the selected model using the entire training set.\n\n# add code here\n\n\n\n\n\n\n\nTip\n\n\n\nSee notes for example code."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#evaluate-the-performance-of-the-selected-model-on-the-testing-data",
    "href": "ae/ae-11-cross-validation.html#evaluate-the-performance-of-the-selected-model-on-the-testing-data",
    "title": "AE 11: Cross validation",
    "section": "Evaluate the performance of the selected model on the testing data",
    "text": "Evaluate the performance of the selected model on the testing data\n\nCalculate predicted values\n\n# add code here\n\n\n\nCalculate \\(RMSE\\)\n\n# add code here\n\n\n\n\n\n\n\nTip\n\n\n\nSee notes notes for example code.\n\n\n\nHow does the model performance on the testing data compare to its performance on the training data?\nIs this what you expected? Why or why not?"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html",
    "href": "ae/ae-10-model-workflow.html",
    "title": "AE 10: Model workflow",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-10 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, October 14 at 11:59pm."
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#packages-data",
    "href": "ae/ae-10-model-workflow.html#packages-data",
    "title": "AE 10: Model workflow",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\nThe data for this AE is from the loan50 data set in the openintro R package. We will focus on the following variables:\nPredictors\n\nannual_income: Annual income (in US dollars)\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nResponse\n\ninterest_rate: Interest rate for the loan (0- 100)"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#analysis-goal",
    "href": "ae/ae-10-model-workflow.html#analysis-goal",
    "title": "AE 10: Model workflow",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis are to build a recipe to fit a linear regression model on the training data that has the following features:\n\nannual_income rescaled to thousands of dollars\n\nDo not include the original variable annual_income in the model\n\nMean-centered quantitative variables\nIndicator (dummy) variables for the categorical predictor\nInteraction term between rescaled annual_income and verified_income\n\nand (2) use prep() and bake() to check the recipe"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#relevel-verified_income",
    "href": "ae/ae-10-model-workflow.html#relevel-verified_income",
    "title": "AE 10: Model workflow",
    "section": "Relevel verified_income",
    "text": "Relevel verified_income\nMake Verified the baseline level for the model.\n\nloan50 &lt;- loan50 |&gt;\n  mutate(verified_income = factor(verified_income,\n                                  levels = c(\"Verified\", \"Not Verified\", \n                                             \"Source Verified\")))"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#testtrain-split",
    "href": "ae/ae-10-model-workflow.html#testtrain-split",
    "title": "AE 10: Model workflow",
    "section": "Test/train split",
    "text": "Test/train split\nSplit the data into 90% training, 10% testing.\n\nset.seed(123)\n\nloan_split &lt;- initial_split(loan50, prop = 0.9)\nloan_train &lt;- training(loan_split)\nloan_test  &lt;- testing(loan_split)"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#build-and-troubleshoot-recipe",
    "href": "ae/ae-10-model-workflow.html#build-and-troubleshoot-recipe",
    "title": "AE 10: Model workflow",
    "section": "Build (and troubleshoot) recipe",
    "text": "Build (and troubleshoot) recipe\n\nUse step_mutate() to create a new variable annual_income_th that is annual_income rescaled to thousands of dollars\nUse step_rm() to remove annual_income from the model\nUse step_center() to mean-center quantitative variables\nUse step_dummy() to create indicator variables for the categorical predictor\nUse step_interact() to create interaction between annual_income_th and verified_income\n\n\nloan_rec &lt;-  recipe(interest_rate ~ annual_income + debt_to_income + verified_income, \n                    data = loan_train) |&gt;\n  step_mutate(annual_income_th = annual_income / 1000) |&gt;\n  step_rm(annual_income) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_interact(terms = ~ annual_income_th:verified_income) \n\n\nloan_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: annual_income / 1000\n\n\n‚Ä¢ Variables removed: annual_income\n\n\n‚Ä¢ Centering for: all_numeric_predictors()\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()\n\n\n‚Ä¢ Interactions with: annual_income_th:verified_income\n\n\n\nCheck recipe using prep() and bake()\nOnce you‚Äôve corrected the code, remove #| eval: false before rendering the document.\n\nloan_rec |&gt;\n  prep() |&gt;\n  bake(loan_train) |&gt;\n  glimpse()\n\n\nIn which step do we have an error?\nClick here to access the recipes reference page. Find the reference page for the relevant step_ function.\nSee the examples at the bottom of the reference page. Which model most closely aligns the interaction we‚Äôre trying to create?\nUse example to help you fix the code. Then, use prep() and bake() to see the updated results."
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#workflows-and-model-fitting",
    "href": "ae/ae-10-model-workflow.html#workflows-and-model-fitting",
    "title": "AE 10: Model workflow",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\n\nSpecify model\n\nloan_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nloan_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\nBuild workflow\n\nloan_wflow &lt;- workflow() |&gt;\n  add_model(loan_spec) |&gt;\n  add_recipe(loan_rec)\n\nloan_wflow\n\n\n\nFit model to training data\nRemove #| eval: false before rendering the document.\n\nloan_fit &lt;- loan_wflow |&gt;\n  fit(data = loan_train)\n\ntidy(loan_fit) |&gt;\n  kable(digits = 3)"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#evaluate-model-on-training-data",
    "href": "ae/ae-10-model-workflow.html#evaluate-model-on-training-data",
    "title": "AE 10: Model workflow",
    "section": "Evaluate model on training data",
    "text": "Evaluate model on training data\n\nMake predictions\nFill in the code and remove #| eval: false before rendering the document.\n\nloan_train_pred &lt;- predict(loan_fit, ______) |&gt;\n  bind_cols(_____)\n\n\n\nCalculate \\(R^2\\)\nFill in the code and remove #| eval: false before rendering the document.\n\nrsq(loan_train_pred, truth = _____, estimate = _____)\n\n\n\nCalculate RMSE\nFill in the code and remove #| eval: false before rendering the document.\n\nrmse(______, ________, ________)\n\n\nIs this RMSE considered high or low? Hint: Consider the range of the response variable to answer this question.\n\nloan_train |&gt;\n  summarise(min = min(interest_rate), max = max(interest_rate))\n\n# A tibble: 1 √ó 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1  5.31  26.3"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#evaluate-model-on-testing-data",
    "href": "ae/ae-10-model-workflow.html#evaluate-model-on-testing-data",
    "title": "AE 10: Model workflow",
    "section": "Evaluate model on testing data",
    "text": "Evaluate model on testing data\n\nMake predictions\n\n# fill in code to make predictions from testing data\n\n\n\nCalculate \\(R^2\\)\n\n# fill in code to calculate R-sq for testing data\n\n\n\nCalculate RMSE\n\n# fill in code to calculate RMSE for testing data"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#compare-training-and-testing-data-results",
    "href": "ae/ae-10-model-workflow.html#compare-training-and-testing-data-results",
    "title": "AE 10: Model workflow",
    "section": "Compare training and testing data results",
    "text": "Compare training and testing data results\n\nCompare the \\(R^2\\) for the training and testing data. Is this what you expected?\nCompare the RMSE for the training and testing data. Is this what you expected?"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#to-submit-the-ae",
    "href": "ae/ae-10-model-workflow.html#to-submit-the-ae",
    "title": "AE 10: Model workflow",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-10 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-06-conditions.html",
    "href": "ae/ae-06-conditions.html",
    "title": "AE 06: Conditions and model evaluation",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-06 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, September 23 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-06-conditions.html#data",
    "href": "ae/ae-06-conditions.html#data",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.¬†happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.¬†sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"data/spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we‚Äôll analyze data on some of the most popular songs on Spotify, i.e.¬†those with track_popularity &gt;= 80. We‚Äôll use linear regression to fit a model to predict a song‚Äôs positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- ggplot(data = spotify, aes(x = valence)) + \n  geom_histogram() + \n  labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- ggplot(data = spotify, aes(x = energy)) + \n  geom_histogram() + \n  labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2\n\n\n\n\n\nggplot(data = spotify, aes(x = energy, y = valence)) + \n  geom_point() + \n  labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-1",
    "href": "ae/ae-06-conditions.html#exercise-1",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model using the energy of a song to predict its valence, i.e.¬†positiveness. Include the 90% confidence interval for the coefficients, and display the output using 3 digits.\n\n## add code"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-2",
    "href": "ae/ae-06-conditions.html#exercise-2",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet‚Äôs check the model conditions before doing any inference. Fill in the code below to use the augment() function to create a new data frame containing the residuals and fitted values (among other information)\n\n\n\n\n\n\nImportant\n\n\n\nNote: Remove #|eval: false from the code chunk after you have filled in the code.\n\n\n\nspotify_aug &lt;- augment(_____)"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-3",
    "href": "ae/ae-06-conditions.html#exercise-3",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 3",
    "text": "Exercise 3\nMake a plot of the residual vs.¬†fitted values.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-4",
    "href": "ae/ae-06-conditions.html#exercise-4",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to make a histogram of the residuals and a normal QQ-plot.\n\nresid_hist &lt;- ggplot(data = ____, aes(x = ____)) +\n  geom_histogram() +\n  labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_qq &lt;- ggplot(data = ____, aes(sample = ____)) + \n  stat_qq() + \n ____() + \n  labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-5",
    "href": "ae/ae-06-conditions.html#exercise-5",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 5",
    "text": "Exercise 5\nAssess the four model conditions. Use the plots from the previous exercises to help make the assessment.\n\nLinearity\nConstant variance\nNormality\nIndependence"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-6",
    "href": "ae/ae-06-conditions.html#exercise-6",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 6",
    "text": "Exercise 6\nCalculate \\(R^2\\) and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-7",
    "href": "ae/ae-06-conditions.html#exercise-7",
    "title": "AE 06: Conditions and model evaluation",
    "section": "Exercise 7",
    "text": "Exercise 7\nCalculate RMSE and interpret this value in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-06 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-05-sim-testing.html",
    "href": "ae/ae-05-sim-testing.html",
    "title": "AE 05: Randomization test for the slope",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-05 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, September 16 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-05-sim-testing.html#data",
    "href": "ae/ae-05-sim-testing.html#data",
    "title": "AE 05: Randomization test for the slope",
    "section": "Data",
    "text": "Data\nThe data are on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. It was originally scraped from Zillow, and can be found in the duke_forest data set in the openintro R package.\n\nglimpse(duke_forest)\n\nRows: 98\nColumns: 13\n$ address    &lt;chr&gt; \"1 Learned Pl, Durham, NC 27705\", \"1616 Pinecrest Rd, Durha‚Ä¶\n$ price      &lt;dbl&gt; 1520000, 1030000, 420000, 680000, 428500, 456000, 1270000, ‚Ä¶\n$ bed        &lt;dbl&gt; 3, 5, 2, 4, 4, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 5, 3, 4, 4, 3,‚Ä¶\n$ bath       &lt;dbl&gt; 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 3.0,‚Ä¶\n$ area       &lt;dbl&gt; 6040, 4475, 1745, 2091, 1772, 1950, 3909, 2841, 3924, 2173,‚Ä¶\n$ type       &lt;chr&gt; \"Single Family\", \"Single Family\", \"Single Family\", \"Single ‚Ä¶\n$ year_built &lt;dbl&gt; 1972, 1969, 1959, 1961, 2020, 2014, 1968, 1973, 1972, 1964,‚Ä¶\n$ heating    &lt;chr&gt; \"Other, Gas\", \"Forced air, Gas\", \"Forced air, Gas\", \"Heat p‚Ä¶\n$ cooling    &lt;fct&gt; central, central, central, central, central, central, centr‚Ä¶\n$ parking    &lt;chr&gt; \"0 spaces\", \"Carport, Covered\", \"Garage - Attached, Covered‚Ä¶\n$ lot        &lt;dbl&gt; 0.97, 1.38, 0.51, 0.84, 0.16, 0.45, 0.94, 0.79, 0.53, 0.73,‚Ä¶\n$ hoa        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ url        &lt;chr&gt; \"https://www.zillow.com/homedetails/1-Learned-Pl-Durham-NC-‚Ä¶"
  },
  {
    "objectID": "ae/ae-05-sim-testing.html#exploratory-data-analysis",
    "href": "ae/ae-05-sim-testing.html#exploratory-data-analysis",
    "title": "AE 05: Randomization test for the slope",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "ae/ae-05-sim-testing.html#model",
    "href": "ae/ae-05-sim-testing.html#model",
    "title": "AE 05: Randomization test for the slope",
    "section": "Model",
    "text": "Model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-05-sim-testing.html#hypothesis-test",
    "href": "ae/ae-05-sim-testing.html#hypothesis-test",
    "title": "AE 05: Randomization test for the slope",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\n\n\n\n\n\nTip\n\n\n\nFor code chunks with fill-in-the-blank code, change code chunk option to #| eval: true once you‚Äôve filled in the code.\n\n\n\nState the null and alternative hypotheses\n[Add hypotheses in mathematical notation]\n\n\nGenerate null distribution using permutation\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(09132023)\n\nnull_dist &lt;- _____ |&gt;\n  specify(______) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = _____, type = \"permute\") |&gt;\n  fit()\n\n\n\nVisualize distribution\n\n# Code for histogram of null distribution\n\n\n\nCalculate the p-value.\n\n# get observed fit \nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\n# calculate p-value\nget_p_value(\n  ____,\n  obs_stat = ____,\n  direction = \"two-sided\"\n)\n\n\nWhat does the warning message mean?\n\n\n\nState conclusion\n[Write your conclusion in the context of the data.]\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-05 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html",
    "href": "ae/ae-08-exam-01-review.html",
    "title": "AE 08: Exam 01 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-08 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 5 at 11:59pm."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#packages",
    "href": "ae/ae-08-exam-01-review.html#packages",
    "title": "AE 08: Exam 01 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#restaurant-tips",
    "href": "ae/ae-08-exam-01-review.html#restaurant-tips",
    "title": "AE 08: Exam 01 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St.¬†Olaf who worked at a local restaurant.1\nThe variables we‚Äôll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exploratory-data-analysis",
    "href": "ae/ae-08-exam-01-review.html#exploratory-data-analysis",
    "title": "AE 08: Exam 01 Review",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\np1 &lt;- ggplot(data = tips, aes(x = Tip)) + \n  geom_histogram(color = \"white\", binwidth = 2) +\n  labs(x = \"Tips ($)\",\n       title = \"Tips at local restaurant\")\n\np2 &lt;- ggplot(data = tips, aes(x = Party)) + \n  geom_histogram(color = \"white\") +\n  labs(x = \"Party\",\n       title = \"Number of diners in party\") +\n  xlim(c(0, 7))\n\np3 &lt;- ggplot(data = tips, aes(x = Party, y = Tip)) + \n  geom_jitter() + \n  labs(x = \"Number of diners in party\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Party\")\n\n(p1 + p2) / p3\n\n\n\n\nThe goal is to fit a model that uses the number of diners in the party to understand variability in the tips."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-1",
    "href": "ae/ae-08-exam-01-review.html#exercise-1",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nWrite the statistical model.\n\n\n\n[add answer here]"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-2",
    "href": "ae/ae-08-exam-01-review.html#exercise-2",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe regression output with 90% confidence intervals for the coefficients is shown below.\n\ntips_fit &lt;- linear_reg() |&gt;\n  fit(Tip ~ Party, data = tips)\n\ntidy(tips_fit, conf.int = TRUE, conf.level = 0.9) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.383\n0.321\n1.195\n0.234\n-0.147\n0.913\n\n\nParty\n1.957\n0.118\n16.553\n0.000\n1.761\n2.152\n\n\n\n\n\n\nglance(tips_fit)$sigma\n\n[1] 2.082824\n\n\n\n\n\n\n\n\nWrite the regression equation.\n\n\n\n[add answer here]"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-3",
    "href": "ae/ae-08-exam-01-review.html#exercise-3",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nThe regression standard error is 2.083. Write the definition of this value in the context of the data.\n\n\n\n[add answer here]"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-4",
    "href": "ae/ae-08-exam-01-review.html#exercise-4",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\n\ntips_aug &lt;- augment(tips_fit$fit)\n\nrsq(tips_aug, truth = Tip, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.621\n\nrmse(tips_aug, truth = Tip, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        2.07\n\n\n\n\n\n\n\n\n\nInterpret \\(R^2\\) in the context of the data.\nInterpret RMSE in the context of the data.\nBased on these values, do you think the model is a good fit for the data? Why or why not?\n\n\n\n\n[add answer here]"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-5",
    "href": "ae/ae-08-exam-01-review.html#exercise-5",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we‚Äôll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\n\n\nset.seed(1234)\n\n1boot_dist &lt;- tips |&gt;\n2  specify(Tip ~ Party) |&gt;\n3  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\n\n1\n\n___________\n\n2\n\n___________\n\n3\n\n___________"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-6",
    "href": "ae/ae-08-exam-01-review.html#exercise-6",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nUse the bootstrap distribution created in the previous exercise, boot_dist, to construct a 90% confidence interval for the slope. Interpret the interval in the context of the data.\n\n\n\n\n# add your code here\n\nConduct a hypothesis test using permutation with 100 reps. State the hypotheses in words and mathematical notation. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\nUse \\(\\alpha = 0.1\\) as the decision-making threshold for rejecting or failing to reject the null hypothesis.\n\nset.seed(1234)\n# add your code here"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-7",
    "href": "ae/ae-08-exam-01-review.html#exercise-7",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nCheck the relevant conditions for Exercises 5 and 6. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-8",
    "href": "ae/ae-08-exam-01-review.html#exercise-8",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nNow repeat Exercises 5 and 6 using approaches based on mathematical models. You can reference output from previous exercises and/or write new code as needed.\n\n\n\n[add answer here]"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-9",
    "href": "ae/ae-08-exam-01-review.html#exercise-9",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nNote\n\n\n\nCheck the relevant conditions for Exercise 8. Are there any violations in conditions that make you reconsider your inferential findings? You can reference previous graphs / conditions and add any new code as needed.\n\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-10",
    "href": "ae/ae-08-exam-01-review.html#exercise-10",
    "title": "AE 08: Exam 01 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\n\nBased on your model, predict the tip for a party of 4.\nSuppose you‚Äôre asked to construct a 95% confidence and a 95% prediction interval for your finding in the previous exercise. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals and comment on whether your guess is confirmed.\n\n\n\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#to-submit-the-ae",
    "href": "ae/ae-08-exam-01-review.html#to-submit-the-ae",
    "title": "AE 08: Exam 01 Review",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-08 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#footnotes",
    "href": "ae/ae-08-exam-01-review.html#footnotes",
    "title": "AE 08: Exam 01 Review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. ‚ÄúThe Effects of Credit Cards on Tipping.‚Äù Project for Statistics 212-Statistics for the Sciences, St.¬†Olaf College.‚Ü©Ô∏é"
  },
  {
    "objectID": "ae/ae-07-model-eval.html",
    "href": "ae/ae-07-model-eval.html",
    "title": "AE 07: Model evaluation",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-07 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, September 28 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#data",
    "href": "ae/ae-07-model-eval.html#data",
    "title": "AE 07: Model evaluation",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.¬†happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.¬†sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"data/spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we‚Äôll analyze data on some of the most popular songs on Spotify, i.e.¬†those with track_popularity &gt;= 80. We‚Äôll use linear regression to fit a model to predict a song‚Äôs positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- ggplot(data = spotify, aes(x = valence)) + \n  geom_histogram() + \n  labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- ggplot(data = spotify, aes(x = energy)) + \n  geom_histogram() + \n  labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2\n\n\n\n\n\nggplot(data = spotify, aes(x = energy, y = valence)) + \n  geom_point() + \n  labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#model-with-90-ci-for-coefficients",
    "href": "ae/ae-07-model-eval.html#model-with-90-ci-for-coefficients",
    "title": "AE 07: Model evaluation",
    "section": "Model with 90% CI for coefficients",
    "text": "Model with 90% CI for coefficients\n\nspotify_fit &lt;- linear_reg() |&gt;\n  fit(valence ~ energy, data = spotify)\n\ntidy(spotify_fit, conf.int = TRUE, conf.level = 0.9) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.121\n0.035\n3.401\n0.001\n0.062\n0.179\n\n\nenergy\n0.614\n0.054\n11.321\n0.000\n0.525\n0.703"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#create-augmented-data-frame",
    "href": "ae/ae-07-model-eval.html#create-augmented-data-frame",
    "title": "AE 07: Model evaluation",
    "section": "Create augmented data frame",
    "text": "Create augmented data frame\n\nspotify_aug &lt;- augment(spotify_fit$fit)"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#create-residual-plots",
    "href": "ae/ae-07-model-eval.html#create-residual-plots",
    "title": "AE 07: Model evaluation",
    "section": "Create residual plots",
    "text": "Create residual plots\n\nggplot(data = spotify_aug, aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Fitted values\", \n       y = \"Residuals\", \n       title = \"Residuals vs. fitted\")\n\n\n\n\n\nresid_hist &lt;- ggplot(data = spotify_aug, aes(x = .resid)) + \n  geom_histogram() + \n  labs(x = \"Residuals\", \n       title = \"Distribution of residuals\")\n\nresid_qq &lt;- ggplot(data = spotify_aug, aes(sample = .resid)) +\n  stat_qq() + \n  stat_qq_line() +\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed residuals\", \n       title = \"Normal QQ-plot\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#exercise-1",
    "href": "ae/ae-07-model-eval.html#exercise-1",
    "title": "AE 07: Model evaluation",
    "section": "Exercise 1",
    "text": "Exercise 1\nAssess the four model conditions. State whether the condition is satisfied and briefly explain your response.\n\nLinearity\nConstant variance\nNormality\nIndependence"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#exercise-2",
    "href": "ae/ae-07-model-eval.html#exercise-2",
    "title": "AE 07: Model evaluation",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate \\(R^2\\) and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/ae-07-model-eval.html#exercise-3",
    "href": "ae/ae-07-model-eval.html#exercise-3",
    "title": "AE 07: Model evaluation",
    "section": "Exercise 3",
    "text": "Exercise 3\nCalculate RMSE and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/ae-07-model-eval.html#evaluate-response-from-chatgpt",
    "href": "ae/ae-07-model-eval.html#evaluate-response-from-chatgpt",
    "title": "AE 07: Model evaluation",
    "section": "Evaluate response from ChatGPT",
    "text": "Evaluate response from ChatGPT\nI typed the following prompt into ChatGPT (‚Äúan AI powered language model‚Äù) ‚ÄúHow do you check the model conditions for simple linear regression?‚Äù\nThe response from ChatGPT shown below would receive partial credit. For each condition, state whether it is (1) correct, (2) incorrect and if so, what is wrong, (3) incomplete and if so, what is missing.\n\nIn simple linear regression, you‚Äôre trying to establish a linear relationship between two variables: a dependent variable (Y) and an independent variable (X). The model conditions or assumptions for simple linear regression are important to check to ensure that your model is valid and reliable. Here are the key assumptions and how to check them:\n1. Linearity Assumption:\n\nAssumption: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nHow to Check: Create a scatterplot of Y against X. If the points roughly form a straight line, the linearity assumption is likely met.\n\n2. Independence Assumption:\n\nAssumption: The residuals (the differences between observed Y and predicted Y) are independent of each other.\nHow to Check: Plot the residuals against the independent variable (X) or the predicted values. Look for patterns or trends in the residuals. There should be no clear structure or autocorrelation in the residuals.\n\n3. Homoscedasticity [Constant variance] Assumption:\n\nAssumption: The variance of the residuals is constant across all levels of the independent variable.\nHow to Check: Plot the residuals against the predicted values or the independent variable (X). The spread of the residuals should remain roughly constant across the range of X.\n\n4. Normality Assumption:\n\nAssumption: The residuals follow a normal distribution.\nHow to Check: Create a histogram or a normal probability plot (Q-Q plot) of the residuals. If the residuals are approximately normally distributed, they should follow a bell-shaped curve in the histogram and lie along a straight line in the Q-Q plot.\n\nIt‚Äôs important to note that these assumptions may not always hold perfectly in real-world data, and minor violations may not necessarily invalidate your model. However, if you detect significant violations of these assumptions, you may need to consider alternative modeling techniques or transformations of your data to address the issues.\n\n\n\n\n\n\n\nNote\n\n\n\nYou will start to think about the answer in class and come up with a final answer with your group in Lab 04."
  },
  {
    "objectID": "ae/ae-07-model-eval.html#to-submit-the-ae",
    "href": "ae/ae-07-model-eval.html#to-submit-the-ae",
    "title": "AE 07: Model evaluation",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-07 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-02-bikeshare.html",
    "href": "ae/ae-02-bikeshare.html",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "",
    "text": "Important\n\n\n\nFor this AE, you will discuss the questions in groups and submit answers on Ed Discussion. This AE does not count towards the Application Exercise grade.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#data",
    "href": "ae/ae-02-bikeshare.html#data",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-02-bikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\np1 &lt;- ggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250) + \n  labs(x = \"Daily bike rentals\")\n\np2 &lt;- ggplot(bikeshare, aes(x = temp_orig)) +\n  geom_histogram() + \n  labs(x = \"Temperature (Celsius)\")\n\np3 &lt;- ggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point() + \n  labs(x = \"Temperature (Celsius)\", \n       y = \"Daily bike rentals\")\n\n(p1 | p2) / p3\n\n\n\n\n\n\nExercise 2\nDescribe the relationship between daily bike rentals and temperature. Comment on how we expect the number of bike rentals to change as the temperature increases.\n\n\nExercise 3\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\n\\[\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp\\_orig} + \\epsilon\\]\nbe an appropriate fit for the data? Why or why not?\nPut your group‚Äôs vote on Ed Discussion and briefly describe your reasoning in the comments.\n\nSection 001 (10:05am): edstem.org/us/courses/44523/discussion/3361086\nSection 002 (1:25pm): edstem.org/us/courses/44523/discussion/3361091"
  },
  {
    "objectID": "project-instructions.html",
    "href": "project-instructions.html",
    "title": "Final project",
    "section": "",
    "text": "Project proposal\n\ndue Friday, October 27 (Tuesday labs)\ndue Sunday, October 29 (Thursday labs)\n\nDraft report + peer review\n\ndue Tuesday, November 14 (Tuesday labs)\ndue Thursday, November 16 (Thursday labs)\n\nRound 1 submission (optional) due Friday, December 1\nPresentation + Presentation comments\n\nTuesday, December 5 (Tuesday labs)\nThursday, December 7 (Thursday labs)\n\nWritten report due Wednesday, December 13\nReproducibility + organization due Wednesday, December 13"
  },
  {
    "objectID": "project-instructions.html#timeline",
    "href": "project-instructions.html#timeline",
    "title": "Final project",
    "section": "",
    "text": "Project proposal\n\ndue Friday, October 27 (Tuesday labs)\ndue Sunday, October 29 (Thursday labs)\n\nDraft report + peer review\n\ndue Tuesday, November 14 (Tuesday labs)\ndue Thursday, November 16 (Thursday labs)\n\nRound 1 submission (optional) due Friday, December 1\nPresentation + Presentation comments\n\nTuesday, December 5 (Tuesday labs)\nThursday, December 7 (Thursday labs)\n\nWritten report due Wednesday, December 13\nReproducibility + organization due Wednesday, December 13"
  },
  {
    "objectID": "project-instructions.html#introduction",
    "href": "project-instructions.html#introduction",
    "title": "Final project",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group‚Äôs interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible.\n\nLogistics\nYou will work on the project with your lab groups. The four primary deliverables for the final project are\n\na written, reproducible report detailing your analysis\na GitHub repository corresponding to your report\nslides and an in-person presentation\nformal peer review on another team‚Äôs work and presentation feedback"
  },
  {
    "objectID": "project-instructions.html#project-proposal",
    "href": "project-instructions.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\n\n\n\n\n\n\nDue dates\n\n\n\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\n\n\nThe purpose of the project proposal is for your team to identify the data set you‚Äôre interested in analyzing for the project, do some preliminary exploratory data analysis, and begin to think about a modeling strategy . If you‚Äôre unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point. It may also help to think of topics you‚Äôre interested in investigating and find data sets on those topics.\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as ‚Äúname‚Äù, ‚ÄúID number‚Äù, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g.¬†‚Äústate abbreviation‚Äù and ‚Äústate name‚Äù), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTypes of data sets to avoid\n\n\n\n\nData that are likely violate the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nData sets in which there is no information about how the data were originally collected\nData sets in which there are missing or unclear definitions about the observations and/or variables\n\n\n\nAsk a member of the teaching team if you‚Äôre unsure whether your data set meets the criteria.\nThe proposal will include the following sections:\n\nSection 1: Introduction\nThe introduction section includes\n\nan introduction to the subject matter you‚Äôre investigating (citing any relevant literature)\nthe motivation for your research question (citing any relevant literature)\nthe primary research question you are interested in exploring\nyour team‚Äôs hypotheses regarding the research question\n\nThis is a narrative about what you think regarding the research question, not formal statistical hypotheses.\n\n\n\n\nSection 2: Data description\nThe data description section includes\n\nthe source of the data set\na description of when and how the data were originally collected (by the original data curator, not necessarily how you found the data)\na description of the observations and general characteristics being measured\n\n\n\nSection 3: Initial exploratory data analysis\nIn this section, you will begin to explore the data. This includes using narrative, visualizations and summary statistics to describe the following:\n\ndistribution of the response variable\ndistributions of one potential quantitative predictor variable and one potential categorical predictor variable\nthe relationships between the response variable and each of the predictors from the previous step\na potential interaction effect you‚Äôre interested in exploring (it doesn‚Äôt have to be an interaction with the two predictors from above)\n\nThese steps are to help get you started on exploratory data analysis and will not be the complete EDA for the final report. The requirements above are minimum requirements, but your group is welcome to include more at this stage.\nIn this section, you will also describe any data cleaning you need to do to prepare for modeling, such as imputing missing values, collapsing levels for categorical predictors, creating new variables, summarizing data, etc.\n\n\nSection 4: Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes\n\na description of the response variable and list of all potential predictors\nregression model technique (multiple linear regression or logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF document.\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nWrite your narrative and analysis for Sections 1 - 4 in the proposal.qmd file. Put the data set and the data dictionary in the data folder.\nSubmit the PDF of the proposal to Gradescope. Mark all pages of the document.\n\n\n\n\nGrading\nThe anticipated length, including all graphs, tables, narrative, etc., is 2 -4 pages; it may not exceed 5 pages.\nThe proposal is worth 15 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (14 - 15 points) : All required elements are completed and are accurate. There is a thorough exploration of the data as descrbied above, and the team has demonstrated a careful and thoughtful approach exploring the data and preparing it for analysis. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong: (11 - 13 points): Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (8 - 10 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (7 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling."
  },
  {
    "objectID": "project-instructions.html#draft-report-peer-review",
    "href": "project-instructions.html#draft-report-peer-review",
    "title": "Final project",
    "section": "Draft report + peer review",
    "text": "Draft report + peer review\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\n\nDraft report\n\n\n\n\n\n\nDue dates\n\n\n\nDraft is due in your project GitHub repo at 9am on\n\nTuesday, November 14 (Tuesday labs)\nThursday, November 16 (Thursday labs)\n\n\n\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope. The draft will be graded based on whether there is demonstration of a reasonable attempt at each of the sections described below in the written-report.qmd file in our GitHub repo by the deadline.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won‚Äôt fit in the body of the report, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you‚Äôre fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model.\n\n\n\nPeer review\n\n\n\n\n\n\nImportant\n\n\n\nPeer review comments are due in GitHub at 11:59pm on\n\nWednesday, November 15 (Tuesday labs)\nFriday, November 17 (Thursday labs)\n\n\n\nCritically reviewing others‚Äô work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams‚Äôs projects to review. Each team should push their draft to their GitHub repo by the 9am on the day their lab‚Äôs draft is due. The lab that week will be dedicate to the peer review, so your team will have time to review and provide quality feedback to two other teams.\nDuring the peer review process, you will be provided read-only access to your partner teams‚Äô GitHub repos. Provide your review in the form of GitHub issues to your partner team‚Äôs GitHub repo using the issue template provided in the repo. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team‚Äôs report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions."
  },
  {
    "objectID": "project-instructions.html#written-report",
    "href": "project-instructions.html#written-report",
    "title": "Final project",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\n\n\n\nYou will submit the PDF of your final report on GitHub.\nThe PDF you submit must match the .qmd in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. There is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won‚Äôt fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you‚Äôre fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\n\n\n\nDiscussion + Conclusion\nIn this section you‚Äôll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report."
  },
  {
    "objectID": "project-instructions.html#round1-submission",
    "href": "project-instructions.html#round1-submission",
    "title": "Final project",
    "section": "Round 1 submission (optional)",
    "text": "Round 1 submission (optional)\n\n\n\n\n\n\nDue date\n\n\n\nFriday, December 1 at 11:59pm on GitHub (all teams)\nReports submitted after this date will not receive preliminary feedback.\n\n\nThe Round 1 submission is an opportunity to receive detailed feedback on your analysis and written report before the final submission. Therefore, to make the feedback most useful, you must submit a complete written report to receive feedback. You will also be notified of the grade you would receive at that point. You will have the option to keep the grade (and thus you don‚Äôt need to turn in an updated report) or resubmit the written report by the final submission deadline to receive a new grade.\n\nTo submit the Round 1 submission:\n\nPush the updated written-report.qmd and written-report.pdf to your GitHub repo.\nOpen an issue with the title ‚ÄúRound 1 Submission‚Äù. You can use the template issue in the GitHub repo. Make sure I am tagged in the issue (@matackett), so I receive an email notification of your Round 1 submission. See Creating an issue from a repository for instructions on opening an issue. Please ask a member of the teaching team for assistance if you need help opening the issue.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this is optional, so there is nograde penalty for not turning in a Round 1 submission. Due to time constraints at the end of the semester, only high-level feedback will be given for the reports submitted at the final written report deadline on December 13."
  },
  {
    "objectID": "project-instructions.html#presentation",
    "href": "project-instructions.html#presentation",
    "title": "Final project",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "project-instructions.html#presentation-comments",
    "href": "project-instructions.html#presentation-comments",
    "title": "Final project",
    "section": "Presentation comments",
    "text": "Presentation comments"
  },
  {
    "objectID": "project-instructions.html#reproducibility-organization",
    "href": "project-instructions.html#reproducibility-organization",
    "title": "Final project",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\nproject.Rproj: File specifying the RStudio project\n.gitignore: File listing all files that are in the local RStudio project but not the GitHub repo\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-instructions.html#peer-teamwork-evaluation",
    "href": "project-instructions.html#peer-teamwork-evaluation",
    "title": "Final project",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation"
  },
  {
    "objectID": "project-instructions.html#overall-grading",
    "href": "project-instructions.html#overall-grading",
    "title": "Final project",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nProject proposal\n15 pts\n\n\nDraft report + peer review\n15 pts\n\n\nPresentation\n20 pts\n\n\nPresentation comments\n5 pts\n\n\nWritten report\n40 pts\n\n\nReproducibility + organization\n5 pts\n\n\n\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort."
  },
  {
    "objectID": "project-instructions.html#late-work-policy",
    "href": "project-instructions.html#late-work-policy",
    "title": "Final project",
    "section": "Late work policy",
    "text": "Late work policy\nThere is no late work accepted on the draft report or presentation. Other components of the project may be accepted up to 48 hours late. A 10% late deduction will apply for each 24-hour period late.\nBe sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you‚Äôll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you‚Äôll use for the course."
  },
  {
    "objectID": "slides/lab-02.html#reminders",
    "href": "slides/lab-02.html#reminders",
    "title": "Lab 02",
    "section": "Reminders",
    "text": "Reminders\n\n\nSelect the pages corresponding to each exercise when you when you submit the assignment on Gradescope.\n\nClick here for written and video instructions on submitting an assignment and marking pages on Gradescope.\n\nIn your write up:\n\nWrite all narrative in complete sentences.\nInclude an informative title and axis labels on graphs.\nWrite responses in the context of the data.\nDescribe distribution using shape, center, spread, and potential outliers. Describe relationships between variables using strength, direction, and shape."
  },
  {
    "objectID": "slides/lab-02.html#axis-labels-and-titles",
    "href": "slides/lab-02.html#axis-labels-and-titles",
    "title": "Lab 02",
    "section": "Axis labels and titles",
    "text": "Axis labels and titles\n\nBelow is a graph of association between flipper length in millimeters and body mass in grams of three species of penguins in Palmer Station, Antarctica. What are informative title and axis labels for this graph?"
  },
  {
    "objectID": "slides/lab-02.html#code-style",
    "href": "slides/lab-02.html#code-style",
    "title": "Lab 02",
    "section": "Code style",
    "text": "Code style\nWhich code chunk would you rather read?\n\n# code chunk 1\npenguins|&gt;filter(!is.na(flipper_length_mm))|&gt;group_by(species)|&gt;summarise(min=min(flipper_length_mm),mean=mean(flipper_length_mm),sd=sd(flipper_length_mm),max=max(flipper_length_mm),n=n())\n\n\n\n\n# code chunk 2\npenguins |&gt; \n  filter(!is.na(flipper_length_mm)) |&gt; \n  group_by(species) |&gt; \n  summarise(min = min(flipper_length_mm), \n            mean = mean(flipper_length_mm), \n            max = max(flipper_length_mm),\n            n = n())"
  },
  {
    "objectID": "slides/lab-02.html#code-style-contd",
    "href": "slides/lab-02.html#code-style-contd",
    "title": "Lab 02",
    "section": "Code style cont‚Äôd",
    "text": "Code style cont‚Äôd\nMake code easier to read and debug by\n\nPutting each element on a different line (start a new line after + and |&gt;)\nPutting spaces before and after operators (+, -, *, =, |&gt; )\nIn general, avoiding long lines of code, i.e.¬†lines longer than 120 characters.\n\nSee the Tidyverse Style Guide for more tips on code styling."
  },
  {
    "objectID": "slides/lab-02.html#todays-lab",
    "href": "slides/lab-02.html#todays-lab",
    "title": "Lab 02",
    "section": "Today‚Äôs lab",
    "text": "Today‚Äôs lab\n\nRemember to use a reproducible workflow with regular commits (and informative commit messages).\nPush all updated files after each commit! The Git pane should be clear after each Render -&gt; Commit -&gt; Push.\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/15-mlr-inference.html#announcements",
    "href": "slides/15-mlr-inference.html#announcements",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Announcements",
    "text": "Announcements\n\nProject propsal due\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\nHW 03 due Wednesday, November 1\n\nreleased after Section 002 lecture"
  },
  {
    "objectID": "slides/15-mlr-inference.html#topics",
    "href": "slides/15-mlr-inference.html#topics",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/15-mlr-inference.html#computational-setup",
    "href": "slides/15-mlr-inference.html#computational-setup",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/15-mlr-inference.html#modeling-workflow",
    "href": "slides/15-mlr-inference.html#modeling-workflow",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nSplit data into training and test sets.\nUse cross validation on the training set to fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\nRefit the model using the entire training set and do ‚Äúfinal‚Äù evaluation on the test set (make sure you have not overfit the model).\n\nAdjust as needed if there is evidence of overfit.\n\nUse model fit on training set for inference and prediction."
  },
  {
    "objectID": "slides/15-mlr-inference.html#data-rail_trail",
    "href": "slides/15-mlr-inference.html#data-rail_trail",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\n# A tibble: 90 √ó 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ‚Ñπ 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/15-mlr-inference.html#variables",
    "href": "slides/15-mlr-inference.html#variables",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of ‚ÄúFall‚Äù, ‚ÄúSpring‚Äù, or ‚ÄúSummer‚Äù\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of ‚Äúweekday‚Äù or ‚Äúweekend‚Äù"
  },
  {
    "objectID": "slides/15-mlr-inference.html#review-simple-linear-regression-slr",
    "href": "slides/15-mlr-inference.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\nggplot(rail_trail, aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"High temp (F)\", y = \"Number of riders\")"
  },
  {
    "objectID": "slides/15-mlr-inference.html#slr-model-summary",
    "href": "slides/15-mlr-inference.html#slr-model-summary",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00"
  },
  {
    "objectID": "slides/15-mlr-inference.html#slr-hypothesis-test",
    "href": "slides/15-mlr-inference.html#slr-hypothesis-test",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00\n\n\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t= 6.72\\) . The p-value is calculated using a \\(t\\) distribution with 88 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e.¬†there is a linear relationship between high temperature and number of daily riders."
  },
  {
    "objectID": "slides/15-mlr-inference.html#multiple-linear-regression",
    "href": "slides/15-mlr-inference.html#multiple-linear-regression",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11"
  },
  {
    "objectID": "slides/15-mlr-inference.html#multiple-linear-regression-1",
    "href": "slides/15-mlr-inference.html#multiple-linear-regression-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#estimating-sigma_epsilon",
    "href": "slides/15-mlr-inference.html#estimating-sigma_epsilon",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[e_i = y_{i} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\\]\n\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n\n\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/15-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "href": "slides/15-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) vs.¬†\\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\). The p-value is calculated using a \\(t\\) distribution with 86 \\((n - p - 1)\\) degrees of freedom. The p-value is \\(\\approx 0\\).\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/15-mlr-inference.html#the-model-for-season-spring",
    "href": "slides/15-mlr-inference.html#the-model-for-season-spring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#the-model-for-season-summer",
    "href": "slides/15-mlr-inference.html#the-model-for-season-summer",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#the-model-for-season-fall",
    "href": "slides/15-mlr-inference.html#the-model-for-season-fall",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#the-models",
    "href": "slides/15-mlr-inference.html#the-models",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#interaction-terms",
    "href": "slides/15-mlr-inference.html#interaction-terms",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interaction terms",
    "text": "Interaction terms\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.53\n166.80\n-0.06\n0.95\n\n\nhightemp\n5.48\n2.95\n1.86\n0.07\n\n\nseasonSpring\n-293.95\n190.33\n-1.54\n0.13\n\n\nseasonSummer\n354.18\n255.08\n1.39\n0.17\n\n\nhightemp:seasonSpring\n4.88\n3.26\n1.50\n0.14\n\n\nhightemp:seasonSummer\n-4.54\n3.75\n-1.21\n0.23\n\n\n\n\n\n\n\n\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms."
  },
  {
    "objectID": "slides/15-mlr-inference.html#confidence-interval-for-beta_j-1",
    "href": "slides/15-mlr-inference.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant."
  },
  {
    "objectID": "slides/15-mlr-inference.html#confidence-interval-for-beta_j-2",
    "href": "slides/15-mlr-inference.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE) |&gt;\n  kable(digits= 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00"
  },
  {
    "objectID": "slides/15-mlr-inference.html#ci-for-hightemp",
    "href": "slides/15-mlr-inference.html#ci-for-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/15-mlr-inference.html#ci-for-seasonspring",
    "href": "slides/15-mlr-inference.html#ci-for-seasonspring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n\nIs season a significant predictor of the number of riders, after accounting for high temperature?"
  },
  {
    "objectID": "slides/15-mlr-inference.html#large-sample-sizes",
    "href": "slides/15-mlr-inference.html#large-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/15-mlr-inference.html#small-sample-sizes",
    "href": "slides/15-mlr-inference.html#small-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON‚ÄôT immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/15-mlr-inference.html#full-model",
    "href": "slides/15-mlr-inference.html#full-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 √ó 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit$fit)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-conditions",
    "href": "slides/15-mlr-inference.html#model-conditions",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-linearity",
    "href": "slides/15-mlr-inference.html#checking-linearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs.¬†predicted values\nLook at a plot of the residuals vs.¬†each predictor\nLinearity is met if there is no discernible pattern in each of these plots"
  },
  {
    "objectID": "slides/15-mlr-inference.html#residuals-vs.-predicted-values",
    "href": "slides/15-mlr-inference.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs.¬†predicted values",
    "text": "Residuals vs.¬†predicted values\n\nggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/15-mlr-inference.html#residuals-vs.-each-predictor",
    "href": "slides/15-mlr-inference.html#residuals-vs.-each-predictor",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs.¬†each predictor",
    "text": "Residuals vs.¬†each predictor"
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-linearity-1",
    "href": "slides/15-mlr-inference.html#checking-linearity-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs.¬†predicted values looked OK\nThe plots of residuals vs.¬†hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-constant-variance",
    "href": "slides/15-mlr-inference.html#checking-constant-variance",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-constant-variance-1",
    "href": "slides/15-mlr-inference.html#checking-constant-variance-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n\n\nWe will talk about to address this later in the notes."
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-normality",
    "href": "slides/15-mlr-inference.html#checking-normality",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking normality",
    "text": "Checking normality\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied."
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-independence",
    "href": "slides/15-mlr-inference.html#checking-independence",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-independence-1",
    "href": "slides/15-mlr-inference.html#checking-independence-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†order of data collection:\n\nggplot(rt_full_aug, aes(y = .resid, x = 1:nrow(rt_full_aug))) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/15-mlr-inference.html#checking-independence-2",
    "href": "slides/15-mlr-inference.html#checking-independence-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nNo clear pattern in the residuals vs.¬†order of data collection plot.\nIndependence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/15-mlr-inference.html#what-is-multicollinearity",
    "href": "slides/15-mlr-inference.html#what-is-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/15-mlr-inference.html#example",
    "href": "slides/15-mlr-inference.html#example",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\nLet‚Äôs assume the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#example-1",
    "href": "slides/15-mlr-inference.html#example-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the ‚Äúbest‚Äù combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#why-multicollinearity-is-a-problem",
    "href": "slides/15-mlr-inference.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e.¬†highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/15-mlr-inference.html#detecting-multicollinearity",
    "href": "slides/15-mlr-inference.html#detecting-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when‚Ä¶\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\n\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere is a quadratic term in the model without mean-centering the variable first\nThere are interactions between two or more continuous variables\n\nCan reduce this by mean-centering the variables first\n\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/15-mlr-inference.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/15-mlr-inference.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/15-mlr-inference.html#detecting-multicollinearity-vif",
    "href": "slides/15-mlr-inference.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/15-mlr-inference.html#detecting-multicollinearity-vif-1",
    "href": "slides/15-mlr-inference.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/15-mlr-inference.html#vif-for-rail-trail-model",
    "href": "slides/15-mlr-inference.html#vif-for-rail-trail-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit$fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-without-hightemp",
    "href": "slides/15-mlr-inference.html#model-without-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-without-avgtemp",
    "href": "slides/15-mlr-inference.html#model-without-avgtemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/15-mlr-inference.html#choosing-a-model",
    "href": "slides/15-mlr-inference.html#choosing-a-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/15-mlr-inference.html#selected-model-for-now",
    "href": "slides/15-mlr-inference.html#selected-model-for-now",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/15-mlr-inference.html#topics-1",
    "href": "slides/15-mlr-inference.html#topics-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/15-mlr-inference.html#residuals-vs.-fitted-for-the-selected-model",
    "href": "slides/15-mlr-inference.html#residuals-vs.-fitted-for-the-selected-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs.¬†fitted for the selected model",
    "text": "Residuals vs.¬†fitted for the selected model\n\nThe constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition."
  },
  {
    "objectID": "slides/15-mlr-inference.html#identifying-a-need-to-transform-y",
    "href": "slides/15-mlr-inference.html#identifying-a-need-to-transform-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Identifying a need to transform \\(Y\\)",
    "text": "Identifying a need to transform \\(Y\\)\n\nTypically, a ‚Äúfan-shaped‚Äù residual plot indicates the need for a transformation of the response variable \\(Y\\)\n\nThere are multiple ways to transform a variable, e.g., \\(\\sqrt{Y}\\), \\(1/Y\\), \\(\\log(Y)\\)\n\\(\\log(Y)\\) the most straightforward to interpret, so we use that transformation when possible\n\n\n\n\nWhen building a model:\n\nChoose a transformation and build the model on the transformed data\nReassess the residual plots\nIf the residuals plots did not sufficiently improve, try a new transformation!"
  },
  {
    "objectID": "slides/15-mlr-inference.html#log-transformation-on-y",
    "href": "slides/15-mlr-inference.html#log-transformation-on-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\n\nIf we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n\\[\n\\log(Y) = \\beta_0+ \\beta_1 X_1 + \\dots +\\beta_pX_p + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n\\]\n\nThe regression equation is\n\n\\[\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_pX_p\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#log-transformation-on-y-1",
    "href": "slides/15-mlr-inference.html#log-transformation-on-y-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\nWe want to interpret the model in terms of the original variable \\(Y\\), not \\(\\log(Y)\\), so we need to write the regression equation in terms of \\(Y\\)\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\nThe predicted value \\(\\hat{Y}\\) is the predicted median of \\(Y\\). Note, when the distribution of \\(Y|X_1, \\ldots, X_p\\) is symmetric, then the median equals the mean. See the slides in the appendix for more detail."
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-interpretation",
    "href": "slides/15-mlr-inference.html#model-interpretation",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\nIntercept: When \\(X_1 = \\dots = X_p =0\\), \\(Y\\) is expected to be \\(\\exp\\{\\hat{\\beta}_0\\}\\)\nSlope: For every one unit increase in \\(X_j\\), the \\(Y\\) is expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_j\\}\\), holding all else constant\n\n\nWhy is the interpretation in terms of a multiplicative change?"
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-for-logvolume",
    "href": "slides/15-mlr-inference.html#model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model for \\(log(volume)\\)",
    "text": "Model for \\(log(volume)\\)\n\n#fit model\nlog_rt_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)\n\ntidy(log_rt_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327"
  },
  {
    "objectID": "slides/15-mlr-inference.html#interpretation-of-model-for-logvolume",
    "href": "slides/15-mlr-inference.html#interpretation-of-model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interpretation of model for \\(\\log(volume)\\)",
    "text": "Interpretation of model for \\(\\log(volume)\\)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/15-mlr-inference.html#residuals-for-model-with-logvolume",
    "href": "slides/15-mlr-inference.html#residuals-for-model-with-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals for model with \\(\\log(volume)\\)",
    "text": "Residuals for model with \\(\\log(volume)\\)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#compare-residual-plots",
    "href": "slides/15-mlr-inference.html#compare-residual-plots",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/15-mlr-inference.html#log-transformation-on-x",
    "href": "slides/15-mlr-inference.html#log-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#respiratory-rate-vs.-age",
    "href": "slides/15-mlr-inference.html#respiratory-rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Respiratory Rate vs.¬†Age",
    "text": "Respiratory Rate vs.¬†Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a ‚Äúhigh‚Äù rate, we first want to understand the relationship between a child‚Äôs age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication ‚ÄúReference Values for Respiratory Rate in the First 3 Years of Life‚Äù.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#rate-vs.-age",
    "href": "slides/15-mlr-inference.html#rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Rate vs.¬†Age",
    "text": "Rate vs.¬†Age"
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-with-transformation-on-x",
    "href": "slides/15-mlr-inference.html#model-with-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e.¬†the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/15-mlr-inference.html#model-interpretation-1",
    "href": "slides/15-mlr-inference.html#model-interpretation-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n50.135\n0.632\n79.330\n0\n\n\nlog(Age)\n-5.982\n0.263\n-22.781\n0\n\n\n\n\n\n\n\n\\[\\hat{\\text{Rate}} = 50.135 - 5.982 \\times \\log\\text{(Age)}\\]\n\n\nInterpret the intercept in the context of the data.\nInterpret the slope in terms of age multiplying by 2 in the context of the data."
  },
  {
    "objectID": "slides/15-mlr-inference.html#learn-more",
    "href": "slides/15-mlr-inference.html#learn-more",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables."
  },
  {
    "objectID": "slides/15-mlr-inference.html#why-medianyx-instead-of-mu_yx",
    "href": "slides/15-mlr-inference.html#why-medianyx-instead-of-mu_yx",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\nSuppose we have a set of values\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet‚Äôs calculate \\(\\overline{\\log(x)}\\)\n\nlog_x &lt;- log(x)\nmean(log_x)\n\n[1] 2.066476\n\n\n\nLet‚Äôs calculate \\(\\log(\\bar{x})\\)\n\nxbar &lt;- mean(x)\nlog(xbar)\n\n[1] 2.228477\n\n\n\n\n\n\n\nNote: \\(\\overline{\\log(x)} \\neq \\log(\\bar{x})\\)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#why-medianyx-instead-of-mu_yx-1",
    "href": "slides/15-mlr-inference.html#why-medianyx-instead-of-mu_yx-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet‚Äôs calculate \\(\\text{Median}(\\log(x))\\)\n\nlog_x &lt;- log(x)\nmedian(log_x)\n\n[1] 2.079442\n\n\n\nLet‚Äôs calculate \\(\\log(\\text{Median}(x))\\)\n\nmedian_x &lt;- median(x)\nlog(median_x)\n\n[1] 2.079442\n\n\n\n\n\n\n\nNote: \\(\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\)"
  },
  {
    "objectID": "slides/15-mlr-inference.html#mean-median-and-log",
    "href": "slides/15-mlr-inference.html#mean-median-and-log",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean, Median, and log",
    "text": "Mean, Median, and log\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\\[\\overline{\\log(x)} \\neq \\log(\\bar{x})\\]\n\nmean(log_x) == log(xbar)\n\n[1] FALSE\n\n\n\n\\[\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\]\n\nmedian(log_x) == log(median_x)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/15-mlr-inference.html#mean-and-median-of-logy",
    "href": "slides/15-mlr-inference.html#mean-and-median-of-logy",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(Y)\\)",
    "text": "Mean and median of \\(\\log(Y)\\)\n\nRecall that \\(Y = \\beta_0 + \\beta_1 X\\) is the mean value of the response at the given value of the predictor \\(X\\). This doesn‚Äôt hold when we log-transform the response variable.\nMathematically, the mean of the logged values is not necessarily equal to the log of the mean value. Therefore at a given value of \\(X\\)\n\n\n\\[\n\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-inference.html#mean-and-median-of-logy-1",
    "href": "slides/15-mlr-inference.html#mean-and-median-of-logy-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(y)\\)",
    "text": "Mean and median of \\(\\log(y)\\)\n\nHowever, the median of the logged values is equal to the log of the median value. Therefore,\n\n\\[\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)\\]\n\n\nIf the distribution of \\(\\log(Y)\\) is symmetric about the regression line, for a given value \\(X\\), we can expect \\(Mean(Y)\\) and \\(Median(Y)\\) to be approximately equal.\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-01.html#do-you-have-the-lab-01-repo",
    "href": "slides/lab-01.html#do-you-have-the-lab-01-repo",
    "title": "Lab 01",
    "section": "Do you have the lab-01 repo?",
    "text": "Do you have the lab-01 repo?\n\nGo to the GitHub course organization: https://github.com/sta210-fa23\nYou should see a repo with the prefix lab-01- followed by your GitHub username\nIf you do not have this repo\n\nClickhere to create your lab-01 repo, AND\nGo to the course organization and accept the invite in the banner at the top of the page. If you do not see the invite, click here to (re)submit your GitHub username."
  },
  {
    "objectID": "slides/lab-01.html#demo",
    "href": "slides/lab-01.html#demo",
    "title": "Lab 01",
    "section": "Demo",
    "text": "Demo\nFollow along as your TA demonstrates the following:\n\nConfigure Git using SSH\nClone repo (using SSH) and start new project in RStudio\nRender document and produce PDF\nUpdate name in YAML in the Quarto doc\n\nRender, commit, push changes to GitHub\nSee updates in your GitHub repo\n\nAdd an R code chunk to the Quarto doc"
  },
  {
    "objectID": "slides/lab-01.html#tips-for-working-on-lab",
    "href": "slides/lab-01.html#tips-for-working-on-lab",
    "title": "Lab 01",
    "section": "Tips for working on lab",
    "text": "Tips for working on lab\n\nYou do not have to finish the lab in class, they will always be due Friday (Tuesday labs) or Sunday (Thursday labs). One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nDo not pressure each other to finish early (particularly once you start working on teams); use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-01.html#when-youre-done-with-lab",
    "href": "slides/lab-01.html#when-youre-done-with-lab",
    "title": "Lab 01",
    "section": "When you‚Äôre done with lab",
    "text": "When you‚Äôre done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\n\nSubmit the PDF of your responses to Gradescope\n\nYou can access Gradescope through Sakai or the course website\nLogin using your Duke NetID credentials\nSee Lab 01 instructions for details on submitting an assignment on Gradescope"
  },
  {
    "objectID": "slides/lab-01.html#lab-01-park-access",
    "href": "slides/lab-01.html#lab-01-park-access",
    "title": "Lab 01",
    "section": "Lab 01: Park access",
    "text": "Lab 01: Park access\n\nToday‚Äôs lab focuses on exploratory data analysis and simple linear regression. Read the lab instructions carefully.\nThere are markers throughout suggesting when to render, commit, and push changes to GitHub. These are to help you start using version control in your workflow.\nThere are points for having a neatly formatted document and implementing a reproducible workflow. See the Workflow & Formatting section in the Lab 01 instructions.\n\nüîó sta210-fa23.netlify.app/labs/lab-01.html\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-04.html#goals",
    "href": "slides/lab-04.html#goals",
    "title": "Lab 04",
    "section": "Goals",
    "text": "Goals\n\nMeet your team!\nTeam agreement\nLab 04: Alumni jobs"
  },
  {
    "objectID": "slides/lab-04.html#meet-your-team",
    "href": "slides/lab-04.html#meet-your-team",
    "title": "Lab 04",
    "section": "Meet your team!",
    "text": "Meet your team!\n\nClick here to find your team.\nSit with your team."
  },
  {
    "objectID": "slides/lab-04.html#team-name-agreement",
    "href": "slides/lab-04.html#team-name-agreement",
    "title": "Lab 04",
    "section": "Team name + agreement",
    "text": "Team name + agreement\n\nCome up with a team name. You can‚Äôt have the same name as another group in the class, so be creative!\n\nYour TA will get your team name by the end of lab.\n\nFill out the team agreement. The goals of the agreement are to‚Ä¶\n\nGain a common understanding of the team‚Äôs goals and expectations for collaboration\nMake a plan for team communication\nMake a plan for working outside of lab"
  },
  {
    "objectID": "slides/lab-04.html#team-workflow",
    "href": "slides/lab-04.html#team-workflow",
    "title": "Lab 04",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are markers in today‚Äôs lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it‚Äôs not your turn type.\n\nDon‚Äôt forget to pull to get your teammates‚Äô updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-04.html#team-workflow-in-action",
    "href": "slides/lab-04.html#team-workflow-in-action",
    "title": "Lab 04",
    "section": "Team workflow, in action",
    "text": "Team workflow, in action\n\nComplete the ‚ÄúWorkflow: Using Git and GitHub as a team‚Äù section of the lab in your teams.\nRaise your hand if you have any questions about the workflow.\n\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/lab-04.html#tips-for-working-on-a-team",
    "href": "slides/lab-04.html#tips-for-working-on-a-team",
    "title": "Lab 04",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other.\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/03-slr-intro.html#topics",
    "href": "slides/03-slr-intro.html#topics",
    "title": "Simple Linear Regression",
    "section": "Topics",
    "text": "Topics\n\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nEstimate the slope and intercept of the regression line using the least squares method.\nInterpret the slope and intercept of the regression line.\nPredict the response given a value of the predictor variable.\nUse tidymodels to fit and summarize regression models in R."
  },
  {
    "objectID": "slides/03-slr-intro.html#computation-set-up",
    "href": "slides/03-slr-intro.html#computation-set-up",
    "title": "Simple Linear Regression",
    "section": "Computation set up",
    "text": "Computation set up\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\nlibrary(knitr)          # for formatting tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/03-slr-intro.html#movie-scores",
    "href": "slides/03-slr-intro.html#movie-scores",
    "title": "Simple Linear Regression",
    "section": "Movie scores",
    "text": "Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango‚Äôs\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/03-slr-intro.html#data-prep",
    "href": "slides/03-slr-intro.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/03-slr-intro.html#data-overview",
    "href": "slides/03-slr-intro.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A‚Ä¶\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2‚Ä¶\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,‚Ä¶\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,‚Ä¶\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,‚Ä¶\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8‚Ä¶\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4‚Ä¶\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5‚Ä¶\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0‚Ä¶\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4‚Ä¶\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3‚Ä¶\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4‚Ä¶\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3‚Ä¶\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3‚Ä¶\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0‚Ä¶\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0‚Ä¶\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0‚Ä¶\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5‚Ä¶\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5‚Ä¶\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54‚Ä¶\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, ‚Ä¶\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, ‚Ä¶\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5‚Ä¶"
  },
  {
    "objectID": "slides/03-slr-intro.html#movie-scores-data",
    "href": "slides/03-slr-intro.html#movie-scores-data",
    "title": "Simple Linear Regression",
    "section": "Movie scores data",
    "text": "Movie scores data\nThe data set contains the ‚ÄúTomatometer‚Äù score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com."
  },
  {
    "objectID": "slides/03-slr-intro.html#movie-ratings-data",
    "href": "slides/03-slr-intro.html#movie-ratings-data",
    "title": "Simple Linear Regression",
    "section": "Movie ratings data",
    "text": "Movie ratings data\nGoal: Fit a line to describe the relationship between the critics score and audience score.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/03-slr-intro.html#why-fit-a-line",
    "href": "slides/03-slr-intro.html#why-fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n\nPrediction\n\nWhat is the audience score expected to be for an upcoming movie that received 35% from the critics?\n\n\n\n\nInference\n\nIs the critics score a useful predictor of the audience score? By how much is the audience score expected to change for each additional point in the critics score?"
  },
  {
    "objectID": "slides/03-slr-intro.html#terminology",
    "href": "slides/03-slr-intro.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, Y: variable describing the outcome of interest\nPredictor, X: variable we use to help understand the variability in the response\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/03-slr-intro.html#regression-model",
    "href": "slides/03-slr-intro.html#regression-model",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the response, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-slr-intro.html#regression-model-1",
    "href": "slides/03-slr-intro.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu_{Y|X}\\) is the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/03-slr-intro.html#regression-model-2",
    "href": "slides/03-slr-intro.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[5pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n\\end{aligned}\n\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/03-slr-intro.html#slr-statistical-model",
    "href": "slides/03-slr-intro.html#slr-statistical-model",
    "title": "Simple Linear Regression",
    "section": "SLR: Statistical model",
    "text": "SLR: Statistical model\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\Large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/03-slr-intro.html#slr-regression-equation",
    "href": "slides/03-slr-intro.html#slr-regression-equation",
    "title": "Simple Linear Regression",
    "section": "SLR: Regression equation",
    "text": "SLR: Regression equation\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/03-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/03-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead."
  },
  {
    "objectID": "slides/03-slr-intro.html#residuals",
    "href": "slides/03-slr-intro.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]"
  },
  {
    "objectID": "slides/03-slr-intro.html#least-squares-line",
    "href": "slides/03-slr-intro.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/03-slr-intro.html#properties-of-least-squares-regression",
    "href": "slides/03-slr-intro.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is approximately zero: \\(\\sum_{i = 1}^n e_i \\approx 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/03-slr-intro.html#estimating-the-slope",
    "href": "slides/03-slr-intro.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\\]\n\n\n\n\nClickhere for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/03-slr-intro.html#estimating-the-intercept",
    "href": "slides/03-slr-intro.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n\\[\\begin{aligned}\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\\]\n\n\n\n\nClick here for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/03-slr-intro.html#interpretation",
    "href": "slides/03-slr-intro.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nPost your answers to the following questions on Ed Discussion:\n\nThe slope of the model for predicting audience score from critics score is 0.5187 . Which of the following is the best interpretation of this value?\n32.3142 is the predicted mean audience score for what type of movies?\n\n\n\n\nLink for Section 001 (10:05am lecture)\nLink for Section 002 (1:25pm lecture)\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/03-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/03-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n\n‚úÖ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n\n\nüõë Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/03-slr-intro.html#making-a-prediction",
    "href": "slides/03-slr-intro.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 70. According to this model, what is the movie‚Äôs predicted audience score?\n\\[\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 70 \\\\\n&= 68.6232\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-slr-intro.html#tidymodels",
    "href": "slides/03-slr-intro.html#tidymodels",
    "title": "Simple Linear Regression",
    "section": "tidymodels",
    "text": "tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\n\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.1.0 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.5     ‚úî rsample      1.1.1\n‚úî dials        1.2.0     ‚úî tune         1.1.1\n‚úî infer        1.0.4     ‚úî workflows    1.1.3\n‚úî modeldata    1.1.0     ‚úî workflowsets 1.0.1\n‚úî parsnip      1.1.0     ‚úî yardstick    1.2.0\n‚úî recipes      1.0.6     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ scales::discard() masks purrr::discard()\n‚úñ dplyr::filter()   masks stats::filter()\n‚úñ recipes::fixed()  masks stringr::fixed()\n‚úñ dplyr::lag()      masks stats::lag()\n‚úñ yardstick::spec() masks readr::spec()\n‚úñ recipes::step()   masks stats::step()\n‚Ä¢ Dig deeper into tidy modeling with R at https://www.tmwr.org"
  },
  {
    "objectID": "slides/03-slr-intro.html#why-tidymodels",
    "href": "slides/03-slr-intro.html#why-tidymodels",
    "title": "Simple Linear Regression",
    "section": "Why tidymodels?",
    "text": "Why tidymodels?\n\nConsistent syntax for different model types (linear, logistic, random forest, Bayesian, etc.)\nStreamline modeling workflow\n\nSplit data into train and test sets\nTransform and create new variables\nAssess model performance\nUse model for prediction and inference"
  },
  {
    "objectID": "slides/03-slr-intro.html#step-1-specify-model",
    "href": "slides/03-slr-intro.html#step-1-specify-model",
    "title": "Simple Linear Regression",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-intro.html#step-2-set-model-fitting-engine",
    "href": "slides/03-slr-intro.html#step-2-set-model-fitting-engine",
    "title": "Simple Linear Regression",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-intro.html#a-closer-look-at-the-regression-output",
    "href": "slides/03-slr-intro.html#a-closer-look-at-the-regression-output",
    "title": "Simple Linear Regression",
    "section": "A closer look at the regression output",
    "text": "A closer look at the regression output\n\nmovie_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/03-slr-intro.html#the-regression-output",
    "href": "slides/03-slr-intro.html#the-regression-output",
    "title": "Simple Linear Regression",
    "section": "The regression output",
    "text": "The regression output\nWe‚Äôll focus on the first column for now‚Ä¶\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/03-slr-intro.html#format-output-with-kable",
    "href": "slides/03-slr-intro.html#format-output-with-kable",
    "title": "Simple Linear Regression",
    "section": "Format output with kable",
    "text": "Format output with kable\nUse the kable function from the knitr package to produce a table and specify number of significant digits\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() |&gt;\n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n32.3155\n2.3425\n13.7953\n0\n\n\ncritics\n0.5187\n0.0345\n15.0281\n0"
  },
  {
    "objectID": "slides/03-slr-intro.html#prediction-1",
    "href": "slides/03-slr-intro.html#prediction-1",
    "title": "Simple Linear Regression",
    "section": "Prediction",
    "text": "Prediction\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 70)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 √ó 1\n  .pred\n  &lt;dbl&gt;\n1  68.6"
  },
  {
    "objectID": "slides/03-slr-intro.html#recap",
    "href": "slides/03-slr-intro.html#recap",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nUsed the least squares method to estimate the slope and intercept.\nInterpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to change by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units\n\nPredicted the response given a value of the predictor variable.\nUsed tidymodels to fit and summarize regression models in R.\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#announcements",
    "href": "slides/03-slr-tidymodels.html#announcements",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Announcements",
    "text": "Announcements\n\nNo office hours today. Office hours start Tuesday, September 6. Click here for full schedule\nCheck your email for an email to join the course GitHub organization. You will receive one by Tuesday, September 6.\nSee Week 02 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#topics",
    "href": "slides/03-slr-tidymodels.html#topics",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Topics",
    "text": "Topics\n\nIntroduce the computing toolkit - RStudio and GitHub\nUse tidymodels to fit and summarize regression models in R\nComplete an application exercise on exploratory data analysis and modeling"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#computational-setup",
    "href": "slides/03-slr-tidymodels.html#computational-setup",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#reproducibility-checklist",
    "href": "slides/03-slr-tidymodels.html#reproducibility-checklist",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Are the tables and figures reproducible from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#toolkit",
    "href": "slides/03-slr-tidymodels.html#toolkit",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\nMore on this in this week‚Äôs lab"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#what-are-r-and-rstudio",
    "href": "slides/03-slr-tidymodels.html#what-are-r-and-rstudio",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "What are R and RStudio?",
    "text": "What are R and RStudio?\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\n\nSource: Modern Dive"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#rstudio-ide",
    "href": "slides/03-slr-tidymodels.html#rstudio-ide",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#quarto",
    "href": "slides/03-slr-tidymodels.html#quarto",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports ‚Äì the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#quarto-1",
    "href": "slides/03-slr-tidymodels.html#quarto-1",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#how-will-we-use-quarto",
    "href": "slides/03-slr-tidymodels.html#how-will-we-use-quarto",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou‚Äôll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#application-exercise",
    "href": "slides/03-slr-tidymodels.html#application-exercise",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#recap-of-last-lecture",
    "href": "slides/03-slr-tidymodels.html#recap-of-last-lecture",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#movie-ratings",
    "href": "slides/03-slr-tidymodels.html#movie-ratings",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango‚Äôs\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#data-prep",
    "href": "slides/03-slr-tidymodels.html#data-prep",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#tidymodels",
    "href": "slides/03-slr-tidymodels.html#tidymodels",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "tidymodels",
    "text": "tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\n\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.1.0 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.5     ‚úî rsample      1.1.1\n‚úî dials        1.2.0     ‚úî tune         1.1.1\n‚úî infer        1.0.4     ‚úî workflows    1.1.3\n‚úî modeldata    1.1.0     ‚úî workflowsets 1.0.1\n‚úî parsnip      1.1.0     ‚úî yardstick    1.2.0\n‚úî recipes      1.0.6     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ scales::discard() masks purrr::discard()\n‚úñ dplyr::filter()   masks stats::filter()\n‚úñ recipes::fixed()  masks stringr::fixed()\n‚úñ dplyr::lag()      masks stats::lag()\n‚úñ yardstick::spec() masks readr::spec()\n‚úñ recipes::step()   masks stats::step()\n‚Ä¢ Use suppressPackageStartupMessages() to eliminate package startup messages"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#why-tidymodels",
    "href": "slides/03-slr-tidymodels.html#why-tidymodels",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Why tidymodels?",
    "text": "Why tidymodels?\n\nConsistent syntax for different model types (linear, logistic, random forest, Bayesian, etc.)\nStreamline modeling workflow\n\nSplit data into train and test sets\nTransform and create new variables\nAssess model performance\nUse model for prediction and inference"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#step-1-specify-model",
    "href": "slides/03-slr-tidymodels.html#step-1-specify-model",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#step-2-set-model-fitting-engine",
    "href": "slides/03-slr-tidymodels.html#step-2-set-model-fitting-engine",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#a-closer-look-at-model-output",
    "href": "slides/03-slr-tidymodels.html#a-closer-look-at-model-output",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\nmovie_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is likely just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#the-regression-output",
    "href": "slides/03-slr-tidymodels.html#the-regression-output",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "The regression output",
    "text": "The regression output\nWe‚Äôll focus on the first column for now‚Ä¶\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#prediction",
    "href": "slides/03-slr-tidymodels.html#prediction",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Prediction",
    "text": "Prediction\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 70)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 √ó 1\n  .pred\n  &lt;dbl&gt;\n1  68.6"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#application-exercise-1",
    "href": "slides/03-slr-tidymodels.html#application-exercise-1",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-fa22/ae-02-bikeshare\n\n\n\nClose document tab"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#recap",
    "href": "slides/03-slr-tidymodels.html#recap",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Recap",
    "text": "Recap\n\nIntroduced the computing toolkit - RStudio and GitHub\nUsed tidymodels to fit and summarize regression models in R\nCompleted an application exercise on exploratory data analysis and modeling\n\n\n\n\nüîó Week 02"
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-tackett",
    "href": "slides/01-welcome.html#meet-prof.-tackett",
    "title": "Welcome to STA 210!",
    "section": "Meet Prof.¬†Tackett!",
    "text": "Meet Prof.¬†Tackett!\n\n\nEducation and career journey\n\nBS in Math and MS in Statistics from University of Tennessee\nStatistician at Capital One\nPhD in Statistics from University of Virginia\nAssistant Professor of the Practice, Department of Statistical Science at Duke\n\nWork focuses on statistics education and sense of belonging in introductory math and statistics classes\nCo-leader of the Bass Connections team Mental Health and the Justice System in Durham County\nMom of (almost) 8-month-old twins üôÇ"
  },
  {
    "objectID": "slides/01-welcome.html#meet-the-teaching-assistants-tas",
    "href": "slides/01-welcome.html#meet-the-teaching-assistants-tas",
    "title": "Welcome to STA 210!",
    "section": "Meet the Teaching Assistants (TAs)",
    "text": "Meet the Teaching Assistants (TAs)\n\nSam Rosen (PhD): Head TA + Lab 01\nBethany Astor (MS): Lab 02\nJon Campbell (MS)\nDonald Cayton (MS): Lab 02\nAllison Li (UG)\nMitchelle Mojekwu (UG): Lab 04\nBen Thorpe (UG)\nLinxuan Wang (MS): Lab 03\nXiaojun Zheng (PhD): Lab 04"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-regression-analysis",
    "href": "slides/01-welcome.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\n‚ÄúIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or ‚Äòpredictors‚Äô). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‚Äòcriterion variable‚Äô) changes when any one of the independent variables is varied, while the other independent variables are held fixed.‚Äù\n\nSource: Wikipedia (previous definition)"
  },
  {
    "objectID": "slides/01-welcome.html#examples-of-regression-in-practice",
    "href": "slides/01-welcome.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight‚Äôs 2020 Presidential Forecast Works ‚Äî And What‚Äôs Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it‚Äôs so freaking hard to make a good COVID-19 model (from March 2020)"
  },
  {
    "objectID": "slides/01-welcome.html#course-faq",
    "href": "slides/01-welcome.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Introductory statistics or probability course at Duke\n\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis, Quarto for writing up results, and GitHub for version control and collaboration\n\n\nQ - Will we learn the mathematical theory of regression?\nA - Yes and No.¬†The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. There a 0.5-credit course STA 211: Mathematics of Regression to take simultaneously or after this course to dive into more of the mathematics."
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze real-world data to answer questions about multivariable relationships.\nuse R to fit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate statistical results through writing and oral presentations."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to STA 210!",
    "section": "Course topics",
    "text": "Course topics\n\n\nUnit 1: Quantitative Response Variable\n\nSimple Linear Regression\nMultiple Linear Regression\n\n\nUnit 2: Categorical Response Variable\n\nLogistic Regression\nMultinomial logistic regression\n\n\nUnit 3: Looking Ahead\n\nSpecial topics\nPresenting statistical results"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website: sta210-fa23.netlify.app\n\nCentral hub for the course!\nTour of the website\n\nSakai: sakai.duke.edu\n\nGradebook\nAnnouncements\nGradescope\nEd Discussion\n\nGitHub: github.com/sta210-fa23\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through STA 210 Docker Containers\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in STA 210 course organization"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-participate-practice-perform",
    "href": "slides/01-welcome.html#prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you‚Äôve learned to analyze real-world data\n\nLab assignments (first individual, later team-based)\nHomework assignments (individual)\nTwo exams\nFinal group project"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n35%\n\n\nFinal project\n15%\n\n\nLab\n15%\n\n\nExam 01\n15%\n\n\nExam 02\n15%\n\n\nApplication Exercises\n2.5%\n\n\nTeamwork\n2.5%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/01-welcome.html#support",
    "href": "slides/01-welcome.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours to meet with a member of the teaching team\n\nProf.¬†Tackett‚Äôs office hours start Fri, Sep 1, 1 - 3pm\nFull office hours schedule starts Tue, Sep 5\n\nAsk and answer questions on course discussion forum\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/01-welcome.html#diversity-inclusion",
    "href": "slides/01-welcome.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\n\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know.\nPlease let me know your preferred pronouns, if you are comfortable sharing.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said or done in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/01-welcome.html#accessibility",
    "href": "slides/01-welcome.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nIf you have documented accommodations from SDAO, please send the documentation as soon as possible.\nI am committed to making all course activities and materials accessible. If any course component is not accessible to you in any way, please don‚Äôt hesitate to let me know."
  },
  {
    "objectID": "slides/01-welcome.html#covid-19-and-other-illness",
    "href": "slides/01-welcome.html#covid-19-and-other-illness",
    "title": "Welcome to STA 210!",
    "section": "COVID-19 and other illness",
    "text": "COVID-19 and other illness\n\nPlease do not come to class if you have tested positive for COVID-19, have possible symptoms and have not yet been tested, or have other illness.\nLecture recordings are available for excused absences. See Lecture recording request in the syllabus for more information and a link to the request form.\nRead and follow the university guidelines regarding COVID-19 at coronavirus.duke.edu."
  },
  {
    "objectID": "slides/01-welcome.html#late-work-waivers-and-regrade-requests",
    "href": "slides/01-welcome.html#late-work-waivers-and-regrade-requests",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, and regrade requests",
    "text": "Late work, waivers, and regrade requests\n\nWe have policies! We will discuss them in detail when the first assignment is released.\nRead more about them in the Course policies section of the syllabus and refer back to them as needed\nIf you have questions, email sta210@duke.edu"
  },
  {
    "objectID": "slides/01-welcome.html#academic-integrity",
    "href": "slides/01-welcome.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the Duke Community Standard."
  },
  {
    "objectID": "slides/01-welcome.html#collaboration-sharing-code",
    "href": "slides/01-welcome.html#collaboration-sharing-code",
    "title": "Welcome to STA 210!",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nWe have policies! We will discuss them in detail when the first assignment is released.\nRead about them in the Academic honesty section of the syllabus and refer to them as needed"
  },
  {
    "objectID": "slides/01-welcome.html#use-of-artificial-intelligence-ai",
    "href": "slides/01-welcome.html#use-of-artificial-intelligence-ai",
    "title": "Welcome to STA 210!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é"
  },
  {
    "objectID": "slides/01-welcome.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/01-welcome.html#use-of-artificial-intelligence-ai-1",
    "title": "Welcome to STA 210!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n‚úÖ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code.\n‚ùå No AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments.\n\n\n\n\n\n\nImportant\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content."
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success",
    "href": "slides/01-welcome.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions.\nDo the homework and labs; get started on homework early when possible.\nDon‚Äôt procrastinate and don‚Äôt let a week pass by with lingering questions.\nStay up-to-date on announcements on Ed Discussion and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#application-exercise",
    "href": "slides/01-welcome.html#application-exercise",
    "title": "Welcome to STA 210!",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 01 - Movie Budgets and Revenues"
  },
  {
    "objectID": "slides/01-welcome.html#for-this-week",
    "href": "slides/01-welcome.html#for-this-week",
    "title": "Welcome to STA 210!",
    "section": "For this week‚Ä¶",
    "text": "For this week‚Ä¶\n\nRead the syllabus\nSee the course schedule for an overview of the semester\nLabs start this week!\n\nSection 001: Labs on Tuesday\nSection 002: Labs on Thursday\n\nThis week‚Äôs lab is focused on introductions and computing\nWednesday‚Äôs lecture: The Big Picture\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/10-exam-01-review.html#announcements",
    "href": "slides/10-exam-01-review.html#announcements",
    "title": "Exam 01 review",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due TODAY at 11:59pm.\nAll lecture recordings available until Wed, Oct 4 at 9am.\n\nClick here for link to videos. You can also find the link in the navigation bar of the course website.\n\nNo labs this week\nNo office hours Wednesday - Friday\nEd Discussion archived (no new posts) Wednesday - Friday"
  },
  {
    "objectID": "slides/10-exam-01-review.html#exam-01",
    "href": "slides/10-exam-01-review.html#exam-01",
    "title": "Exam 01 review",
    "section": "Exam 01",
    "text": "Exam 01\n\n15% of final course grade\n50 points total\n\nin-class: 40 points\ntake-home: 10 points\n\nIn-class: 75 minutes during Wed, Oct 4 lecture\nTake-home:\n\nReleased: Wed, Oct 4 ~ 3pm\nDue: Fri, Oct 6 at 11:59pm\nSimilar in format to a lab/ HW\n\nIf you miss any part of the exam for an excused absence (with academic dean‚Äôs note), then Exam 02 score will be counted twice"
  },
  {
    "objectID": "slides/10-exam-01-review.html#content-weeks-1---5",
    "href": "slides/10-exam-01-review.html#content-weeks-1---5",
    "title": "Exam 01 review",
    "section": "Content: Weeks 1 - 5",
    "text": "Content: Weeks 1 - 5\n\n\n\nExploratory data analysis\nFitting and interpreting simple linear regression models\nStatistical models and regression equations\nSimulation-based inference\nMathematical models for inference\n\n\n\nPrediction\nModel conditions\nModel evaluation\nBasics of multiple linear regression"
  },
  {
    "objectID": "slides/10-exam-01-review.html#outline-of-in-class-portion",
    "href": "slides/10-exam-01-review.html#outline-of-in-class-portion",
    "title": "Exam 01 review",
    "section": "Outline of in-class portion",
    "text": "Outline of in-class portion\n\nClosed-book, closed-note.\n10 questions, some with multiple parts\nQuestion types:\n\nMultiple choice\nShort answer (no more than 3 sentences)\nTrue/ False.\n\nIf false, write 1 - 2 sentence justification about why it is false.\n\nEvaluate a response.\n\nLast page (front and back) will have analysis output.\nJust need a pencil. No calculator permitted on exam."
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#announcements",
    "href": "slides/05-slr-sim-testing.html#announcements",
    "title": "SLR: Randomization test for the slope",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due\n\nFriday, Sep 15 at 11:59pm (Tuesday labs)\nSunday, Sep 17 at 11:59pm (Thursday labs)\n\nHW 01\n\nReleased later today (will get email when HW is available)\ndue Wed, Sep 20 at 11:59pm\n\nStatistics experience - due Mon, Nov 20 at 11:59pm"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#topics",
    "href": "slides/05-slr-sim-testing.html#topics",
    "title": "SLR: Randomization test for the slope",
    "section": "Topics",
    "text": "Topics\n\nEvaluate a claim about the slope using hypothesis testing\nDefine mathematical models to conduct inference for slope"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#computational-setup",
    "href": "slides/05-slr-sim-testing.html#computational-setup",
    "title": "SLR: Randomization test for the slope",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#data-duke-forest-houses",
    "href": "slides/05-slr-sim-testing.html#data-duke-forest-houses",
    "title": "SLR: Randomization test for the slope",
    "section": "Data: Duke Forest houses",
    "text": "Data: Duke Forest houses"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#the-regression-model",
    "href": "slides/05-slr-sim-testing.html#the-regression-model",
    "title": "SLR: Randomization test for the slope",
    "section": "The regression model",
    "text": "The regression model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\nSlope: For each additional square foot, we expect the sale price of Duke Forest houses to be higher by $159, on average."
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#inference-for-simple-linear-regression",
    "href": "slides/05-slr-sim-testing.html#inference-for-simple-linear-regression",
    "title": "SLR: Randomization test for the slope",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#sampling-is-natural",
    "href": "slides/05-slr-sim-testing.html#sampling-is-natural",
    "title": "SLR: Randomization test for the slope",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn‚Äôt salty enough, that‚Äôs exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that‚Äôs an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#confidence-interval-via-bootstrapping",
    "href": "slides/05-slr-sim-testing.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Randomization test for the slope",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-i",
    "href": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-i",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline I",
    "text": "Bootstrapping pipeline I\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 √ó 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ñπ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-ii",
    "href": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-ii",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline II",
    "text": "Bootstrapping pipeline II\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate   price  area\n       &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1         1  290000  2414\n 2         1  285000  2108\n 3         1  265000  1300\n 4         1  416000  2949\n 5         1  541000  2740\n 6         1  525000  2256\n 7         1 1270000  3909\n 8         1  265000  1300\n 9         1  815000  3904\n10         1  535000  2937\n# ‚Ñπ 97,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-iii",
    "href": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-iii",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline III",
    "text": "Bootstrapping pipeline III\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\n# A tibble: 2,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   80699.\n 2         1 area          168.\n 3         2 intercept  -18821.\n 4         2 area          205.\n 5         3 intercept  234297.\n 6         3 area          117.\n 7         4 intercept  134481.\n 8         4 area          150.\n 9         5 intercept   23861.\n10         5 area          190.\n# ‚Ñπ 1,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-iv",
    "href": "slides/05-slr-sim-testing.html#bootstrapping-pipeline-iv",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline IV",
    "text": "Bootstrapping pipeline IV\n\nset.seed(210)\n\nboot_dist &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#visualize-the-bootstrap-distribution",
    "href": "slides/05-slr-sim-testing.html#visualize-the-bootstrap-distribution",
    "title": "SLR: Randomization test for the slope",
    "section": "Visualize the bootstrap distribution",
    "text": "Visualize the bootstrap distribution\n\nboot_dist |&gt;\n  filter(term == \"area\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10)"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#compute-the-ci",
    "href": "slides/05-slr-sim-testing.html#compute-the-ci",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute the CI",
    "text": "Compute the CI"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#but-first",
    "href": "slides/05-slr-sim-testing.html#but-first",
    "title": "SLR: Randomization test for the slope",
    "section": "But first‚Ä¶",
    "text": "But first‚Ä¶\n\nobs_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobs_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#compute-95-confidence-interval",
    "href": "slides/05-slr-sim-testing.html#compute-95-confidence-interval",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute 95% confidence interval",
    "text": "Compute 95% confidence interval\n\nboot_dist |&gt;\n  get_confidence_interval(\n    point_estimate = obs_fit,\n    level = 0.95,\n    type = \"percentile\"\n  )\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          91.7     211.\n2 intercept -18290.   287711."
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#research-question-and-hypotheses",
    "href": "slides/05-slr-sim-testing.html#research-question-and-hypotheses",
    "title": "SLR: Randomization test for the slope",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù\n\nNull hypothesis: there is no linear relationship between area and price\n\\[\nH_0: \\beta_1 = 0\n\\]\n\n\nAlternative hypothesis: there is a linear relationship between area and price\n\\[\nH_A: \\beta_1 \\ne 0\n\\]"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/05-slr-sim-testing.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Randomization test for the slope",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: ‚ÄúCould these data plausibly have happened by chance if the null hypothesis were true?‚Äù\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#hypothesis-testing-framework",
    "href": "slides/05-slr-sim-testing.html#hypothesis-testing-framework",
    "title": "SLR: Randomization test for the slope",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e.¬†claim we‚Äôre testing\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of getting the observed or a more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#quantify-the-variability-of-the-slope",
    "href": "slides/05-slr-sim-testing.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Randomization test for the slope",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor testing\n\n\nTwo approaches:\n\nVia simulation\nVia mathematical models\n\nUse Randomization to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-described",
    "href": "slides/05-slr-sim-testing.html#permutation-described",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nUse permuting to simulate data under the assumption the null hypothesis is true and measure the natural variability in the data due to sampling, not due to variables being correlated\n\nPermute one variable to eliminate any existing relationship between the variables\n\nEach price value is randomly assigned to the area of a given house, i.e.¬†area and price are no longer matched for a given house\n\n\n\n\n# A tibble: 98 √ó 3\n   price_Observed price_Permuted  area\n            &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1        1520000         342500  6040\n 2        1030000         750000  4475\n 3         420000         645000  1745\n 4         680000         697500  2091\n 5         428500         428500  1772\n 6         456000         481000  1950\n 7        1270000         610000  3909\n 8         557450         680000  2841\n 9         697500         485000  3924\n10         650000         105000  2173\n# ‚Ñπ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-visualized",
    "href": "slides/05-slr-sim-testing.html#permutation-visualized",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nThe permutation removes the relationship between area and price\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-repeated",
    "href": "slides/05-slr-sim-testing.html#permutation-repeated",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#concluding-the-hypothesis-test",
    "href": "slides/05-slr-sim-testing.html#concluding-the-hypothesis-test",
    "title": "SLR: Randomization test for the slope",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 159\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: ‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-pipeline-i",
    "href": "slides/05-slr-sim-testing.html#permutation-pipeline-i",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 √ó 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ñπ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-pipeline-ii",
    "href": "slides/05-slr-sim-testing.html#permutation-pipeline-ii",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98 √ó 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ñπ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-pipeline-iii",
    "href": "slides/05-slr-sim-testing.html#permutation-pipeline-iii",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98,000 √ó 3\n# Groups:   replicate [1,000]\n     price  area replicate\n     &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;\n 1  465000  6040         1\n 2  481000  4475         1\n 3 1020000  1745         1\n 4  520000  2091         1\n 5  592000  1772         1\n 6  650000  1950         1\n 7  473000  3909         1\n 8  705000  2841         1\n 9  785000  3924         1\n10  671500  2173         1\n# ‚Ñπ 97,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-pipeline-iv",
    "href": "slides/05-slr-sim-testing.html#permutation-pipeline-iv",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# A tibble: 2,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate term       estimate\n       &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1         1 intercept 553355.  \n 2         1 area           2.35\n 3         2 intercept 635824.  \n 4         2 area         -27.3 \n 5         3 intercept 536072.  \n 6         3 area           8.57\n 7         4 intercept 598649.  \n 8         4 area         -13.9 \n 9         5 intercept 556202.  \n10         5 area           1.33\n# ‚Ñπ 1,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#permutation-pipeline-v",
    "href": "slides/05-slr-sim-testing.html#permutation-pipeline-v",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\nset.seed(1125)\n\nnull_dist &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#visualize-the-null-distribution",
    "href": "slides/05-slr-sim-testing.html#visualize-the-null-distribution",
    "title": "SLR: Randomization test for the slope",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\nnull_dist |&gt;\n  filter(term == \"area\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10, color = \"white\")"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#reason-around-the-p-value",
    "href": "slides/05-slr-sim-testing.html#reason-around-the-p-value",
    "title": "SLR: Randomization test for the slope",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the area of a Duke Forest house and in its price (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 98 houses where the slope fo the model predicting price from area is 159 or even more extreme?\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#compute-the-p-value",
    "href": "slides/05-slr-sim-testing.html#compute-the-p-value",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = obs_fit,\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 2 √ó 2\n  term      p_value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 area            0\n2 intercept       0"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#the-regression-model-revisited",
    "href": "slides/05-slr-sim-testing.html#the-regression-model-revisited",
    "title": "SLR: Randomization test for the slope",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#inference-revisited",
    "href": "slides/05-slr-sim-testing.html#inference-revisited",
    "title": "SLR: Randomization test for the slope",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we‚Äôll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#mathematical-representation-of-the-model",
    "href": "slides/05-slr-sim-testing.html#mathematical-representation-of-the-model",
    "title": "SLR: Randomization test for the slope",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn‚Äôt tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#mathematical-representation-visualized",
    "href": "slides/05-slr-sim-testing.html#mathematical-representation-visualized",
    "title": "SLR: Randomization test for the slope",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nGraph reproduced from Beyond Multiple Linear Regression.\n\n\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#regression-standard-error",
    "href": "slides/05-slr-sim-testing.html#regression-standard-error",
    "title": "SLR: Randomization test for the slope",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error (the spread of the distribution of the response, for a given value of the predictor variable):\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/05-slr-sim-testing.html#standard-error-of-hatbeta_1",
    "href": "slides/05-slr-sim-testing.html#standard-error-of-hatbeta_1",
    "title": "SLR: Randomization test for the slope",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\nor‚Ä¶\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/12-model-workflow.html#announcements",
    "href": "slides/12-model-workflow.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nLab 05 due October 20\nProf.¬†Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule\n\n\n\nüçÅ Have a good Fall Break! üçÅ"
  },
  {
    "objectID": "slides/12-model-workflow.html#topics",
    "href": "slides/12-model-workflow.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\n\nFeature engineering with recipes\nWorkflows to bring together models and recipes\nRMSE and \\(R^2\\) for model evaluation on training and tests sets"
  },
  {
    "objectID": "slides/12-model-workflow.html#computational-setup",
    "href": "slides/12-model-workflow.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/12-model-workflow.html#the-office",
    "href": "slides/12-model-workflow.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/12-model-workflow.html#data-goal",
    "href": "slides/12-model-workflow.html#data-goal",
    "title": "Feature engineering",
    "section": "Data & goal",
    "text": "Data & goal\n\nData: The data come from data.world, by way of TidyTuesday\nGoal: Predict imdb_rating from other variables in the dataset\n\n\n\n# A tibble: 188 √ó 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ‚Ñπ 178 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#train-test",
    "href": "slides/12-model-workflow.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/12-model-workflow.html#training-data",
    "href": "slides/12-model-workflow.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 √ó 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ñπ 131 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#recap-feature-engineering",
    "href": "slides/12-model-workflow.html#recap-feature-engineering",
    "title": "Feature engineering",
    "section": "Recap: Feature engineering",
    "text": "Recap: Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "slides/12-model-workflow.html#recap-modeling-workflow-revisited",
    "href": "slides/12-model-workflow.html#recap-modeling-workflow-revisited",
    "title": "Feature engineering",
    "section": "Recap: Modeling workflow, revisited",
    "text": "Recap: Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/12-model-workflow.html#initiate-a-recipe",
    "href": "slides/12-model-workflow.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-1-alter-roles",
    "href": "slides/12-model-workflow.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn‚Äôt a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-2-add-features",
    "href": "slides/12-model-workflow.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date"
  },
  {
    "objectID": "slides/12-model-workflow.html#working-with-recipes",
    "href": "slides/12-model-workflow.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don‚Äôt get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\n\nNote\n\n\nThis is optional, we‚Äôll show the results for demonstrative purposes. It doesn‚Äôt need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/12-model-workflow.html#step-2-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-2-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 2: Prep and bake",
    "text": "Step 2: Prep and bake\n\n# determine required parameters to be estimated\noffice_rec |&gt;\n  prep() |&gt;\n# apply recipe computations to data\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 8\n$ season         &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2‚Ä¶\n$ episode        &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20,‚Ä¶\n$ title          &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Review‚Ä¶\n$ total_votes    &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1‚Ä¶\n$ air_date       &lt;date&gt; 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-0‚Ä¶\n$ imdb_rating    &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, ‚Ä¶\n$ air_date_dow   &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, ‚Ä¶\n$ air_date_month &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr, Jan, May, ‚Ä¶"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-3-add-more-features",
    "href": "slides/12-model-workflow.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-3-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-3-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 3: Prep and bake",
    "text": "Step 3: Prep and bake\n\noffice_rec |&gt;\n  prep() |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7‚Ä¶\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26‚Ä¶\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor‚Ä¶\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2‚Ä¶\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0‚Ä¶\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu‚Ä¶\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr‚Ä¶\n$ air_date_USThanksgivingDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USChristmasDay    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USNewYearsDay     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USIndependenceDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-4-convert-numbers-to-factors",
    "href": "slides/12-model-workflow.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-4-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-4-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 4: Prep and bake",
    "text": "Step 4: Prep and bake\n\noffice_rec |&gt;\n  prep() |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;fct&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7‚Ä¶\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26‚Ä¶\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor‚Ä¶\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2‚Ä¶\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0‚Ä¶\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu‚Ä¶\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr‚Ä¶\n$ air_date_USThanksgivingDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USChristmasDay    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USNewYearsDay     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USIndependenceDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-5-make-dummy-variables",
    "href": "slides/12-model-workflow.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-5-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-5-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 5: Prep and bake",
    "text": "Step 5: Prep and bake\n\n# determine required parameters to be estimated\noffice_rec |&gt;\n  prep() |&gt;\n# apply recipe computations to data\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 33\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26‚Ä¶\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor‚Ä¶\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2‚Ä¶\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0‚Ä¶\n$ air_date_USThanksgivingDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USChristmasDay    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USNewYearsDay     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USIndependenceDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X2                  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X3                  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X4                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X5                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0‚Ä¶\n$ season_X6                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n$ season_X7                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1‚Ä¶\n$ season_X8                  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0‚Ä¶\n$ season_X9                  &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0‚Ä¶\n$ air_date_dow_Mon           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Tue           &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Wed           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Thu           &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ air_date_dow_Fri           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Sat           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Feb         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Mar         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Apr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1‚Ä¶\n$ air_date_month_May         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0‚Ä¶\n$ air_date_month_Jun         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Jul         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Aug         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Sep         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n$ air_date_month_Oct         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Nov         &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Dec         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-6-remove-zero-variance-predictors",
    "href": "slides/12-model-workflow.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()\n\n\n‚Ä¢ Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-6-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-6-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 6: Prep and bake",
    "text": "Step 6: Prep and bake\n\noffice_rec |&gt;\n  prep() |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 22\n$ episode            &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,‚Ä¶\n$ title              &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Re‚Ä¶\n$ total_votes        &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144‚Ä¶\n$ imdb_rating        &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8‚Ä¶\n$ season_X2          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X3          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X4          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X5          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, ‚Ä¶\n$ season_X6          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ‚Ä¶\n$ season_X7          &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ‚Ä¶\n$ season_X8          &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X9          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ air_date_dow_Tue   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_dow_Thu   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ air_date_month_Feb &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_month_Mar &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_month_Apr &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ‚Ä¶\n$ air_date_month_May &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_month_Sep &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ‚Ä¶\n$ air_date_month_Oct &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ‚Ä¶\n$ air_date_month_Nov &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ‚Ä¶\n$ air_date_month_Dec &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶"
  },
  {
    "objectID": "slides/12-model-workflow.html#putting-it-all-together",
    "href": "slides/12-model-workflow.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/12-model-workflow.html#putting-it-all-together-1",
    "href": "slides/12-model-workflow.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()\n\n\n‚Ä¢ Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/12-model-workflow.html#recipe-workflow",
    "href": "slides/12-model-workflow.html#recipe-workflow",
    "title": "Feature engineering",
    "section": "Recipe workflow",
    "text": "Recipe workflow\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/12-model-workflow.html#specify-model",
    "href": "slides/12-model-workflow.html#specify-model",
    "title": "Feature engineering",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/12-model-workflow.html#build-workflow",
    "href": "slides/12-model-workflow.html#build-workflow",
    "title": "Feature engineering",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\noffice_wflow &lt;- workflow() |&gt;\n  add_model(office_spec) |&gt;\n  add_recipe(office_rec)\n\n\nSee next slide for workflow‚Ä¶"
  },
  {
    "objectID": "slides/12-model-workflow.html#view-workflow",
    "href": "slides/12-model-workflow.html#view-workflow",
    "title": "Feature engineering",
    "section": "View workflow",
    "text": "View workflow\n\noffice_wflow\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_date()\n‚Ä¢ step_holiday()\n‚Ä¢ step_num2factor()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/12-model-workflow.html#fit-model-to-training-data",
    "href": "slides/12-model-workflow.html#fit-model-to-training-data",
    "title": "Feature engineering",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\noffice_fit &lt;- office_wflow |&gt;\n  fit(data = office_train)\n\ntidy(office_fit)\n\n# A tibble: 21 √ó 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  6.40     0.510        12.5   1.51e-23\n 2 episode     -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes  0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2    0.811    0.327         2.48  1.44e- 2\n 5 season_X3    1.04     0.343         3.04  2.91e- 3\n 6 season_X4    1.09     0.295         3.70  3.32e- 4\n 7 season_X5    1.08     0.348         3.11  2.34e- 3\n 8 season_X6    1.00     0.367         2.74  7.18e- 3\n 9 season_X7    1.02     0.352         2.89  4.52e- 3\n10 season_X8    0.497    0.348         1.43  1.55e- 1\n# ‚Ñπ 11 more rows\n\n\n\n\nSo many predictors!"
  },
  {
    "objectID": "slides/12-model-workflow.html#model-fit-summary",
    "href": "slides/12-model-workflow.html#model-fit-summary",
    "title": "Feature engineering",
    "section": "Model fit summary",
    "text": "Model fit summary\n\ntidy(office_fit) |&gt; print(n = 21)\n\n# A tibble: 21 √ó 5\n   term                estimate std.error statistic  p.value\n   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)         6.40     0.510        12.5   1.51e-23\n 2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2           0.811    0.327         2.48  1.44e- 2\n 5 season_X3           1.04     0.343         3.04  2.91e- 3\n 6 season_X4           1.09     0.295         3.70  3.32e- 4\n 7 season_X5           1.08     0.348         3.11  2.34e- 3\n 8 season_X6           1.00     0.367         2.74  7.18e- 3\n 9 season_X7           1.02     0.352         2.89  4.52e- 3\n10 season_X8           0.497    0.348         1.43  1.55e- 1\n11 season_X9           0.621    0.345         1.80  7.41e- 2\n12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"
  },
  {
    "objectID": "slides/12-model-workflow.html#make-predictions-for-training-data",
    "href": "slides/12-model-workflow.html#make-predictions-for-training-data",
    "title": "Feature engineering",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\noffice_train_pred &lt;- predict(office_fit, office_train) |&gt;\n  bind_cols(office_train)\n\noffice_train_pred\n\n# A tibble: 141 √ó 7\n   .pred season episode title               imdb_rating total_votes air_date  \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1  7.57      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2  7.77      9      14 Vandalism                   7.6        1402 2013-01-31\n 3  8.31      2       8 Performance Review          8.2        2416 2005-11-15\n 4  7.67      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5  8.84      3      22 Beach Games                 9.1        2783 2007-05-10\n 6  8.33      7       1 Nepotism                    8.4        1897 2010-09-23\n 7  8.46      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8  8.14      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9  7.87      9      18 Promos                      8          1445 2013-04-04\n10  7.74      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ñπ 131 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#r-squared",
    "href": "slides/12-model-workflow.html#r-squared",
    "title": "Feature engineering",
    "section": "R-squared",
    "text": "R-squared\nPercentage of variability in the IMDB ratings explained by the model.\n\n\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.670\n\n\n\n\n\nAre models with high or low \\(R^2\\) more preferable?"
  },
  {
    "objectID": "slides/12-model-workflow.html#rmse",
    "href": "slides/12-model-workflow.html#rmse",
    "title": "Feature engineering",
    "section": "RMSE",
    "text": "RMSE\nAn alternative model performance statistic: root mean square error.\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} \\]\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\n\nAre models with high or low RMSE are more preferable?"
  },
  {
    "objectID": "slides/12-model-workflow.html#interpreting-rmse",
    "href": "slides/12-model-workflow.html#interpreting-rmse",
    "title": "Feature engineering",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\n\nIs this RMSE considered low or high?\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\nDepends‚Ä¶\n\noffice_train |&gt;\n  summarise(min = min(imdb_rating), max = max(imdb_rating))\n\n# A tibble: 1 √ó 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7"
  },
  {
    "objectID": "slides/12-model-workflow.html#but-really",
    "href": "slides/12-model-workflow.html#but-really",
    "title": "Feature engineering",
    "section": "But, really‚Ä¶",
    "text": "But, really‚Ä¶\nwho cares about predictions on training data?"
  },
  {
    "objectID": "slides/12-model-workflow.html#make-predictions-for-testing-data",
    "href": "slides/12-model-workflow.html#make-predictions-for-testing-data",
    "title": "Feature engineering",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\noffice_test_pred &lt;- predict(office_fit, office_test) |&gt;\n  bind_cols(office_test)\n\noffice_test_pred\n\n# A tibble: 47 √ó 7\n   .pred season episode title               imdb_rating total_votes air_date  \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1  8.03      1       2 Diversity Day               8.3        3566 2005-03-29\n 2  7.98      1       3 Health Care                 7.9        2983 2005-04-05\n 3  8.41      2       4 The Fire                    8.4        2713 2005-10-11\n 4  8.35      2       5 Halloween                   8.2        2561 2005-10-18\n 5  8.35      2       9 E-Mail Surveillance         8.4        2527 2005-11-22\n 6  8.68      2      12 The Injury                  9          3282 2006-01-12\n 7  8.32      2      14 The Carpet                  7.9        2342 2006-01-26\n 8  8.93      2      22 Casino Night                9.3        3644 2006-05-11\n 9  8.80      3       1 Gay Witch Hunt              8.9        3087 2006-09-21\n10  8.37      3       5 Initiation                  8.2        2254 2006-10-19\n# ‚Ñπ 37 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#evaluate-performance-for-testing-data",
    "href": "slides/12-model-workflow.html#evaluate-performance-for-testing-data",
    "title": "Feature engineering",
    "section": "Evaluate performance for testing data",
    "text": "Evaluate performance for testing data\nRMSE of model fit to testing data\n\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.411\n\n\nR-sq of model fit to testing data\n\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.468"
  },
  {
    "objectID": "slides/12-model-workflow.html#training-vs.-testing",
    "href": "slides/12-model-workflow.html#training-vs.-testing",
    "title": "Feature engineering",
    "section": "Training vs.¬†testing",
    "text": "Training vs.¬†testing\n\n\n\n\n\n\n\n\n\nmetric\ntrain\ntest\ncomparison\n\n\n\n\nRMSE\n0.302\n0.411\nRMSE lower for training\n\n\nR-squared\n0.67\n0.468\nR-squared higher for training"
  },
  {
    "objectID": "slides/12-model-workflow.html#evaluating-performance-on-training-data",
    "href": "slides/12-model-workflow.html#evaluating-performance-on-training-data",
    "title": "Feature engineering",
    "section": "Evaluating performance on training data",
    "text": "Evaluating performance on training data\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test."
  },
  {
    "objectID": "slides/08-slr-model-eval.html#announcements",
    "href": "slides/08-slr-model-eval.html#announcements",
    "title": "SLR: Model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due Mon, Oct 2 at 11:59pm. (released after section 002)\nAll lecture recordings available until Wed, Oct 4 at 9am.\n\nClick here for link to videos. You can also find the link in the navigation bar of the course website.\n\nLab groups start this week. You will get your assigned group when you go to lab.\nLooking ahead: Exam 01:\n\nClosed note in-class: Wed, Oct 4\nOpen note take-home: Wed, Oct 4 - Fri, Oct 6\n\nReleased after Section 002\n\nExam review: Mon, Oct 2"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#statistician-of-the-day-robert-santos",
    "href": "slides/08-slr-model-eval.html#statistician-of-the-day-robert-santos",
    "title": "SLR: Model evaluation",
    "section": "Statistician of the day: Robert Santos",
    "text": "Statistician of the day: Robert Santos\n\n\n\n\nRobert Santos received an MA in Statistics from the University of Michigan, Ann Arbor. He served as president of the American Statistical Association in 2021. As a survey researcher, he worked at National Opinion Research Center (NORC, University of Chicago) and the Urban Institute in Washington, DC.\nAs a Mexican-American, he is the first non-white person to serve as the Director of the US Census Bureau (appointed by Joe Biden and approved by the US Senate in 2022).\nSource: hardin47.github.io/CURV/scholars/santos.html"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#robert-santos",
    "href": "slides/08-slr-model-eval.html#robert-santos",
    "title": "SLR: Model evaluation",
    "section": "Robert Santos",
    "text": "Robert Santos\nSantos is a survey researcher with much of his recent focus on the US Census. In particular, he has written extensively about miscounting particular groups of people in the Census and the relationship between race and ethnicity in surveys.\nFrom his article ‚ÄúIs It Time to Postpone the 2020 Census?‚Äú (written during his time at the Urban Institute)\n\n‚ÄúThis would create a worst-case scenario when it comes to political representation and allocation of federal resources‚Ä¶And the 2020 counts would then be baked in to population projections used to calibrate federal statistics and surveys, thus informing federal funds allocations and eligibility thresholds for the next 10 years.\n\nRelated work: Interactive feature Who‚Äôs at Risk of Being Miscounted?"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#computational-set-up",
    "href": "slides/08-slr-model-eval.html#computational-set-up",
    "title": "SLR: Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#model-conditions-1",
    "href": "slides/08-slr-model-eval.html#model-conditions-1",
    "title": "SLR: Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#augmented-data-frame",
    "href": "slides/08-slr-model-eval.html#augmented-data-frame",
    "title": "SLR: Model evaluation",
    "section": "Augmented data frame",
    "text": "Augmented data frame\n\ndf_fit &lt;- linear_reg() |&gt;\n  fit(price ~ area, data = duke_forest)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nhead(df_aug)\n\n# A tibble: 6 √ó 8\n    price  area  .fitted  .resid   .hat  .sigma  .cooksd .std.resid\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80 \n2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21 \n3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150\n4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37 \n5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175\n6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#two-statistics",
    "href": "slides/08-slr-model-eval.html#two-statistics",
    "title": "SLR: Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#r2",
    "href": "slides/08-slr-model-eval.html#r2",
    "title": "SLR: Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#interpreting-r2",
    "href": "slides/08-slr-model-eval.html#interpreting-r2",
    "title": "SLR: Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\nüó≥Ô∏è Discussion\n\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price\n44.5% of the time price for houses in Duke Forest can be predicted by area."
  },
  {
    "objectID": "slides/08-slr-model-eval.html#alternative-approach-for-r2",
    "href": "slides/08-slr-model-eval.html#alternative-approach-for-r2",
    "title": "SLR: Model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(df_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nglance(df_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#rmse",
    "href": "slides/08-slr-model-eval.html#rmse",
    "title": "SLR: Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\nThe value of RMSE is not very meaningful on its own, but it‚Äôs useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#obtaining-r2-and-rmse",
    "href": "slides/08-slr-model-eval.html#obtaining-r2-and-rmse",
    "title": "SLR: Model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(df_aug, truth = price, estimate = .fitted)\nrmse(df_aug, truth = price, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#announcements",
    "href": "slides/07-slr-conditions-eval.html#announcements",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nHW 01: due TODAY at 11:59pm\nLab 03:\n\ndue Fri at 11:59pm (Tue labs)\ndue Sun at 11:59pm (Thu labs)\n\nLooking ahead: Exam 01:\n\nClosed note in-class: Wed, Oct 4\nOpen note take-home: Wed, Oct 4 - Fri, Oct 6\n\nReleased after Section 002\n\nMore about the exam next week"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#computational-set-up",
    "href": "slides/07-slr-conditions-eval.html#computational-set-up",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#regression-model-revisited",
    "href": "slides/07-slr-conditions-eval.html#regression-model-revisited",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#mathematical-representation-visualized",
    "href": "slides/07-slr-conditions-eval.html#mathematical-representation-visualized",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#model-conditions-1",
    "href": "slides/07-slr-conditions-eval.html#model-conditions-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#linearity",
    "href": "slides/07-slr-conditions-eval.html#linearity",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\) adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals vs.¬†the fitted values\nLinearity satisfied if there is no distinguishable pattern in the residuals plot, i.e.¬†the residuals should be randomly scattered\nA non-random pattern (e.g.¬†a parabola) suggests a linear model does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#linearity-1",
    "href": "slides/07-slr-conditions-eval.html#linearity-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n‚úÖ The residuals vs.¬†fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#residuals-vs.-fitted-values-code",
    "href": "slides/07-slr-conditions-eval.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Residuals vs.¬†fitted values (code)",
    "text": "Residuals vs.¬†fitted values (code)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#non-linear-relationships",
    "href": "slides/07-slr-conditions-eval.html#non-linear-relationships",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#constant-variance",
    "href": "slides/07-slr-conditions-eval.html#constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\)then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs.¬†the fitted values\nConstant variance satisfied if the vertical spread of the residuals is approximately equal as you move from left to right (i.e.¬†there is no ‚Äúfan‚Äù pattern)\nA fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#constant-variance-1",
    "href": "slides/07-slr-conditions-eval.html#constant-variance-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n‚úÖ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#non-constant-variance",
    "href": "slides/07-slr-conditions-eval.html#non-constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-constant variance",
    "text": "Non-constant variance"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#normality",
    "href": "slides/07-slr-conditions-eval.html#normality",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nNormality satisfied if a histogram of the residuals is approximately normal\n\nCan also check that the points on a normal QQ-plot falls along a diagonal line\n\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#normality-1",
    "href": "slides/07-slr-conditions-eval.html#normality-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#check-normality-using-a-qq-plot",
    "href": "slides/07-slr-conditions-eval.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\nggplot(df_aug, aes(x = .resid)) +\n  geom_histogram(binwidth = 50000, color = \"white\")  +\n  labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df_aug, aes(sample = .resid)) +\n  stat_qq()+\n  stat_qq_line() + \n  labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#normality-2",
    "href": "slides/07-slr-conditions-eval.html#normality-2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\n‚ùå The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n‚úÖ The sample size \\(n = 98 &gt; 30\\), so the sample size is large enough to relax this condition and proceed with inference."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#independence",
    "href": "slides/07-slr-conditions-eval.html#independence",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nSerial Effect: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of house), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#independence-1",
    "href": "slides/07-slr-conditions-eval.html#independence-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\nRecall the description of the data:\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\n\n\n‚úÖ Based on the information we have, we can reasonably treat this as a random sample of Duke Forest Houses and assume the error for one house does not tell us anything about the error for another house."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#recap",
    "href": "slides/07-slr-conditions-eval.html#recap",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\n\nEd Discussion [Section 001][Section 002]\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#comparing-inferential-methods",
    "href": "slides/07-slr-conditions-eval.html#comparing-inferential-methods",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Comparing inferential methods",
    "text": "Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#two-statistics",
    "href": "slides/07-slr-conditions-eval.html#two-statistics",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#r2",
    "href": "slides/07-slr-conditions-eval.html#r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#interpreting-r2",
    "href": "slides/07-slr-conditions-eval.html#interpreting-r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\nüó≥Ô∏è Discussion\n\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price\n44.5% of the time price for houses in Duke Forest can be predicted by area."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#alternative-approach-for-r2",
    "href": "slides/07-slr-conditions-eval.html#alternative-approach-for-r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(df_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nglance(df_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#rmse",
    "href": "slides/07-slr-conditions-eval.html#rmse",
    "title": "SLR: Conditions + Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\nThe value of RMSE is not very meaningful on its own, but it‚Äôs useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#obtaining-r2-and-rmse",
    "href": "slides/07-slr-conditions-eval.html#obtaining-r2-and-rmse",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(df_aug, truth = price, estimate = .fitted)\nrmse(df_aug, truth = price, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#announcements",
    "href": "slides/11-feature-engineering.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nGroup labs resume this week\nProf.¬†Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule"
  },
  {
    "objectID": "slides/11-feature-engineering.html#statistician-of-the-day-rafael-irizarry",
    "href": "slides/11-feature-engineering.html#statistician-of-the-day-rafael-irizarry",
    "title": "Feature engineering",
    "section": "Statistician of the day: Rafael Irizarry",
    "text": "Statistician of the day: Rafael Irizarry\n\n\nDr.¬†Rafael Irizarry is a Professor of Biostatistics at the Harvard T.H. Chan School of Public Health and Professor of Biostatistics and Computational Biology at the Dana-Farber Cancer Institute. He earned a Bachelor of Science degree in Mathematics from the University of Puerto Rico at Rio Piedras and a PhD from the University of California, Berkeley in Statistics. Dr.¬†Irizarry‚Äôs work is highly cited, and he has been given many prestigious awards including the Presidents‚Äô Award given by the Committee of Presidents of Statistical Societies.\n\n\n\n\n\n\n\n\nSource: hardin47.github.io/CURV/scholars/irizarry"
  },
  {
    "objectID": "slides/11-feature-engineering.html#work-on-impacts-of-hurricane-maria",
    "href": "slides/11-feature-engineering.html#work-on-impacts-of-hurricane-maria",
    "title": "Feature engineering",
    "section": "Work on impacts of Hurricane Maria",
    "text": "Work on impacts of Hurricane Maria\n\nPart of a team that used stratified sampling to survey residents in Puerto Rico about the impacts of the 2017 Hurricane Maria\nEstimated percent of population who lost access to services, such as electricity and water, and the association with remoteness\nUsed confidence intervals to estimate deaths that were directly and indirectly attributable to the hurricane\n\nTheir estimate was more than 70 times the official count\n\n\nArticle: Kishore, N., Marqu√©s, D., Mahmud, A., Kiang, M. V., Rodriguez, I., Fuller, A., ... & Buckee, C. O. (2018). Mortality in Puerto Rico after Hurricane Maria. New England journal of medicine, 379(2), 162-170.\nGitHub repo: github.com/c2-d2/pr_mort_official"
  },
  {
    "objectID": "slides/11-feature-engineering.html#topics",
    "href": "slides/11-feature-engineering.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nUnderstanding categorical predictors and interaction terms\nFeature engineering"
  },
  {
    "objectID": "slides/11-feature-engineering.html#computational-setup",
    "href": "slides/11-feature-engineering.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\nlibrary(gghighlight)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-peer-to-peer-lender",
    "href": "slides/11-feature-engineering.html#data-peer-to-peer-lender",
    "title": "Feature engineering",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday‚Äôs data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 √ó 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ‚Ñπ 40 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#variables",
    "href": "slides/11-feature-engineering.html#variables",
    "title": "Feature engineering",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/11-feature-engineering.html#response-interest_rate",
    "href": "slides/11-feature-engineering.html#response-interest_rate",
    "title": "Feature engineering",
    "section": "Response: interest_rate",
    "text": "Response: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/11-feature-engineering.html#predictors",
    "href": "slides/11-feature-engineering.html#predictors",
    "title": "Feature engineering",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-1-rescale-income",
    "href": "slides/11-feature-engineering.html#data-manipulation-1-rescale-income",
    "title": "Feature engineering",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/11-feature-engineering.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Feature engineering",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/11-feature-engineering.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Feature engineering",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interest-rate-vs.-annual-income",
    "href": "slides/11-feature-engineering.html#interest-rate-vs.-annual-income",
    "title": "Feature engineering",
    "section": "Interest rate vs.¬†annual income",
    "text": "Interest rate vs.¬†annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/11-feature-engineering.html#data-manipulation-4-create-interaction-variables",
    "title": "Feature engineering",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, ‚Ä¶\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.‚Ä¶\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, ‚Ä¶\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, ‚Ä¶\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, ‚Ä¶\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0‚Ä¶\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11‚Ä¶"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interaction-term-in-the-model",
    "href": "slides/11-feature-engineering.html#interaction-term-in-the-model",
    "title": "Feature engineering",
    "section": "Interaction term in the model",
    "text": "Interaction term in the model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent  +  verified_income + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interpreting-interaction-terms",
    "href": "slides/11-feature-engineering.html#interpreting-interaction-terms",
    "title": "Feature engineering",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/11-feature-engineering.html#understanding-the-model",
    "href": "slides/11-feature-engineering.html#understanding-the-model",
    "title": "Feature engineering",
    "section": "Understanding the model",
    "text": "Understanding the model\n\\[\n\\begin{aligned}\n\\hat{interest\\_rate} &= 9.484 + 0.691 \\times debt\\_inc\\_cent\\\\ &- 0.007 \\times annual\\_income\\_th\\_cent \\\\ &+ 2.157 \\times SourceVerified + 7.181 \\times Verified \\\\ &- 0.016 \\times annual\\_inc\\_th\\_cent \\times SourceVerified\\\\ &- 0.032 \\times annual\\_inc\\_th\\_cent \\times Verified\n\\end{aligned}\n\\]\n\n\nWhat is \\(p\\), the number of predictor terms in the model?\nWrite the equation of the model to predict interest rate for applicants with Not Verified income.\nWrite the equation of the model to predict interest rate for applicants with Verified income."
  },
  {
    "objectID": "slides/11-feature-engineering.html#the-office",
    "href": "slides/11-feature-engineering.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data",
    "href": "slides/11-feature-engineering.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\noffice_ratings\n\n# A tibble: 188 √ó 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ‚Ñπ 178 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings",
    "href": "slides/11-feature-engineering.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†number of votes",
    "text": "IMDB ratings vs.¬†number of votes"
  },
  {
    "objectID": "slides/11-feature-engineering.html#outliers",
    "href": "slides/11-feature-engineering.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-air-date",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-air-date",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†air date",
    "text": "IMDB ratings vs.¬†air date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-seasons",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†seasons",
    "text": "IMDB ratings vs.¬†seasons"
  },
  {
    "objectID": "slides/11-feature-engineering.html#spending-our-data",
    "href": "slides/11-feature-engineering.html#spending-our-data",
    "title": "Feature engineering",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we‚Äôve done so far)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#splitting-the-data",
    "href": "slides/11-feature-engineering.html#splitting-the-data",
    "title": "Feature engineering",
    "section": "Splitting the data",
    "text": "Splitting the data\n\nTake a random sample of X% of the data and set aside (testing data)\n\nTypically 10 - 20%\n\nFit a model on the remaining Y% of the data (training data)\n\nTypically 80 - 90%\n\nUse the coefficients from the model fit on training data to make predictions and evaluate performance on the testing data"
  },
  {
    "objectID": "slides/11-feature-engineering.html#train-test",
    "href": "slides/11-feature-engineering.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings, prop = 0.75) # prop = 0.75 by default\n\n\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/11-feature-engineering.html#training-data",
    "href": "slides/11-feature-engineering.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 √ó 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ñπ 131 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#feature-engineering-1",
    "href": "slides/11-feature-engineering.html#feature-engineering-1",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance and improve interpretability)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#feature-engineering-with-dplyr",
    "href": "slides/11-feature-engineering.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\noffice_train |&gt;\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 √ó 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 8           18 Last Day in Flo‚Ä¶         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev‚Ä¶         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb‚Ä¶         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# ‚Ñπ 135 more rows\n\n\n\n\nCan you identify any potential problems with this approach?"
  },
  {
    "objectID": "slides/11-feature-engineering.html#modeling-workflow",
    "href": "slides/11-feature-engineering.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\n\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/11-feature-engineering.html#initiate-a-recipe",
    "href": "slides/11-feature-engineering.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-1-alter-roles",
    "href": "slides/11-feature-engineering.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn‚Äôt a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-2-add-features",
    "href": "slides/11-feature-engineering.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-3-add-more-features",
    "href": "slides/11-feature-engineering.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-4-convert-numbers-to-factors",
    "href": "slides/11-feature-engineering.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-5-make-dummy-variables",
    "href": "slides/11-feature-engineering.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-6-remove-zero-variance-predictors",
    "href": "slides/11-feature-engineering.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()\n\n\n‚Ä¢ Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#putting-it-all-together",
    "href": "slides/11-feature-engineering.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/11-feature-engineering.html#putting-it-all-together-1",
    "href": "slides/11-feature-engineering.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Date features from: air_date\n\n\n‚Ä¢ Holiday features from: air_date\n\n\n‚Ä¢ Factor variables from: season\n\n\n‚Ä¢ Dummy variables from: all_nominal_predictors()\n\n\n‚Ä¢ Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#next-step",
    "href": "slides/11-feature-engineering.html#next-step",
    "title": "Feature engineering",
    "section": "Next step‚Ä¶",
    "text": "Next step‚Ä¶\nWe will complete the workflow to fit a model predicting IMDB ratings that includes the following predictors:\n\nepisode\ntotal_votes\nindicator variables for season\nindicator variables for day of week aired (created using air_date)\nindicator variables for month aired (created using air_date)\n\n\n\nWhat feature will not end up in the final model? Why is it not included?"
  },
  {
    "objectID": "slides/11-feature-engineering.html#working-with-recipes",
    "href": "slides/11-feature-engineering.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don‚Äôt get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\nNote\n\n\nThis is optional, we‚Äôll show the results for demonstrative purposes. It doesn‚Äôt need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/11-feature-engineering.html#recap",
    "href": "slides/11-feature-engineering.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\n\nReview: Training and testing splits\nFeature engineering with recipes\n\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#announcements",
    "href": "slides/18-logistic-prediction.html#announcements",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Announcements",
    "text": "Announcements\n\nLab this week: Work on project\nProject draft due in your GitHub repo at 9am on\n\nNovember 14 (Tuesday labs)\nNovember 16 (Thursday labs)\nWill do peer review in lab those days\n\nTeam Feedback #1 due Friday, November 10 at 11:5pm\n\nWill receive email from Teammates"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#odds-ratios-practice",
    "href": "slides/18-logistic-prediction.html#odds-ratios-practice",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Odds ratios practice",
    "text": "Odds ratios practice\nLet‚Äôs take a look at one of the models from Lab 06 using flipper length and species to predict the odds a penguin is large (has a body mass above average).\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-39.151\n7.149\n-5.477\n0.000\n\n\nspeciesChinstrap\n-1.870\n0.580\n-3.221\n0.001\n\n\nspeciesGentoo\n0.512\n0.843\n0.607\n0.544\n\n\nflipper_length_mm\n0.195\n0.037\n5.295\n0.000\n\n\n\n\n\n\n\nInterpret the coefficient of flipper_length_mm in terms of the odds a penguin is large.\nInterpret the coefficient of speciesChinstrap in terms of the odds a penguin is large.\n\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#topics",
    "href": "slides/18-logistic-prediction.html#topics",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#computational-setup",
    "href": "slides/18-logistic-prediction.html#computational-setup",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#openintroemail",
    "href": "slides/18-logistic-prediction.html#openintroemail",
    "title": "Logistic Regression: Prediction + classification",
    "section": "openintro::email",
    "text": "openintro::email\nThese data represent incoming emails for the first three months of 2012 for an email account.\n\n\nOutcome: spam - Indicator for whether the email was spam.\nPredictors: spam, to_multiple, from, cc, sent_email, time, image, attach, dollar, winner, inherit, viagra, password, num_char, line_breaks, format, re_subj, exclaim_subj, urgent_subj, exclaim_mess, number.\n\n\nClick here for more detailed information on the variables."
  },
  {
    "objectID": "slides/18-logistic-prediction.html#training-and-testing-split",
    "href": "slides/18-logistic-prediction.html#training-and-testing-split",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Training and testing split",
    "text": "Training and testing split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1109)\n\n# Put 75% of the data into the training set \nemail_split &lt;- initial_split(email)\n\n# Create data frames for the two sets\nemail_train &lt;- training(email_split)\nemail_test  &lt;- testing(email_split)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#exploratory-data-analysis",
    "href": "slides/18-logistic-prediction.html#exploratory-data-analysis",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThe sample is unbalanced with respect to spam."
  },
  {
    "objectID": "slides/18-logistic-prediction.html#reminder-modeling-workflow",
    "href": "slides/18-logistic-prediction.html#reminder-modeling-workflow",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Reminder: Modeling workflow",
    "text": "Reminder: Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\n\nUse cross-validation if deciding between multiple models\n\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#initiate-a-recipe",
    "href": "slides/18-logistic-prediction.html#initiate-a-recipe",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec &lt;- recipe(\n  spam ~ .,          # formula\n  data = email_train  # data to use for cataloging names and types of variables\n  )\nsummary(email_rec)\n\n\n\n# A tibble: 21 √ó 4\n   variable     type      role      source  \n   &lt;chr&gt;        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 to_multiple  &lt;chr [3]&gt; predictor original\n 2 from         &lt;chr [3]&gt; predictor original\n 3 cc           &lt;chr [2]&gt; predictor original\n 4 sent_email   &lt;chr [3]&gt; predictor original\n 5 time         &lt;chr [1]&gt; predictor original\n 6 image        &lt;chr [2]&gt; predictor original\n 7 attach       &lt;chr [2]&gt; predictor original\n 8 dollar       &lt;chr [2]&gt; predictor original\n 9 winner       &lt;chr [3]&gt; predictor original\n10 inherit      &lt;chr [2]&gt; predictor original\n11 viagra       &lt;chr [2]&gt; predictor original\n12 password     &lt;chr [2]&gt; predictor original\n13 num_char     &lt;chr [2]&gt; predictor original\n14 line_breaks  &lt;chr [2]&gt; predictor original\n15 format       &lt;chr [3]&gt; predictor original\n16 re_subj      &lt;chr [3]&gt; predictor original\n17 exclaim_subj &lt;chr [2]&gt; predictor original\n18 urgent_subj  &lt;chr [3]&gt; predictor original\n19 exclaim_mess &lt;chr [2]&gt; predictor original\n20 number       &lt;chr [3]&gt; predictor original\n21 spam         &lt;chr [3]&gt; outcome   original"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#remove-certain-variables",
    "href": "slides/18-logistic-prediction.html#remove-certain-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_rm(from, sent_email)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#feature-engineer-date",
    "href": "slides/18-logistic-prediction.html#feature-engineer-date",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nemail_rec &lt;- email_rec |&gt;\n  step_date(time, features = c(\"dow\", \"month\")) |&gt;\n  step_rm(time)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#discretize-numeric-variables",
    "href": "slides/18-logistic-prediction.html#discretize-numeric-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Discretize numeric variables",
    "text": "Discretize numeric variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_cut(cc, attach, dollar, breaks = c(0, 1))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#create-dummy-variables",
    "href": "slides/18-logistic-prediction.html#create-dummy-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_dummy(all_nominal(), -all_outcomes())"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#remove-zero-variance-variables",
    "href": "slides/18-logistic-prediction.html#remove-zero-variance-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value\n\nemail_rec &lt;- email_rec |&gt;\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#recipe-all-in-one-place",
    "href": "slides/18-logistic-prediction.html#recipe-all-in-one-place",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Recipe: All in one place",
    "text": "Recipe: All in one place\n\nemail_rec &lt;- recipe(spam ~ ., data = email_train) |&gt;\n  step_rm(from, sent_email) |&gt;\n  step_date(time, features = c(\"dow\", \"month\")) |&gt;               \n  step_rm(time) |&gt;\n  step_cut(cc, attach, dollar, breaks = c(0, 1)) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#define-model",
    "href": "slides/18-logistic-prediction.html#define-model",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Define model",
    "text": "Define model\n\nemail_spec &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\nemail_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#define-workflow",
    "href": "slides/18-logistic-prediction.html#define-workflow",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Define workflow",
    "text": "Define workflow\nRemember: Workflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow &lt;- workflow() |&gt; \n  add_model(email_spec) |&gt; \n  add_recipe(email_rec)\n\n\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_date()\n‚Ä¢ step_rm()\n‚Ä¢ step_cut()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#fit-model-to-training-data",
    "href": "slides/18-logistic-prediction.html#fit-model-to-training-data",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit &lt;- email_wflow |&gt; \n  fit(data = email_train)\n\ntidy(email_fit) |&gt; print(n = 31)\n\n# A tibble: 27 √ó 5\n   term           estimate std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)    -1.24      0.274     -4.51   6.43e- 6\n 2 image          -1.36      0.679     -2.00   4.59e- 2\n 3 inherit         0.352     0.185      1.90   5.69e- 2\n 4 viagra          1.96     40.6        0.0482 9.62e- 1\n 5 password       -0.941     0.387     -2.43   1.51e- 2\n 6 num_char        0.0572    0.0257     2.23   2.58e- 2\n 7 line_breaks    -0.00554   0.00147   -3.77   1.66e- 4\n 8 exclaim_subj   -0.245     0.303     -0.807  4.19e- 1\n 9 exclaim_mess    0.00916   0.00195    4.69   2.67e- 6\n10 to_multiple_X1 -2.91      0.388     -7.50   6.37e-14\n11 cc_X.1.68.     -0.105     0.446     -0.236  8.14e- 1\n12 attach_X.1.21.  2.33      0.385      6.06   1.37e- 9\n13 dollar_X.1.64.  0.0136    0.241      0.0565 9.55e- 1\n14 winner_yes      2.46      0.480      5.12   3.02e- 7\n15 format_X1      -1.02      0.173     -5.88   4.07e- 9\n16 re_subj_X1     -2.93      0.436     -6.72   1.81e-11\n17 urgent_subj_X1  4.37      1.25       3.51   4.54e- 4\n18 number_small   -0.728     0.178     -4.08   4.45e- 5\n19 number_big      0.261     0.255      1.03   3.05e- 1\n20 time_dow_Mon    0.123     0.320      0.386  7.00e- 1\n21 time_dow_Tue    0.309     0.294      1.05   2.94e- 1\n22 time_dow_Wed   -0.133     0.297     -0.447  6.55e- 1\n23 time_dow_Thu    0.104     0.303      0.343  7.32e- 1\n24 time_dow_Fri    0.280     0.292      0.960  3.37e- 1\n25 time_dow_Sat    0.439     0.323      1.36   1.74e- 1\n26 time_month_Feb  1.06      0.192      5.54   3.06e- 8\n27 time_month_Mar  0.575     0.198      2.91   3.60e- 3"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#make-predictions-for-test-data",
    "href": "slides/18-logistic-prediction.html#make-predictions-for-test-data",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred &lt;- predict(email_fit, email_test, type = \"prob\") |&gt; \n  bind_cols(email_test) \nemail_pred\n\n# A tibble: 981 √ó 23\n   .pred_0  .pred_1 spam  to_multiple from     cc sent_email time               \n     &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;             \n 1   0.921 0.0786   0     0           1         0 0          2012-01-01 01:16:41\n 2   0.961 0.0391   0     0           1         0 0          2012-01-01 05:00:01\n 3   0.999 0.000988 0     0           1         1 1          2012-01-01 14:38:32\n 4   0.999 0.000591 0     0           1         1 1          2012-01-01 18:40:14\n 5   0.991 0.00878  0     0           1         0 0          2012-01-02 00:42:16\n 6   0.910 0.0902   0     0           1         0 0          2012-01-01 21:05:45\n 7   1.00  0.000108 0     1           1         3 0          2012-01-02 08:41:11\n 8   0.975 0.0248   0     0           1         0 0          2012-01-02 20:07:17\n 9   0.952 0.0477   0     0           1         0 0          2012-01-02 23:31:03\n10   0.992 0.00819  0     1           1         0 0          2012-01-03 08:36:16\n# ‚Ñπ 971 more rows\n# ‚Ñπ 15 more variables: image &lt;dbl&gt;, attach &lt;dbl&gt;, dollar &lt;dbl&gt;, winner &lt;fct&gt;,\n#   inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;,\n#   line_breaks &lt;int&gt;, format &lt;fct&gt;, re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;,\n#   urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;, number &lt;fct&gt;"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#a-closer-look-at-predictions",
    "href": "slides/18-logistic-prediction.html#a-closer-look-at-predictions",
    "title": "Logistic Regression: Prediction + classification",
    "section": "A closer look at predictions",
    "text": "A closer look at predictions\n\nWhich of the following 10 emails will be misclassified?\n\n\nemail_pred |&gt;\n  arrange(desc(.pred_1)) |&gt;\n  select(contains(\"pred\"), spam) |&gt; slice(1:10)\n\n# A tibble: 10 √ó 3\n   .pred_0 .pred_1 spam \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;\n 1  0.0750   0.925 0    \n 2  0.110    0.890 0    \n 3  0.116    0.884 1    \n 4  0.127    0.873 1    \n 5  0.170    0.830 1    \n 6  0.189    0.811 1    \n 7  0.204    0.796 1    \n 8  0.208    0.792 1    \n 9  0.224    0.776 1    \n10  0.295    0.705 1"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#false-positive-and-negative",
    "href": "slides/18-logistic-prediction.html#false-positive-and-negative",
    "title": "Logistic Regression: Prediction + classification",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\n\nFalse negative rate = P(classified as not spam | Email spam)\n\n= FN / (TP + FN)\n\nFalse positive rate = P(classified as spam | Email not spam)\n\n= FP / (FP + TN)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#sensitivity-and-specificity-1",
    "href": "slides/18-logistic-prediction.html#sensitivity-and-specificity-1",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\nSensitivity = P(classified as spam | Email spam) = TP / (TP + FN)\n\nSensitivity = 1 ‚àí False negative rate\n\nSpecificity = P(classified as not spam | Email not spam) = TN / (FP + TN)\n\nSpecificity = 1 ‚àí False positive rate\n\n\n\n\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#evaluate-the-performance",
    "href": "slides/18-logistic-prediction.html#evaluate-the-performance",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve+ plots the true positive rate (sensitivity) vs.¬†false positive rate (1 - specificity).\n\n\n\n\nemail_pred |&gt;\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+ Originally developed for operators of military radar receivers, hence the name."
  },
  {
    "objectID": "slides/18-logistic-prediction.html#roc-curve-under-the-hood",
    "href": "slides/18-logistic-prediction.html#roc-curve-under-the-hood",
    "title": "Logistic Regression: Prediction + classification",
    "section": "ROC curve, under the hood",
    "text": "ROC curve, under the hood\n\nemail_pred |&gt;\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 978 √ó 3\n    .threshold specificity sensitivity\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf            0                 1\n 2    3.36e-10     0                 1\n 3    2.27e- 9     0.00226           1\n 4    8.69e- 7     0.00339           1\n 5    9.89e- 7     0.00452           1\n 6    1.43e- 6     0.00565           1\n 7    9.16e- 6     0.00678           1\n 8    1.03e- 5     0.00791           1\n 9    2.58e- 5     0.00904           1\n10    3.35e- 5     0.0102            1\n# ‚Ñπ 968 more rows"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#roc-curve",
    "href": "slides/18-logistic-prediction.html#roc-curve",
    "title": "Logistic Regression: Prediction + classification",
    "section": "ROC curve",
    "text": "ROC curve"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#evaluate-the-performance-auc",
    "href": "slides/18-logistic-prediction.html#evaluate-the-performance-auc",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Evaluate the performance: AUC",
    "text": "Evaluate the performance: AUC\n\nemail_pred |&gt;\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.836\n\n\n\nThe area under the curve (AUC) can be used to assess how well the logistic model fits the data\n\nAUC=0.5: model is a very bad fit (no better than a coin flip)\nAUC close to 1: model is a good fit"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#cutoff-probability-0.5",
    "href": "slides/18-logistic-prediction.html#cutoff-probability-0.5",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n877\n82\n\n\nEmail classified as spam\n8\n14\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.5\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#confusion-matrix",
    "href": "slides/18-logistic-prediction.html#confusion-matrix",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nCross-tabulation of observed and predicted classes:\n\ncutoff_prob &lt;- 0.5\nemail_pred |&gt;\n  mutate(spam_predicted = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0))) |&gt;\n  conf_mat(truth = spam, estimate = spam_predicted)\n\n          Truth\nPrediction   0   1\n         0 877  82\n         1   8  14"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#classification",
    "href": "slides/18-logistic-prediction.html#classification",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#cutoff-probability-0.25",
    "href": "slides/18-logistic-prediction.html#cutoff-probability-0.25",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n830\n52\n\n\nEmail classified as spam\n55\n44\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.25\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#classification-1",
    "href": "slides/18-logistic-prediction.html#classification-1",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#cutoff-probability-0.75",
    "href": "slides/18-logistic-prediction.html#cutoff-probability-0.75",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n883\n89\n\n\nEmail classified as spam\n2\n7\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.75\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#classification-2",
    "href": "slides/18-logistic-prediction.html#classification-2",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#use-roc-curve",
    "href": "slides/18-logistic-prediction.html#use-roc-curve",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Use ROC curve",
    "text": "Use ROC curve\nUse the ROC curve to determine the best cutoff probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 √ó 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1     0.0769       0.736       0.792\n 2     0.0770       0.736       0.781\n 3     0.0780       0.737       0.781\n 4     0.0785       0.737       0.771\n 5     0.0786       0.738       0.771\n 6     0.0787       0.739       0.771\n 7     0.0789       0.739       0.760\n 8     0.0802       0.740       0.760\n 9     0.0802       0.741       0.760\n10     0.0805       0.742       0.760"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#recap",
    "href": "slides/18-logistic-prediction.html#recap",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Recap",
    "text": "Recap\n\nBuilt predictive logistic regression models\nDefined and calculated sensitivity and specificity\nMade classification decisions based on sensitivity and specificity\n\n\n\n\nüîó STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nüîó on Duke Container Manager\n\n\nCourse GitHub organization\nüîó on GitHub\n\n\nDiscussion forum\nüîó on Ed Discussion\n\n\nAssignment submission\nüîó on Gradescope\n\n\nZoom links\nüîó on Sakai"
  },
  {
    "objectID": "project-tips.html",
    "href": "project-tips.html",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Data sources\n\nSome resources that may be helpful as you find data:\n\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\nOther data repositories\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\n\n\n\n\nTips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your Qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you‚Äôre welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that‚Äôs fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\n\nFormatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\nAn alternative approach is to add the following code to the YAML:\n\nexecute:\n  echo: false\n  warning: false\n  message: false\n\n\n\n\nHeaders\n\nUse headers to clearly label each section. Make sure there is a space between the last # and the title, so the header renders correctly. For example, ###Section Title will not render as header, but ### Section Title will.\n\n\n\nReferences\n\nInclude all references in a section called ‚ÄúReferences‚Äù at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called ‚ÄúAppendix‚Äù.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\n\nResize plots and figures, so you have more space for the narrative.\n\nResize individual figures: Use the code chunk header {r plot1, fig.height = 3, fig.width = 5}, replacing plot1 with a meaningful label and the height and width with values appropriate for your write up.\nResize all figures: Include the fig_width and fig_height options in your YAML header as shown below:\n\n\n\n---\ntitle: \"Your title\"\nauthor: \"Your names\"\nformat:\n  pdf:\n    fig-width: 7\n    fig-height: 5\n---\nReplace the height and width values with values appropriate for your write up.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\n\nIf you‚Äôre using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\nIf you‚Äôre using base R function, i.e.¬†when using the emplogit functions, put the code par(mfrow = c(rows,columns)) before the code to make the plots. For example, par(mfrow = c(2,3)) will arrange plots in a grid with 2 rows and 3 columns.\n\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\nUse coord_flip() to flip the x and y axes on the plot. This is useful if you a bar plot with an x-axis that is difficult to read due to overlapping text.\n\n‚ùå NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n‚úÖ YES! Names are readable\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  ggplot(aes(x = fct_reorder(manufacturer,n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_bw() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon‚Äôt use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n‚ùå There is a negative linear relationship between mpg and hp.\n‚úÖ There is a negative linear relationship between a car‚Äôs fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don‚Äôt assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the ‚Äúso what‚Äù: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e.¬†what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n‚ùå For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n‚úÖ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it‚Äôs from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\nAdditional resources\n\nExploring RStudio‚Äôs Visual Markdown Editor\nR for Data Science\nQuarto documentation:\n\nQuarto PDF Basics\nPresentations in Quarto\n\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you‚Äôre having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you‚Äôll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots)."
  }
]