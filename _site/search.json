[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here."
  },
  {
    "objectID": "schedule.html#notes",
    "href": "schedule.html#notes",
    "title": "Schedule",
    "section": "Notes",
    "text": "Notes\n\nLab:\n\nLab sections 01 & 02 (Tuesdays): Labs will be due on Fridays at 11:59pm.\nLab sections 03 & 04 (Thursdays): Labs will be due on Sundays at 11:59pm.\n\nProject dates will be added when the project is assigned."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF of the syllabus."
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\nClass meetings\n\n\n\n\n\n\n\n\n\nLecture\nSection 001\nMon & Wed 10:05 - 11:20am\nReuben-Cooke 130\n\n\n\nSection 002\nMon & Wed 1:45 - 3pm\nOld Chem 116\n\n\nLab\nLab 01\nTue 10:05 - 11:20am\nReuben-Cooke 129\n\n\n\nLab 02\nTue 11:45am - 1pm\nLSRC A155\n\n\n\nLab 03\nThu 10:05 - 11:20am\nReuben-Cooke 129\n\n\n\nLab 04\nThu 11:45am - 1pm\nSocial Sciences 124\n\n\n\n\n\nTeaching team\n\n\n\nName\nRole\n\n\n\n\n\nProf.¬†Maria Tackett\nInstructor\nOffice Hours: Fri 1 - 3pm\n\n\nSam Rosen\nHead TA\nLab 01: Tue 10:05 - 11:20am\n\n\nDonald Cayton\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nLinxuan Wang\nTA\nLab 03: Thu 10:05 - 11:20am\n\n\nXiaojun Zheng\nTA\nLab 04: Thu 11:45am - 1pm\n\n\nBethany Astor\nTA\n\n\n\nJon Campbell\nTA\n\n\n\nAllison Li\nTA\n\n\n\nMitchelle Mojekwu\nTA\n\n\n\nBen Thorpe\nTA\n\n\n\n\n\n\n\n\n\n\nClick here for schedule of office hours."
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze real-world data to answer questions about multivariable relationships.\nfit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard(DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can learn more at the Center for Sexual and Gender Diversity's website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, sta210-fa23.netlify.app.\nAnnouncements will be emailed through Sakai Announcements periodically. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts in Ed Discussion before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\n\n\nEmail\nIf there is a question that‚Äôs not appropriate for the public forum, you are welcome to email me directly. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources."
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nWhile there is no official textbook for the course, readings will primarily be assigned from the following texts (all freely available online).\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine √áetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler"
  },
  {
    "objectID": "syllabus.html#lectures-and-labs",
    "href": "syllabus.html#lectures-and-labs",
    "title": "Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you to new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to prepare for lectures by completing assigned readings, attend all lecture and lab sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded for completion.\nYou are expected to bring a laptop, tablet, or Chromebook to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone."
  },
  {
    "objectID": "syllabus.html#teams",
    "href": "syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of the semester. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of the labs and project, and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team‚Äôs overall mark.\nYou are expected to make use of the provided GitHub repository as their central collaborative platform. Commits to this repository will be used as a metric (one of several) of each team member‚Äôs relative contribution for each project."
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on six components: application exercises, homework, labs, exams, project, and teamwork.\n\nApplication Exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice apply the statistical concepts and code introduced in the prepare assignment. These AEs are due within three days of the corresponding lecture period. Specifically, AEs from Monday lectures are due Thursday by 11:59p ET, and AEs from Wednesday lectures are due Saturday by 11:59p ET.\nBecause these AEs are for practice, they will be graded based on making a good-faith effort in attempting all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture. Successful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation and communication. Most lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team‚Äôs Git repository on the course‚Äôs GitHub page as the central platform for collaboration. Commits to this repository will be used as a metric of each team member‚Äôs relative contribution for each lab, and there will be periodic peer evaluation on the team collaboration. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted for grading in Gradescope.\nThe lowest lab grade will be dropped at the end of the semester.\n\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you‚Äôre learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will include a closed-notes in-class component and an open-note take-home component. Through these exams you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The exams will focus on both conceptual understanding of the content and application through analysis and computational tasks. The content of the exam will be related to the content in reading assignments, lectures, application exercises, homework, and lab assignments. More detail about the exams will be given during the semester.\n\n\nProject\nThe purpose of the final project is to apply what you‚Äôve learned throughout the semester to analyze an interesting data-driven research question. The project will be completed with your lab teams, and each team will present their work through a written report and presentation. More information about the project will be provided during the semester."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n35%\n\n\nFinal project\n15%\n\n\nLab\n15%\n\n\nExam 01\n15%\n\n\nExam 02\n15%\n\n\nApplication exercises\n2.5%\n\n\nTeamwork\n2.5%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60"
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TAs and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TAs, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you‚Äôre not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. It‚Äôs not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon‚Äôt procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don‚Äôt let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don‚Äôt cheat!\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\n\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\nFor the projects and team labs, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project or team labs across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g.¬†StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity(e.g., completing one‚Äôs own work, following proper citation of sources,adhering to guidance around group work projects,and more).Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 3 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work is accepted for application exercises, since these are designed as in-class activities to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email Professor Tackett before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let Professor Tackett know if you need help contacting your academic dean.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture and labs. There may be times, however, when you cannot attend class. Lecture recordings are available upon request for students who have an excused absence. See the Lecture recording request policy for more detail. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture. Labs dedicated to completing the lab assignment and collaborating with your lab team. If you miss a lab session, make sure to communicate with your lab TA and teammates about how you can make up your contribution. If you know you're going to miss a lab session and you're feeling well enough to do so, notify your lab TA and teammates ahead of time.\nMore details on Trinity attendance policies are available here.\n\nAttendance Policy Related to COVID Symptoms, Exposure, or Infection\nStudent health, safety, and well-being are the university‚Äôs top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have tested positive for COVID-19 or have possible symptoms and have not yet been tested. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health (dshcheckin@duke.edu, 919-681-9355). Learn more about current university policy related to COVID-19 at coronavirus.duke.edu.\nTo keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\n\nLecture recording request\nLectures will be recorded on Panopto and will be made available to students with an excused absence upon request. Videos shared with such students will be available for a week after the lecture date. To request a particular lecture‚Äôs video, please fill out the form at forms.office.com/r/FbRXJm4Ln1. Please submit the form within 24 hours of missing lecture to ensure you have sufficient time to watch the recording. Please also make sure that any official documentation, such as STINFs, Dean‚Äôs excuses, NOVAPs, and quarantine/removal from class notices from student health are also uploaded to the form.\nAbout one week before each exam, the class recordings will be available to all students. These videos will be available until the exam deadline."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you are a student with a disability and need accommodations for this class, it is your responsibility to register with the Student Disability Access Office (SDAO) and provide them with documentation of your disability. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.\n\n\nReligious accommodations\nStudents are permitted by university policy to be absent from class to observe a religious holiday. Accordingly, Trinity College of Arts & Sciences and the Pratt School of Engineering have established procedures to be followed by students for notifying their instructors of an absence necessitated by the observance of a religious holiday. Please submit requests for religious accommodations at the beginning of the semester so that we can work to make suitable arrangements well ahead of time. You can find the policy and relevant notification form here: trinity.duke.edu/undergraduate/academic-policies/religious-holidays"
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\nAcademic Resource Center\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917.\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000."
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nAug 28: Classes begin\nSep 8: Drop/add ends\nOct 10 - 11: Fall break\nNov 11 Last day to withdraw with W\nNov 23 - 25: Thanksgiving recess\nDec 9: Classes end\nDec 10 - 13 Reading period\nDec 14 - 19: Final exams\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.‚Ü©Ô∏é"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nüîó on Duke Container Manager\n\n\nCourse GitHub organization\nüîó on GitHub\n\n\nDiscussion forum\nüîó on Ed Discussion\n\n\nGradescope (Section 001)\nüîó on Sakai\n\n\nGradescope (Section 002)\nüîó on Sakai\n\n\nZoom links\nüîó on Sakai"
  },
  {
    "objectID": "slides/02-slr-intro.html#announcements",
    "href": "slides/02-slr-intro.html#announcements",
    "title": "Simple Linear Regression",
    "section": "Announcements",
    "text": "Announcements\n\nR Resources page updated on the course website\nUpcoming R workshops by Duke Center for Data and Visualization Sciences:\n\n‚ÄãR for data science: getting started, EDA, data wrangling - Thu, Sep 15 at 1pm\nR for data science: visualization, pivot, join, regression - Thu, Sep 22 at 1pm\n\nPolicyon requesting class recordings\nSee Week 01 for lecture notes, readings, AEs, and assignments"
  },
  {
    "objectID": "slides/02-slr-intro.html#meet-your-neighbor",
    "href": "slides/02-slr-intro.html#meet-your-neighbor",
    "title": "Simple Linear Regression",
    "section": "Meet your neighbor",
    "text": "Meet your neighbor\nTake a few moments to meet or (reconnect with) your neighbor!\n\nName\nYear\nMajor\nA highlight or something that stood out in the first two days of class\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/02-slr-intro.html#topics",
    "href": "slides/02-slr-intro.html#topics",
    "title": "Simple Linear Regression",
    "section": "Topics",
    "text": "Topics\n\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nEstimate the slope and intercept of the regression line using the least squares method.\nInterpret the slope and intercept of the regression line.\nPredict the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/02-slr-intro.html#computation-set-up",
    "href": "slides/02-slr-intro.html#computation-set-up",
    "title": "Simple Linear Regression",
    "section": "Computation set up",
    "text": "Computation set up\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/02-slr-intro.html#movie-ratings",
    "href": "slides/02-slr-intro.html#movie-ratings",
    "title": "Simple Linear Regression",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango‚Äôs\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/02-slr-intro.html#data-prep",
    "href": "slides/02-slr-intro.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/02-slr-intro.html#data-overview",
    "href": "slides/02-slr-intro.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A‚Ä¶\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2‚Ä¶\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,‚Ä¶\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,‚Ä¶\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,‚Ä¶\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8‚Ä¶\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4‚Ä¶\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5‚Ä¶\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0‚Ä¶\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4‚Ä¶\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3‚Ä¶\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4‚Ä¶\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3‚Ä¶\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3‚Ä¶\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0‚Ä¶\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0‚Ä¶\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0‚Ä¶\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5‚Ä¶\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5‚Ä¶\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54‚Ä¶\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, ‚Ä¶\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, ‚Ä¶\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5‚Ä¶"
  },
  {
    "objectID": "slides/02-slr-intro.html#movie-ratings-data",
    "href": "slides/02-slr-intro.html#movie-ratings-data",
    "title": "Simple Linear Regression",
    "section": "Movie ratings data",
    "text": "Movie ratings data\nThe data set contains the ‚ÄúTomatometer‚Äù score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com."
  },
  {
    "objectID": "slides/02-slr-intro.html#movie-ratings-data-1",
    "href": "slides/02-slr-intro.html#movie-ratings-data-1",
    "title": "Simple Linear Regression",
    "section": "Movie ratings data",
    "text": "Movie ratings data\nGoal: Fit a line to describe the relationship between the critics score and audience score."
  },
  {
    "objectID": "slides/02-slr-intro.html#why-fit-a-line",
    "href": "slides/02-slr-intro.html#why-fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n\nPrediction\n\nWhat is the audience score expected to be for an upcoming movie that received 35% from the critics?\n\n\n\n\nInference\n\nIs the critics score a useful predictor of the audience score? By how much is the audience score expected to change for each additional point in the critics score?"
  },
  {
    "objectID": "slides/02-slr-intro.html#terminology",
    "href": "slides/02-slr-intro.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, Y: variable describing the outcome of interest\nPredictor, X: variable we use to help understand the variability in the response"
  },
  {
    "objectID": "slides/02-slr-intro.html#regression-model",
    "href": "slides/02-slr-intro.html#regression-model",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the response, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#regression-model-1",
    "href": "slides/02-slr-intro.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu_{Y|X}\\) is the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/02-slr-intro.html#regression-model-2",
    "href": "slides/02-slr-intro.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[5pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#simple-linear-regression-1",
    "href": "slides/02-slr-intro.html#simple-linear-regression-1",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\Large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/02-slr-intro.html#simple-linear-regression-2",
    "href": "slides/02-slr-intro.html#simple-linear-regression-2",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/02-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/02-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-slr-intro.html#residuals",
    "href": "slides/02-slr-intro.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#least-squares-line",
    "href": "slides/02-slr-intro.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/02-slr-intro.html#properties-of-least-squares-regression",
    "href": "slides/02-slr-intro.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/02-slr-intro.html#estimating-the-slope",
    "href": "slides/02-slr-intro.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\\]\n\n\n\n\nClick here for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/02-slr-intro.html#estimating-the-intercept",
    "href": "slides/02-slr-intro.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n\\[\\begin{aligned}\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\\]\n\n\n\n\nClick here for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/02-slr-intro.html#interpretation",
    "href": "slides/02-slr-intro.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nPost your answers to the following questions on Ed Discussion:\n\nThe slope of the model for predicting audience score from critics score is 0.5187 . Which of the following is the best interpretation of this value?\n32.3142 is the predicted mean audience score for what type of movies?\n\n\n\n\nLink for Section 001 (10:15am lecture)\nLink for Section 002 (3:30pm lecture)\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/02-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/02-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n\n‚úÖ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n\n\nüõë Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/02-slr-intro.html#making-a-prediction",
    "href": "slides/02-slr-intro.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 70. According to this model, what is the movie‚Äôs predicted audience score?\n\\[\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 70 \\\\\n&= 68.6232\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#extrapolation",
    "href": "slides/02-slr-intro.html#extrapolation",
    "title": "Simple Linear Regression",
    "section": "‚ö†Ô∏è Extrapolation",
    "text": "‚ö†Ô∏è Extrapolation\n\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nSuppose that a movie has a critics score of 0. According to this model, what is the movie‚Äôs predicted audience score?"
  },
  {
    "objectID": "slides/02-slr-intro.html#recap",
    "href": "slides/02-slr-intro.html#recap",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to change by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/02-slr-intro.html#next-class",
    "href": "slides/02-slr-intro.html#next-class",
    "title": "Simple Linear Regression",
    "section": "Next class",
    "text": "Next class\nWe will talk about fitting linear models in R with tidymodels.\n\nReserve STA 210 Docker Container before Monday‚Äôs lecture\nComplete the STA 210 Student Survey (will ask for a GitHub username) by Friday at 11:59pm\n\n\n\n\nüîó Week 01"
  },
  {
    "objectID": "slides/13-variable-transformations.html#announcements",
    "href": "slides/13-variable-transformations.html#announcements",
    "title": "Variable transformations",
    "section": "Announcements",
    "text": "Announcements\n\nClick here to fill out mid-semester survey by Friday.\nLab 04 due:\n\nThu, Oct 13, 11:59pm (Thu labs)\nFri, Oct 14, 11:59pm (Fri labs)\n\nHW 02 due Wed, Oct 19, 11:59pm (released later today)\nOffice hours resume tomorrow (Thursday)\nClick here for Week 07 activities."
  },
  {
    "objectID": "slides/13-variable-transformations.html#topics",
    "href": "slides/13-variable-transformations.html#topics",
    "title": "Variable transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/13-variable-transformations.html#computational-set-up",
    "href": "slides/13-variable-transformations.html#computational-set-up",
    "title": "Variable transformations",
    "section": "Computational set up",
    "text": "Computational set up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Sleuth3) \nlibrary(patchwork)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/13-variable-transformations.html#respiratory-rate-vs.-age",
    "href": "slides/13-variable-transformations.html#respiratory-rate-vs.-age",
    "title": "Variable transformations",
    "section": "Respiratory Rate vs.¬†Age",
    "text": "Respiratory Rate vs.¬†Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a ‚Äúhigh‚Äù rate, we first want to understand the relationship between a child‚Äôs age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication ‚ÄúReference Values for Respiratory Rate in the First 3 Years of Life‚Äù.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/13-variable-transformations.html#rate-vs.-age",
    "href": "slides/13-variable-transformations.html#rate-vs.-age",
    "title": "Variable transformations",
    "section": "Rate vs.¬†Age",
    "text": "Rate vs.¬†Age\n\n\nWhat do you notice in this plot?"
  },
  {
    "objectID": "slides/13-variable-transformations.html#training-test-sets",
    "href": "slides/13-variable-transformations.html#training-test-sets",
    "title": "Variable transformations",
    "section": "Training + test sets",
    "text": "Training + test sets\n\nset.seed(101222)\n# iniital split \nresp_split &lt;- initial_split(respiratory)\n\n# training set\nresp_train &lt;- training(resp_split)\n\n# test set\nresp_test &lt;- testing(resp_split)"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-1-rate-vs.-age",
    "href": "slides/13-variable-transformations.html#model-1-rate-vs.-age",
    "title": "Variable transformations",
    "section": "Model 1: Rate vs.¬†Age",
    "text": "Model 1: Rate vs.¬†Age\n\nresp_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Rate ~ Age, data = resp_train)\n\ntidy(resp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n46.458\n0.589\n78.924\n0\n\n\nAge\n-0.659\n0.034\n-19.498\n0"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-1-residuals",
    "href": "slides/13-variable-transformations.html#model-1-residuals",
    "title": "Variable transformations",
    "section": "Model 1: Residuals",
    "text": "Model 1: Residuals\n\n\nWhat do you notice in this plot?"
  },
  {
    "objectID": "slides/13-variable-transformations.html#consider-different-transformations",
    "href": "slides/13-variable-transformations.html#consider-different-transformations",
    "title": "Variable transformations",
    "section": "Consider different transformations‚Ä¶",
    "text": "Consider different transformations‚Ä¶"
  },
  {
    "objectID": "slides/13-variable-transformations.html#identifying-a-need-to-transform-y",
    "href": "slides/13-variable-transformations.html#identifying-a-need-to-transform-y",
    "title": "Variable transformations",
    "section": "Identifying a need to transform \\(Y\\)",
    "text": "Identifying a need to transform \\(Y\\)\n\nTypically, a ‚Äúfan-shaped‚Äù residual plot indicates the need for a transformation of the response variable \\(Y\\)\n\nThere are multiple ways to transform a variable, e.g., \\(\\sqrt{Y}\\), \\(1/Y\\), \\(\\log(Y)\\).\n\\(\\log(Y)\\) the most straightforward to interpret, so we use that transformation when possible\n\n\n\n\nWhen building a model:\n\nChoose a transformation and build the model on the transformed data\nReassess the residual plots\nIf the residuals plots did not sufficiently improve, try a new transformation!"
  },
  {
    "objectID": "slides/13-variable-transformations.html#log-transformation-on-y",
    "href": "slides/13-variable-transformations.html#log-transformation-on-y",
    "title": "Variable transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\n\nIf we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n\\[\n\\log(Y) = \\beta_0+ \\beta_1 X + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n\\]\n\nThe regression equation is\n\n\\[\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X\\]"
  },
  {
    "objectID": "slides/13-variable-transformations.html#log-transformation-on-y-1",
    "href": "slides/13-variable-transformations.html#log-transformation-on-y-1",
    "title": "Variable transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\nWe want to interpret the model in terms of the original variable \\(Y\\), not \\(\\log(Y)\\), so we need to write the model in terms of \\(Y\\)\n\\[\\hat{Y} = \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X\\} = \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\]\n\n\\[\\widehat{\\text{Median}({Y|X})} = \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1 X\\}\\]"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-interpretation",
    "href": "slides/13-variable-transformations.html#model-interpretation",
    "title": "Variable transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\\[\\hat{Y} = \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X\\} = \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\]\n\n\nIntercept: When \\(X=0\\), the median of \\(Y\\) is expected to be \\(\\exp\\{\\hat{\\beta}_0\\}\\)\nSlope: For every one unit increase in \\(X\\), the median of \\(Y\\) is expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_1\\}\\)\n\n\nWhy is the interpretation in terms of a multiplicative change?"
  },
  {
    "objectID": "slides/13-variable-transformations.html#why-medianyx-instead-of-mu_yx",
    "href": "slides/13-variable-transformations.html#why-medianyx-instead-of-mu_yx",
    "title": "Variable transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\nSuppose we have a set of values\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet‚Äôs calculate \\(\\overline{\\log(x)}\\)\n\nlog_x &lt;- log(x)\nmean(log_x)\n\n[1] 2.066476\n\n\n\nLet‚Äôs calculate \\(\\log(\\bar{x})\\)\n\nxbar &lt;- mean(x)\nlog(xbar)\n\n[1] 2.228477\n\n\n\n\n\n\n\nNote: \\(\\overline{\\log(x)} \\neq \\log(\\bar{x})\\)"
  },
  {
    "objectID": "slides/13-variable-transformations.html#why-medianyx-instead-of-mu_yx-1",
    "href": "slides/13-variable-transformations.html#why-medianyx-instead-of-mu_yx-1",
    "title": "Variable transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet‚Äôs calculate \\(\\text{Median}(\\log(x))\\)\n\nlog_x &lt;- log(x)\nmedian(log_x)\n\n[1] 2.079442\n\n\n\nLet‚Äôs calculate \\(\\log(\\text{Median}(x))\\)\n\nmedian_x &lt;- median(x)\nlog(median_x)\n\n[1] 2.079442\n\n\n\n\n\n\n\nNote: \\(\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\)"
  },
  {
    "objectID": "slides/13-variable-transformations.html#mean-median-and-log",
    "href": "slides/13-variable-transformations.html#mean-median-and-log",
    "title": "Variable transformations",
    "section": "Mean, Median, and log",
    "text": "Mean, Median, and log\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\\[\\overline{\\log(x)} \\neq \\log(\\bar{x})\\]\n\nmean(log_x) == log(xbar)\n\n[1] FALSE\n\n\n\n\\[\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\]\n\nmedian(log_x) == log(median_x)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/13-variable-transformations.html#mean-and-median-of-logy",
    "href": "slides/13-variable-transformations.html#mean-and-median-of-logy",
    "title": "Variable transformations",
    "section": "Mean and median of \\(\\log(Y)\\)",
    "text": "Mean and median of \\(\\log(Y)\\)\n\nRecall that \\(Y = \\beta_0 + \\beta_1 X\\) is the mean value of the response at the given value of the predictor \\(X\\). This doesn‚Äôt hold when we log-transform the response variable.\nMathematically, the mean of the logged values is not necessarily equal to the log of the mean value. Therefore at a given value of \\(X\\)\n\n\n$$\n\\[\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/13-variable-transformations.html#mean-and-median-of-logy-1",
    "href": "slides/13-variable-transformations.html#mean-and-median-of-logy-1",
    "title": "Variable transformations",
    "section": "Mean and median of \\(\\log(y)\\)",
    "text": "Mean and median of \\(\\log(y)\\)\n\nHowever, the median of the logged values is equal to the log of the median value. Therefore,\n\n\\[\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)\\]\n\n\nIf the distribution of \\(\\log(Y)\\) is symmetric about the regression line, for a given value \\(X\\), we can expect \\(Mean(Y)\\) and \\(Median(Y)\\) to be approximately equal."
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-2-lograte-vs.-age",
    "href": "slides/13-variable-transformations.html#model-2-lograte-vs.-age",
    "title": "Variable transformations",
    "section": "Model 2: log(Rate) vs.¬†Age",
    "text": "Model 2: log(Rate) vs.¬†Age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.831\n0.015\n259.086\n0\n\n\nAge\n-0.018\n0.001\n-21.243\n0\n\n\n\n\n\n\n\nInterpret the slope and intercept in the context of the data.\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-2-residuals",
    "href": "slides/13-variable-transformations.html#model-2-residuals",
    "title": "Variable transformations",
    "section": "Model 2: Residuals",
    "text": "Model 2: Residuals"
  },
  {
    "objectID": "slides/13-variable-transformations.html#compare-residual-plots",
    "href": "slides/13-variable-transformations.html#compare-residual-plots",
    "title": "Variable transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/13-variable-transformations.html#log-transformation-on-x",
    "href": "slides/13-variable-transformations.html#log-transformation-on-x",
    "title": "Variable transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/13-variable-transformations.html#rate-vs.-logage",
    "href": "slides/13-variable-transformations.html#rate-vs.-logage",
    "title": "Variable transformations",
    "section": "Rate vs.¬†log(Age)",
    "text": "Rate vs.¬†log(Age)"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-with-transformation-on-x",
    "href": "slides/13-variable-transformations.html#model-with-transformation-on-x",
    "title": "Variable transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e.¬†the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-3-rate-vs.-logage",
    "href": "slides/13-variable-transformations.html#model-3-rate-vs.-logage",
    "title": "Variable transformations",
    "section": "Model 3: Rate vs.¬†log(Age)",
    "text": "Model 3: Rate vs.¬†log(Age)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n49.397\n0.755\n65.436\n0\n\n\nlog_age\n-5.668\n0.311\n-18.248\n0\n\n\n\n\n\n\n\nInterpret the slope and intercept in the context of the data.\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/13-variable-transformations.html#model-3-residuals",
    "href": "slides/13-variable-transformations.html#model-3-residuals",
    "title": "Variable transformations",
    "section": "Model 3: Residuals",
    "text": "Model 3: Residuals"
  },
  {
    "objectID": "slides/13-variable-transformations.html#choose-a-model",
    "href": "slides/13-variable-transformations.html#choose-a-model",
    "title": "Variable transformations",
    "section": "Choose a model",
    "text": "Choose a model\nRecall the goal of the analysis:\nIn order to determine what indicates a ‚Äúhigh‚Äù rate, we first want to understand the relationship between a child‚Äôs age and their respiratory rate.\n\n\nWhich is the preferred metric to compare the models - \\(R^2\\) or RMSE?"
  },
  {
    "objectID": "slides/13-variable-transformations.html#compare-models-on-testing-data",
    "href": "slides/13-variable-transformations.html#compare-models-on-testing-data",
    "title": "Variable transformations",
    "section": "Compare models on testing data",
    "text": "Compare models on testing data\n\n\n\n\n\n\n\n\nRate vs.¬†Age\nlog(Rate) vs.¬†Age\nRate vs.¬†log(Age)\n\n\n\n\n0.549\n0.596\n0.559\n\n\n\n\n\nWhich model would you choose?"
  },
  {
    "objectID": "slides/13-variable-transformations.html#learn-more",
    "href": "slides/13-variable-transformations.html#learn-more",
    "title": "Variable transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables.\n\n\n\nüîó Week 07"
  },
  {
    "objectID": "slides/11-feature-engineering.html#announcements",
    "href": "slides/11-feature-engineering.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\nClick here for slides from presentation about the Academic Resource Center."
  },
  {
    "objectID": "slides/11-feature-engineering.html#topics",
    "href": "slides/11-feature-engineering.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nUnderstanding categorical predictors and interaction terms\nFeature engineering"
  },
  {
    "objectID": "slides/11-feature-engineering.html#computational-setup",
    "href": "slides/11-feature-engineering.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-peer-to-peer-lender",
    "href": "slides/11-feature-engineering.html#data-peer-to-peer-lender",
    "title": "Feature engineering",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday‚Äôs data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 √ó 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ‚Ä¶ with 40 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#variables",
    "href": "slides/11-feature-engineering.html#variables",
    "title": "Feature engineering",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/11-feature-engineering.html#outcome-interest_rate",
    "href": "slides/11-feature-engineering.html#outcome-interest_rate",
    "title": "Feature engineering",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/11-feature-engineering.html#predictors",
    "href": "slides/11-feature-engineering.html#predictors",
    "title": "Feature engineering",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-1-rescale-income",
    "href": "slides/11-feature-engineering.html#data-manipulation-1-rescale-income",
    "title": "Feature engineering",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/11-feature-engineering.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Feature engineering",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/11-feature-engineering.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Feature engineering",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interest-rate-vs.-annual-income",
    "href": "slides/11-feature-engineering.html#interest-rate-vs.-annual-income",
    "title": "Feature engineering",
    "section": "Interest rate vs.¬†annual income",
    "text": "Interest rate vs.¬†annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/11-feature-engineering.html#data-manipulation-4-create-interaction-variables",
    "title": "Feature engineering",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, ‚Ä¶\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.‚Ä¶\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, ‚Ä¶\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, ‚Ä¶\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, ‚Ä¶\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0‚Ä¶\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11‚Ä¶"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interaction-term-in-the-model",
    "href": "slides/11-feature-engineering.html#interaction-term-in-the-model",
    "title": "Feature engineering",
    "section": "Interaction term in the model",
    "text": "Interaction term in the model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent  +  verified_income + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interpreting-interaction-terms",
    "href": "slides/11-feature-engineering.html#interpreting-interaction-terms",
    "title": "Feature engineering",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/11-feature-engineering.html#understanding-the-model",
    "href": "slides/11-feature-engineering.html#understanding-the-model",
    "title": "Feature engineering",
    "section": "Understanding the model",
    "text": "Understanding the model\n\\[\n\\begin{aligned}\n\\hat{interest\\_rate} &= 9.484 + 0.691 \\times debt\\_inc\\_cent\\\\ &- 0.007 \\times annual\\_income\\_th\\_cent \\\\ &+ 2.157 \\times SourceVerified + 7.181 \\times Verified \\\\ &- 0.016 \\times annual\\_inc\\_th\\_cent \\times SourceVerified\\\\ &- 0.032 \\times annual\\_inc\\_th\\_cent \\times Verified\n\\end{aligned}\n\\]\n\n\nWhat is \\(p\\), the number of predictor terms in the model?\nWrite the equation of the model to predict interest rate for applicants with Not Verified income.\nWrite the equation of the model to predict interest rate for applicants with Verified income."
  },
  {
    "objectID": "slides/11-feature-engineering.html#topics-1",
    "href": "slides/11-feature-engineering.html#topics-1",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\n\nReview: Training and testing splits\nFeature engineering with recipes"
  },
  {
    "objectID": "slides/11-feature-engineering.html#computational-setup-1",
    "href": "slides/11-feature-engineering.html#computational-setup-1",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/11-feature-engineering.html#the-office",
    "href": "slides/11-feature-engineering.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data",
    "href": "slides/11-feature-engineering.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\noffice_ratings\n\n# A tibble: 188 √ó 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ‚Ä¶ with 178 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings",
    "href": "slides/11-feature-engineering.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†number of votes",
    "text": "IMDB ratings vs.¬†number of votes"
  },
  {
    "objectID": "slides/11-feature-engineering.html#outliers",
    "href": "slides/11-feature-engineering.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-air-date",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-air-date",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†air date",
    "text": "IMDB ratings vs.¬†air date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-seasons",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†seasons",
    "text": "IMDB ratings vs.¬†seasons"
  },
  {
    "objectID": "slides/11-feature-engineering.html#spending-our-data",
    "href": "slides/11-feature-engineering.html#spending-our-data",
    "title": "Feature engineering",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we‚Äôve done so far)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#train-test",
    "href": "slides/11-feature-engineering.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\n\n\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/11-feature-engineering.html#training-data",
    "href": "slides/11-feature-engineering.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 √ó 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ä¶ with 131 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#feature-engineering-1",
    "href": "slides/11-feature-engineering.html#feature-engineering-1",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#feature-engineering-with-dplyr",
    "href": "slides/11-feature-engineering.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\noffice_train |&gt;\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 √ó 8\n  season episode title               imdb_rating total_‚Ä¶¬π air_date   month  wday\n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 8           18 Last Day in Florida         7.8     1429 2012-03-08     3     5\n2 9           14 Vandalism                   7.6     1402 2013-01-31     1     5\n3 2            8 Performance Review          8.2     2416 2005-11-15    11     3\n4 9            5 Here Comes Treble           7.1     1515 2012-10-25    10     5\n5 3           22 Beach Games                 9.1     2783 2007-05-10     5     5\n6 7            1 Nepotism                    8.4     1897 2010-09-23     9     5\n# ‚Ä¶ with 135 more rows, and abbreviated variable name ¬π‚Äãtotal_votes\n\n\n\n\nCan you identify any potential problems with this approach?"
  },
  {
    "objectID": "slides/11-feature-engineering.html#modeling-workflow",
    "href": "slides/11-feature-engineering.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\n\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/11-feature-engineering.html#initiate-a-recipe",
    "href": "slides/11-feature-engineering.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-1-alter-roles",
    "href": "slides/11-feature-engineering.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn‚Äôt a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-2-add-features",
    "href": "slides/11-feature-engineering.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-3-add-more-features",
    "href": "slides/11-feature-engineering.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-4-convert-numbers-to-factors",
    "href": "slides/11-feature-engineering.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-5-make-dummy-variables",
    "href": "slides/11-feature-engineering.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-6-remove-zero-variance-predictors",
    "href": "slides/11-feature-engineering.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#putting-it-all-together",
    "href": "slides/11-feature-engineering.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/11-feature-engineering.html#putting-it-all-together-1",
    "href": "slides/11-feature-engineering.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#next-step",
    "href": "slides/11-feature-engineering.html#next-step",
    "title": "Feature engineering",
    "section": "Next step‚Ä¶",
    "text": "Next step‚Ä¶\nWe will complete the workflow to fit a model predicting IMDB ratings that includes the following predictors:\n\nepisode\ntotal_votes\nindicator variables for season\nindicator variables for day of week aired (created using air_date)\nindicator variables for month aired (created using air_date)\n\n\n\nWhat feature will not end up in the final model? Why is it not included?"
  },
  {
    "objectID": "slides/11-feature-engineering.html#recap",
    "href": "slides/11-feature-engineering.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\n\nReview: Training and testing splits\nFeature engineering with recipes\n\n\n\n\n\nüîó Week 06"
  },
  {
    "objectID": "slides/08-slr-conditions.html#announcements",
    "href": "slides/08-slr-conditions.html#announcements",
    "title": "SLR: Conditions",
    "section": "Announcements",
    "text": "Announcements\n\nHW 01: due TODAY at 11:59pm\nStatistics experience - due Fri, Dec 09 at 11:59pm\nAaditya‚Äôs office hours today: 1 - 2pm and 7 -8pm on Zoom (link in Sakai)\nSee Week 04 for this week‚Äôs activities.\nUpdated masking policy starting Sep 22\nLooking ahead: Exam 01: Sep 28 - 30"
  },
  {
    "objectID": "slides/08-slr-conditions.html#exam-01",
    "href": "slides/08-slr-conditions.html#exam-01",
    "title": "SLR: Conditions",
    "section": "Exam 01",
    "text": "Exam 01\n\nReleased Sep 28 late afternoon, due Sep 30 at 11:59pm.\n\nNo labs or office hours Sep 28 - 30\n\nCovers content Weeks 01 - 05\nConceptual questions + analysis problems\nWill receive exam through GitHub repo, use a reproducible workflow and submit on GitHub and Gradescope (like labs and HW)\nLecture recordings for Weeks 01 -05 available here until September 28 at 11:59pm.\nLab and HW solutions will be posted after the late submission deadlines.\nExam 01 review in class on September 28"
  },
  {
    "objectID": "slides/08-slr-conditions.html#computational-set-up",
    "href": "slides/08-slr-conditions.html#computational-set-up",
    "title": "SLR: Conditions",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/08-slr-conditions.html#regression-model-revisited",
    "href": "slides/08-slr-conditions.html#regression-model-revisited",
    "title": "SLR: Conditions",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/08-slr-conditions.html#mathematical-representation-visualized",
    "href": "slides/08-slr-conditions.html#mathematical-representation-visualized",
    "title": "SLR: Conditions",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]"
  },
  {
    "objectID": "slides/08-slr-conditions.html#model-conditions",
    "href": "slides/08-slr-conditions.html#model-conditions",
    "title": "SLR: Conditions",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable, i.e.¬†the errors are homeoscedastic\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/08-slr-conditions.html#linearity",
    "href": "slides/08-slr-conditions.html#linearity",
    "title": "SLR: Conditions",
    "section": "Linearity",
    "text": "Linearity\n‚úÖ The residuals vs.¬†fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/08-slr-conditions.html#residuals-vs.-fitted-values-code",
    "href": "slides/08-slr-conditions.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions",
    "section": "Residuals vs.¬†fitted values (code)",
    "text": "Residuals vs.¬†fitted values (code)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/08-slr-conditions.html#non-linear-relationships",
    "href": "slides/08-slr-conditions.html#non-linear-relationships",
    "title": "SLR: Conditions",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/08-slr-conditions.html#constant-variance",
    "href": "slides/08-slr-conditions.html#constant-variance",
    "title": "SLR: Conditions",
    "section": "Constant variance",
    "text": "Constant variance\n‚úÖ The vertical spread of the residuals should be relatively constant across the plot"
  },
  {
    "objectID": "slides/08-slr-conditions.html#non-constant-variance",
    "href": "slides/08-slr-conditions.html#non-constant-variance",
    "title": "SLR: Conditions",
    "section": "Non-constant variance",
    "text": "Non-constant variance"
  },
  {
    "objectID": "slides/08-slr-conditions.html#normality",
    "href": "slides/08-slr-conditions.html#normality",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/08-slr-conditions.html#independence",
    "href": "slides/08-slr-conditions.html#independence",
    "title": "SLR: Conditions",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected\n\n\n‚úÖ If this is a random sample of Duke Houses, the error for one house does not tell us anything about the error for another use"
  },
  {
    "objectID": "slides/08-slr-conditions.html#recap",
    "href": "slides/08-slr-conditions.html#recap",
    "title": "SLR: Conditions",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\n\n\n\n\n03:00\n\n\n\n\n\n\nüîó Week 04"
  },
  {
    "objectID": "slides/lab-dec01-02.html#announcements",
    "href": "slides/lab-dec01-02.html#announcements",
    "title": "Lab: Written report",
    "section": "Announcements",
    "text": "Announcements\n\nTeammates feedback #2 due Tue, Dec 06\nPlease fill out course evaluations and TA evaluations!\nIf you submitted a Round 1 submission, check GitHub Issues for feedback.\nProject written report due Fri, Dec 09, 11:59pm. Will be accepted (with no late penalty) until Sun, Dec 11, 11:59pm"
  },
  {
    "objectID": "slides/lab-dec01-02.html#written-report-reminders",
    "href": "slides/lab-dec01-02.html#written-report-reminders",
    "title": "Lab: Written report",
    "section": "Written report reminders",
    "text": "Written report reminders\n\nUse a descriptive title for the report.\nSuppress all code, warnings, and messages.\nNeatly format every table / data frame using kable().\nWrite the report in narrative form (avoid bullet point lists).\nUse variable descriptions, not variable names, in narrative.\nUse neatly formatted links and citations; include a reference list at the end of the report.\n\nAt a minimum you should have a citation for the data set.\nClick here for guide on adding citations in Quarto using the visual editor\n\n\n\n\n\nüîó Week 14"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#announcements",
    "href": "slides/21-logistic-prediction.html#announcements",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Announcements",
    "text": "Announcements\n\nAaditya‚Äôs office hours permanently moved to Wed 6 - 8pm\n\nNew time reflected on website and Sakai\n\nSee Week 11 activities"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#odds-ratios-practice",
    "href": "slides/21-logistic-prediction.html#odds-ratios-practice",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Odds ratios practice",
    "text": "Odds ratios practice\nLet‚Äôs take a look at one of the models from Lab 06.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.676\n1.881\n-7.804\n0.000\n\n\nislandDream\n-0.892\n0.359\n-2.481\n0.013\n\n\nislandTorgersen\n18.132\n822.821\n0.022\n0.982\n\n\nbill_depth_mm\n0.836\n0.113\n7.416\n0.000\n\n\n\n\n\n\n\nInterpret the coefficient of bill_depth_mm in terms of the odds a penguin is from Adelie species.\nInterpret the coefficient of islandDream in terms of the odds a penguin is from Adelie species.\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#topics",
    "href": "slides/21-logistic-prediction.html#topics",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#computational-setup",
    "href": "slides/21-logistic-prediction.html#computational-setup",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#openintroemail",
    "href": "slides/21-logistic-prediction.html#openintroemail",
    "title": "Logistic Regression: Prediction + classification",
    "section": "openintro::email",
    "text": "openintro::email\nThese data represent incoming emails for the first three months of 2012 for an email account.\n\n\nOutcome: spam - Indicator for whether the email was spam.\nPredictors: spam, to_multiple, from, cc, sent_email, time, image, attach, dollar, winner, inherit, viagra, password, num_char, line_breaks, format, re_subj, exclaim_subj, urgent_subj, exclaim_mess, number.\n\n\nClick here for more detailed information on the variables."
  },
  {
    "objectID": "slides/21-logistic-prediction.html#training-and-testing-split",
    "href": "slides/21-logistic-prediction.html#training-and-testing-split",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Training and testing split",
    "text": "Training and testing split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1109)\n\n# Put 75% of the data into the training set \nemail_split &lt;- initial_split(email)\n\n# Create data frames for the two sets\nemail_train &lt;- training(email_split)\nemail_test  &lt;- testing(email_split)"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#exploratory-data-analysis",
    "href": "slides/21-logistic-prediction.html#exploratory-data-analysis",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThe sample is unbalanced with respect to spam."
  },
  {
    "objectID": "slides/21-logistic-prediction.html#reminder-modeling-workflow",
    "href": "slides/21-logistic-prediction.html#reminder-modeling-workflow",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Reminder: Modeling workflow",
    "text": "Reminder: Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\n\nUse cross-validation if deciding between multiple models\n\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#initiate-a-recipe",
    "href": "slides/21-logistic-prediction.html#initiate-a-recipe",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec &lt;- recipe(\n  spam ~ .,          # formula\n  data = email_train  # data to use for cataloging names and types of variables\n  )\nsummary(email_rec)\n\n\n\n# A tibble: 21 √ó 4\n   variable     type    role      source  \n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   \n 1 to_multiple  nominal predictor original\n 2 from         nominal predictor original\n 3 cc           numeric predictor original\n 4 sent_email   nominal predictor original\n 5 time         date    predictor original\n 6 image        numeric predictor original\n 7 attach       numeric predictor original\n 8 dollar       numeric predictor original\n 9 winner       nominal predictor original\n10 inherit      numeric predictor original\n11 viagra       numeric predictor original\n12 password     numeric predictor original\n13 num_char     numeric predictor original\n14 line_breaks  numeric predictor original\n15 format       nominal predictor original\n16 re_subj      nominal predictor original\n17 exclaim_subj numeric predictor original\n18 urgent_subj  nominal predictor original\n19 exclaim_mess numeric predictor original\n20 number       nominal predictor original\n21 spam         nominal outcome   original"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#remove-certain-variables",
    "href": "slides/21-logistic-prediction.html#remove-certain-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_rm(from, sent_email)\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#feature-engineer-date",
    "href": "slides/21-logistic-prediction.html#feature-engineer-date",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nemail_rec &lt;- email_rec |&gt;\n  step_date(time, features = c(\"dow\", \"month\")) |&gt;\n  step_rm(time)\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email\nDate features from time\nVariables removed time"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#discretize-numeric-variables",
    "href": "slides/21-logistic-prediction.html#discretize-numeric-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Discretize numeric variables",
    "text": "Discretize numeric variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_cut(cc, attach, dollar, breaks = c(0, 1))\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email\nDate features from time\nVariables removed time\nCut numeric for cc, attach, dollar"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#create-dummy-variables",
    "href": "slides/21-logistic-prediction.html#create-dummy-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_dummy(all_nominal(), -all_outcomes())\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email\nDate features from time\nVariables removed time\nCut numeric for cc, attach, dollar\nDummy variables from all_nominal(), -all_outcomes()"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#remove-zero-variance-variables",
    "href": "slides/21-logistic-prediction.html#remove-zero-variance-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value\n\nemail_rec &lt;- email_rec |&gt;\n  step_zv(all_predictors())\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email\nDate features from time\nVariables removed time\nCut numeric for cc, attach, dollar\nDummy variables from all_nominal(), -all_outcomes()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#recipe-all-in-one-place",
    "href": "slides/21-logistic-prediction.html#recipe-all-in-one-place",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Recipe: All in one place",
    "text": "Recipe: All in one place\n\nemail_rec &lt;- recipe(spam ~ ., data = email_train) |&gt;\n  step_rm(from, sent_email) |&gt;\n  step_date(time, features = c(\"dow\", \"month\")) |&gt;               \n  step_rm(time) |&gt;\n  step_cut(cc, attach, dollar, breaks = c(0, 1)) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#define-model",
    "href": "slides/21-logistic-prediction.html#define-model",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Define model",
    "text": "Define model\n\nemail_spec &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\nemail_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#define-workflow",
    "href": "slides/21-logistic-prediction.html#define-workflow",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Define workflow",
    "text": "Define workflow\nRemember: Workflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow &lt;- workflow() |&gt; \n  add_model(email_spec) |&gt; \n  add_recipe(email_rec)\n\n\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_date()\n‚Ä¢ step_rm()\n‚Ä¢ step_cut()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#fit-model-to-training-data",
    "href": "slides/21-logistic-prediction.html#fit-model-to-training-data",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit &lt;- email_wflow |&gt; \n  fit(data = email_train)\n\ntidy(email_fit) |&gt; print(n = 31)\n\n# A tibble: 27 √ó 5\n   term           estimate std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)    -1.24      0.274     -4.51   6.43e- 6\n 2 image          -1.36      0.679     -2.00   4.59e- 2\n 3 inherit         0.352     0.185      1.90   5.69e- 2\n 4 viagra          1.96     40.6        0.0482 9.62e- 1\n 5 password       -0.941     0.387     -2.43   1.51e- 2\n 6 num_char        0.0572    0.0257     2.23   2.58e- 2\n 7 line_breaks    -0.00554   0.00147   -3.77   1.66e- 4\n 8 exclaim_subj   -0.245     0.303     -0.807  4.19e- 1\n 9 exclaim_mess    0.00916   0.00195    4.69   2.67e- 6\n10 to_multiple_X1 -2.91      0.388     -7.50   6.37e-14\n11 cc_X.1.68.     -0.105     0.446     -0.236  8.14e- 1\n12 attach_X.1.21.  2.33      0.385      6.06   1.37e- 9\n13 dollar_X.1.64.  0.0136    0.241      0.0565 9.55e- 1\n14 winner_yes      2.46      0.480      5.12   3.02e- 7\n15 format_X1      -1.02      0.173     -5.88   4.07e- 9\n16 re_subj_X1     -2.93      0.436     -6.72   1.81e-11\n17 urgent_subj_X1  4.37      1.25       3.51   4.54e- 4\n18 number_small   -0.728     0.178     -4.08   4.45e- 5\n19 number_big      0.261     0.255      1.03   3.05e- 1\n20 time_dow_Mon    0.123     0.320      0.386  7.00e- 1\n21 time_dow_Tue    0.309     0.294      1.05   2.94e- 1\n22 time_dow_Wed   -0.133     0.297     -0.447  6.55e- 1\n23 time_dow_Thu    0.104     0.303      0.343  7.32e- 1\n24 time_dow_Fri    0.280     0.292      0.960  3.37e- 1\n25 time_dow_Sat    0.439     0.323      1.36   1.74e- 1\n26 time_month_Feb  1.06      0.192      5.54   3.06e- 8\n27 time_month_Mar  0.575     0.198      2.91   3.60e- 3"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#make-predictions-for-test-data",
    "href": "slides/21-logistic-prediction.html#make-predictions-for-test-data",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred &lt;- predict(email_fit, email_test, type = \"prob\") |&gt; \n  bind_cols(email_test) \nemail_pred\n\n# A tibble: 981 √ó 23\n   .pred_0  .pred_1 spam  to_mul‚Ä¶¬π from     cc sent_‚Ä¶¬≤ time                image\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   &lt;dttm&gt;              &lt;dbl&gt;\n 1   0.921 0.0786   0     0        1         0 0       2012-01-01 01:16:41     0\n 2   0.961 0.0391   0     0        1         0 0       2012-01-01 05:00:01     0\n 3   0.999 0.000988 0     0        1         1 1       2012-01-01 14:38:32     0\n 4   0.999 0.000591 0     0        1         1 1       2012-01-01 18:40:14     0\n 5   0.991 0.00878  0     0        1         0 0       2012-01-02 00:42:16     0\n 6   0.910 0.0902   0     0        1         0 0       2012-01-01 21:05:45     0\n 7   1.00  0.000108 0     1        1         3 0       2012-01-02 08:41:11     0\n 8   0.975 0.0248   0     0        1         0 0       2012-01-02 20:07:17     0\n 9   0.952 0.0477   0     0        1         0 0       2012-01-02 23:31:03     0\n10   0.992 0.00819  0     1        1         0 0       2012-01-03 08:36:16     0\n# ‚Ä¶ with 971 more rows, 14 more variables: attach &lt;dbl&gt;, dollar &lt;dbl&gt;,\n#   winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;,\n#   line_breaks &lt;int&gt;, format &lt;fct&gt;, re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;,\n#   urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;, number &lt;fct&gt;, and abbreviated\n#   variable names ¬π‚Äãto_multiple, ¬≤‚Äãsent_email"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#a-closer-look-at-predictions",
    "href": "slides/21-logistic-prediction.html#a-closer-look-at-predictions",
    "title": "Logistic Regression: Prediction + classification",
    "section": "A closer look at predictions",
    "text": "A closer look at predictions\n\nWhich of the following 10 emails will be misclassified?\n\n\nemail_pred |&gt;\n  arrange(desc(.pred_1)) |&gt;\n  select(contains(\"pred\"), spam) |&gt; slice(1:10)\n\n# A tibble: 10 √ó 3\n   .pred_0 .pred_1 spam \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;\n 1  0.0750   0.925 0    \n 2  0.110    0.890 0    \n 3  0.116    0.884 1    \n 4  0.127    0.873 1    \n 5  0.170    0.830 1    \n 6  0.189    0.811 1    \n 7  0.204    0.796 1    \n 8  0.208    0.792 1    \n 9  0.224    0.776 1    \n10  0.295    0.705 1"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#false-positive-and-negative",
    "href": "slides/21-logistic-prediction.html#false-positive-and-negative",
    "title": "Logistic Regression: Prediction + classification",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\n\nFalse negative rate = P(classified as not spam | Email spam) = FN / (TP + FN)\nFalse positive rate = P(classified as spam | Email not spam) = FP / (FP + TN)"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#sensitivity-and-specificity-1",
    "href": "slides/21-logistic-prediction.html#sensitivity-and-specificity-1",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\n\nSensitivity = P(classified as spam | Email spam) = TP / (TP + FN)\n\nSensitivity = 1 ‚àí False negative rate\n\nSpecificity = P(classified as not spam | Email not spam) = TN / (FP + TN)\n\nSpecificity = 1 ‚àí False positive rate\n\n\n\n\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#evaluate-the-performance",
    "href": "slides/21-logistic-prediction.html#evaluate-the-performance",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve+ plots the true positive rate (sensitivity) vs.¬†false positive rate (1 - specificity).\n\n\n\n\nemail_pred |&gt;\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+ Originally developed for operators of military radar receivers, hence the name."
  },
  {
    "objectID": "slides/21-logistic-prediction.html#roc-curve-under-the-hood",
    "href": "slides/21-logistic-prediction.html#roc-curve-under-the-hood",
    "title": "Logistic Regression: Prediction + classification",
    "section": "ROC curve, under the hood",
    "text": "ROC curve, under the hood\n\nemail_pred |&gt;\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 978 √ó 3\n    .threshold specificity sensitivity\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf            0                 1\n 2    3.36e-10     0                 1\n 3    2.27e- 9     0.00226           1\n 4    8.69e- 7     0.00339           1\n 5    9.89e- 7     0.00452           1\n 6    1.43e- 6     0.00565           1\n 7    9.16e- 6     0.00678           1\n 8    1.03e- 5     0.00791           1\n 9    2.58e- 5     0.00904           1\n10    3.35e- 5     0.0102            1\n# ‚Ä¶ with 968 more rows"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#roc-curve",
    "href": "slides/21-logistic-prediction.html#roc-curve",
    "title": "Logistic Regression: Prediction + classification",
    "section": "ROC curve",
    "text": "ROC curve"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#evaluate-the-performance-auc",
    "href": "slides/21-logistic-prediction.html#evaluate-the-performance-auc",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Evaluate the performance: AUC",
    "text": "Evaluate the performance: AUC\n\nemail_pred |&gt;\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.836\n\n\n\nThe area under the curve (AUC) can be used to assess how well the logistic model fits the data\n\nAUC=0.5: model is a very bad fit (no better than a coin flip)\nAUC close to 1: model is a good fit"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#cutoff-probability-0.5",
    "href": "slides/21-logistic-prediction.html#cutoff-probability-0.5",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n877\n82\n\n\nEmail classified as spam\n8\n14\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.5\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#confusion-matrix",
    "href": "slides/21-logistic-prediction.html#confusion-matrix",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nCross-tabulation of observed and predicted classes:\n\ncutoff_prob &lt;- 0.5\nemail_pred |&gt;\n  mutate(spam_predicted = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0))) |&gt;\n  conf_mat(truth = spam, estimate = spam_predicted)\n\n          Truth\nPrediction   0   1\n         0 877  82\n         1   8  14"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#classification",
    "href": "slides/21-logistic-prediction.html#classification",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#cutoff-probability-0.25",
    "href": "slides/21-logistic-prediction.html#cutoff-probability-0.25",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n830\n52\n\n\nEmail classified as spam\n55\n44\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.25\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#classification-1",
    "href": "slides/21-logistic-prediction.html#classification-1",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#cutoff-probability-0.75",
    "href": "slides/21-logistic-prediction.html#cutoff-probability-0.75",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n883\n89\n\n\nEmail classified as spam\n2\n7\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.75\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#classification-2",
    "href": "slides/21-logistic-prediction.html#classification-2",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/21-logistic-prediction.html#use-roc-curve",
    "href": "slides/21-logistic-prediction.html#use-roc-curve",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Use ROC curve",
    "text": "Use ROC curve\nUse the ROC curve to determine the best cutoff probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 √ó 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1     0.0769       0.736       0.792\n 2     0.0770       0.736       0.781\n 3     0.0780       0.737       0.781\n 4     0.0785       0.737       0.771\n 5     0.0786       0.738       0.771\n 6     0.0787       0.739       0.771\n 7     0.0789       0.739       0.760\n 8     0.0802       0.740       0.760\n 9     0.0802       0.741       0.760\n10     0.0805       0.742       0.760\n\n\n\n\n\n\n\nüîó Week 11"
  },
  {
    "objectID": "slides/lab-project-proposal.html#goals",
    "href": "slides/lab-project-proposal.html#goals",
    "title": "Lab: Project Proposal",
    "section": "Goals",
    "text": "Goals\n\nReview models with log-transformed response or predictor variables\nProject proposal"
  },
  {
    "objectID": "slides/lab-project-proposal.html#movies-data",
    "href": "slides/lab-project-proposal.html#movies-data",
    "title": "Lab: Project Proposal",
    "section": "Movies data",
    "text": "Movies data\nThe goal of this analysis is to predict the total gross revenue of a movie using opening weekend statistics. The data set includes movies released in the U.S. in 2009 that opened on 500 or more theater screens. The data were obtained from Handbook of Regression Analysis.\nThe variables we‚Äôll use are\n\nTotalGross: Total US gross revenue in millions of dollars\nOpening: opening weekend gross revenue in millions of dollars\nScreens: the number of screens on which the movie opened"
  },
  {
    "objectID": "slides/lab-project-proposal.html#exploratory-data-analysis",
    "href": "slides/lab-project-proposal.html#exploratory-data-analysis",
    "title": "Lab: Project Proposal",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nBelow are the distributions and measure of center for the response and each predictor variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\n\n\n77.03\n42.67\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\n\n\n23.33\n15.83\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\n\n\n2751.55\n2756"
  },
  {
    "objectID": "slides/lab-project-proposal.html#log-transformed-response-variable",
    "href": "slides/lab-project-proposal.html#log-transformed-response-variable",
    "title": "Lab: Project Proposal",
    "section": "Log-transformed response variable",
    "text": "Log-transformed response variable\nWe‚Äôll start by considering the following model with a log-transformed response variable. Note that OpeningCent and ScreenCent are the mean-centered version of Opening and Screens.\n\\[\n\\log(TotalGross) = \\beta_0 + \\beta_1 \\times OpeningCent + \\beta_2 \\times ScreensCent + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\]\n\nThe model output is below:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.8470\n0.0420\n91.6190\n0\n\n\nOpeningCent\n0.0251\n0.0026\n9.6891\n0\n\n\nScreensCent\n0.0005\n0.0001\n6.1691\n0\n\n\n\n\n\n\n\n\n\n\nInterpret the intercept in the context of the data.\nInterpret the effect of Opening in the context of the data."
  },
  {
    "objectID": "slides/lab-project-proposal.html#log-transformed-predictor-variable",
    "href": "slides/lab-project-proposal.html#log-transformed-predictor-variable",
    "title": "Lab: Project Proposal",
    "section": "Log-transformed predictor variable",
    "text": "Log-transformed predictor variable\nNext let‚Äôs consider the following model with a log-transformed predictor. Note that OpeningCent and ScreenCent are the mean-centered version of Opening and Screens.\n\\[TotalGross = \\beta_0 + \\beta_1 \\times \\log(OpeningCent) + \\beta_2 \\times ScreensCent + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\]\n\nThe model output is below:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n57.8738\n27.3014\n2.1198\n0.0400\n\n\nlog(OpeningCent)\n31.2932\n11.0651\n2.8281\n0.0071\n\n\nScreensCent\n0.0483\n0.0368\n1.3119\n0.1967\n\n\n\n\n\n\n\n\n\n\nInterpret the intercept in the context of the data.\nInterpret the effect of a 10% increase in Opening in the context of the data."
  },
  {
    "objectID": "slides/lab-project-proposal.html#project-proposal-1",
    "href": "slides/lab-project-proposal.html#project-proposal-1",
    "title": "Lab: Project Proposal",
    "section": "Project proposal",
    "text": "Project proposal\n\nChoose one of the usable data sets proposed in the Topic Ideas\nClick here for proposal instructions\n\n\n\n\nüîó Week 09"
  },
  {
    "objectID": "slides/19-logistic-intro.html#announcements",
    "href": "slides/19-logistic-intro.html#announcements",
    "title": "Logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nProject proposal due Fri, Nov 04 at 11:59pm\nHW 03 due Mon, Nov 07 at 11:59pm\nTeam Feedback #1 due Tue, Nov 08 at 11:59pm\n\nReceived email from Teammates around 10am\nCheck your junk/spam folder if you do not see the email.\nEmail me if you still don‚Äôt see it.\n\nSee Week 10 activities"
  },
  {
    "objectID": "slides/19-logistic-intro.html#topics",
    "href": "slides/19-logistic-intro.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUse logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/19-logistic-intro.html#computational-setup",
    "href": "slides/19-logistic-intro.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/19-logistic-intro.html#types-of-outcome-variables",
    "href": "slides/19-logistic-intro.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house in Levittown, NY\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcome variable:\n\nHigh risk of coronary heart disease\nModel: Probability an adult is high risk of heart disease given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/19-logistic-intro.html#models-for-categorical-outcomes",
    "href": "slides/19-logistic-intro.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n1: Yes, 0: No\n\nMultinomial logistic regression\n3+ Outcomes\n1: Democrat, 2: Republican, 3: Independent"
  },
  {
    "objectID": "slides/19-logistic-intro.html#election-forecasts",
    "href": "slides/19-logistic-intro.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2022 election forecasts",
    "text": "2022 election forecasts\n\nSource: FiveThirtyEight 2022 Election Forecasts"
  },
  {
    "objectID": "slides/19-logistic-intro.html#nba-finals-predictions",
    "href": "slides/19-logistic-intro.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "2020 NBA finals predictions",
    "text": "2020 NBA finals predictions\n\nSource: FiveThirtyEight 2019-20 NBA Predictions"
  },
  {
    "objectID": "slides/19-logistic-intro.html#do-teenagers-get-7-hours-of-sleep",
    "href": "slides/19-logistic-intro.html#do-teenagers-get-7-hours-of-sleep",
    "title": "Logistic regression",
    "section": "Do teenagers get 7+ hours of sleep?",
    "text": "Do teenagers get 7+ hours of sleep?\n\n\nStudents in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.\nSleep7\n1: yes\n0: no\n\n\ndata(YouthRisk2009) #from Stat2Data package\nsleep &lt;- YouthRisk2009 |&gt;\n  as_tibble() |&gt;\n  filter(!is.na(Age), !is.na(Sleep7))\nsleep |&gt;\n  relocate(Age, Sleep7)\n\n# A tibble: 446 √ó 6\n     Age Sleep7 Sleep           SmokeLife SmokeDaily MarijuaEver\n   &lt;int&gt;  &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;     &lt;fct&gt;            &lt;int&gt;\n 1    16      1 8 hours         Yes       Yes                  1\n 2    17      0 5 hours         Yes       Yes                  1\n 3    18      0 5 hours         Yes       Yes                  1\n 4    17      1 7 hours         Yes       No                   1\n 5    15      0 4 or less hours No        No                   0\n 6    17      0 6 hours         No        No                   0\n 7    17      1 7 hours         No        No                   0\n 8    16      1 8 hours         Yes       No                   0\n 9    16      1 8 hours         No        No                   0\n10    18      0 4 or less hours Yes       Yes                  1\n# ‚Ä¶ with 436 more rows"
  },
  {
    "objectID": "slides/19-logistic-intro.html#plot-the-data",
    "href": "slides/19-logistic-intro.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data\n\nggplot(sleep, aes(x = Age, y = Sleep7)) +\n  geom_point() + \n  labs(y = \"Getting 7+ hours of sleep\")"
  },
  {
    "objectID": "slides/19-logistic-intro.html#lets-fit-a-linear-regression-model",
    "href": "slides/19-logistic-intro.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs fit a linear regression model",
    "text": "Let‚Äôs fit a linear regression model\nOutcome: \\(Y\\) = 1: yes, 0: no"
  },
  {
    "objectID": "slides/19-logistic-intro.html#lets-use-proportions",
    "href": "slides/19-logistic-intro.html#lets-use-proportions",
    "title": "Logistic regression",
    "section": "Let‚Äôs use proportions",
    "text": "Let‚Äôs use proportions\nOutcome: Probability of getting 7+ hours of sleep"
  },
  {
    "objectID": "slides/19-logistic-intro.html#what-happens-if-we-zoom-out",
    "href": "slides/19-logistic-intro.html#what-happens-if-we-zoom-out",
    "title": "Logistic regression",
    "section": "What happens if we zoom out?",
    "text": "What happens if we zoom out?\nOutcome: Probability of getting 7+ hours of sleep\n\nüõë This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/19-logistic-intro.html#lets-try-another-model",
    "href": "slides/19-logistic-intro.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs try another model",
    "text": "Let‚Äôs try another model\n\n‚úÖ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/19-logistic-intro.html#the-code",
    "href": "slides/19-logistic-intro.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nggplot(sleep_age, aes(x = Age, y = prop)) +\n  geom_point() + \n  geom_hline(yintercept = c(0,1), lty = 2) + \n  stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE) +\n  labs(y = \"P(7+ hours of sleep)\") +\n  xlim(1, 40) +\n  ylim(-0.5, 1.5)"
  },
  {
    "objectID": "slides/19-logistic-intro.html#different-types-of-models",
    "href": "slides/19-logistic-intro.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLinear regression (transform Y)\nQuantitative\n\\(\\log(Y) = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)"
  },
  {
    "objectID": "slides/19-logistic-intro.html#application-exercise",
    "href": "slides/19-logistic-intro.html#application-exercise",
    "title": "Logistic regression",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 13: Logistic Regression Intro\nLinear Regression vs.¬†Logistic Regression"
  },
  {
    "objectID": "slides/19-logistic-intro.html#binary-response-variable",
    "href": "slides/19-logistic-intro.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\n\\(Y = 1: \\text{ yes}, 0: \\text{ no}\\)\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/19-logistic-intro.html#odds",
    "href": "slides/19-logistic-intro.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\n\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won‚Äôt rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)"
  },
  {
    "objectID": "slides/19-logistic-intro.html#are-teenagers-getting-enough-sleep",
    "href": "slides/19-logistic-intro.html#are-teenagers-getting-enough-sleep",
    "title": "Logistic regression",
    "section": "Are teenagers getting enough sleep?",
    "text": "Are teenagers getting enough sleep?\n\nsleep |&gt;\n  count(Sleep7) |&gt;\n  mutate(p = round(n / sum(n), 3))\n\n# A tibble: 2 √ó 3\n  Sleep7     n     p\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1      0   150 0.336\n2      1   296 0.664\n\n\n\n\\(P(\\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664\\)\n\n\n\\(P(\\text{&lt; 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336\\)\n\n\n\\(P(\\text{odds of 7+ hours of sleep}) = \\frac{0.664}{0.336} = 1.976\\)"
  },
  {
    "objectID": "slides/19-logistic-intro.html#from-odds-to-probabilities",
    "href": "slides/19-logistic-intro.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nodds\n\\[\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{\\omega}{1 + \\omega}\\]"
  },
  {
    "objectID": "slides/19-logistic-intro.html#from-odds-to-probabilities-1",
    "href": "slides/19-logistic-intro.html#from-odds-to-probabilities-1",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nLogistic model: log odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/19-logistic-intro.html#logistic-regression-model",
    "href": "slides/19-logistic-intro.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\n\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\n\\]"
  },
  {
    "objectID": "slides/19-logistic-intro.html#risk-of-coronary-heart-disease",
    "href": "slides/19-logistic-intro.html#risk-of-coronary-heart-disease",
    "title": "Logistic regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use age to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.\n\nhigh_risk:\n\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\n\nage: Age at exam time (in years)"
  },
  {
    "objectID": "slides/19-logistic-intro.html#data-heart_disease",
    "href": "slides/19-logistic-intro.html#data-heart_disease",
    "title": "Logistic regression",
    "section": "Data: heart_disease",
    "text": "Data: heart_disease\n\n\n# A tibble: 4,240 √ó 2\n     age high_risk\n   &lt;dbl&gt; &lt;fct&gt;    \n 1    39 0        \n 2    46 0        \n 3    48 0        \n 4    61 1        \n 5    46 0        \n 6    43 0        \n 7    63 1        \n 8    45 0        \n 9    52 0        \n10    43 0        \n# ‚Ä¶ with 4,230 more rows"
  },
  {
    "objectID": "slides/19-logistic-intro.html#high-risk-vs.-age",
    "href": "slides/19-logistic-intro.html#high-risk-vs.-age",
    "title": "Logistic regression",
    "section": "High risk vs.¬†age",
    "text": "High risk vs.¬†age\n\nggplot(heart_disease, aes(x = high_risk, y = age)) +\n  geom_boxplot(fill = \"steelblue\") +\n  labs(x = \"High risk - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. High risk of heart disease\")"
  },
  {
    "objectID": "slides/19-logistic-intro.html#lets-fit-the-model",
    "href": "slides/19-logistic-intro.html#lets-fit-the-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs fit the model",
    "text": "Let‚Äôs fit the model\n\nheart_disease_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0"
  },
  {
    "objectID": "slides/19-logistic-intro.html#the-model",
    "href": "slides/19-logistic-intro.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0\n\n\n\n\n\n\n\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.075 \\times \\text{age}\\] where \\(\\hat{\\pi}\\) is the predicted probability of being high risk of heart disease"
  },
  {
    "objectID": "slides/19-logistic-intro.html#predicted-log-odds",
    "href": "slides/19-logistic-intro.html#predicted-log-odds",
    "title": "Logistic regression",
    "section": "Predicted log odds",
    "text": "Predicted log odds\n\naugment(heart_disease_fit$fit) |&gt; select(.fitted, .resid)\n\n# A tibble: 4,240 √ó 2\n   .fitted .resid\n     &lt;dbl&gt;  &lt;dbl&gt;\n 1  -2.65  -0.370\n 2  -2.13  -0.475\n 3  -1.98  -0.509\n 4  -1.01   1.62 \n 5  -2.13  -0.475\n 6  -2.35  -0.427\n 7  -0.858  1.56 \n 8  -2.20  -0.458\n 9  -1.68  -0.585\n10  -2.35  -0.427\n# ‚Ä¶ with 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted odds} = \\hat{\\omega} = \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp\\{-2.650\\} = 0.071\\]"
  },
  {
    "objectID": "slides/19-logistic-intro.html#predicted-probabilities",
    "href": "slides/19-logistic-intro.html#predicted-probabilities",
    "title": "Logistic regression",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"prob\")\n\n# A tibble: 4,240 √ó 2\n   .pred_0 .pred_1\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1   0.934  0.0660\n 2   0.894  0.106 \n 3   0.878  0.122 \n 4   0.733  0.267 \n 5   0.894  0.106 \n 6   0.913  0.0870\n 7   0.702  0.298 \n 8   0.900  0.0996\n 9   0.843  0.157 \n10   0.913  0.0870\n# ‚Ä¶ with 4,230 more rows\n\n\n\n\\[\\text{predicted probabilities} = \\hat{\\pi} = \\frac{\\exp\\{-2.650\\}}{1 + \\exp\\{-2.650\\}} = 0.066\\]"
  },
  {
    "objectID": "slides/19-logistic-intro.html#predicted-classes",
    "href": "slides/19-logistic-intro.html#predicted-classes",
    "title": "Logistic regression",
    "section": "Predicted classes",
    "text": "Predicted classes\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"class\")\n\n# A tibble: 4,240 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ‚Ä¶ with 4,230 more rows"
  },
  {
    "objectID": "slides/19-logistic-intro.html#default-prediction",
    "href": "slides/19-logistic-intro.html#default-prediction",
    "title": "Logistic regression",
    "section": "Default prediction",
    "text": "Default prediction\nFor a logistic regression, the default prediction is the class.\n\npredict(heart_disease_fit, new_data = heart_disease)\n\n# A tibble: 4,240 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ‚Ä¶ with 4,230 more rows"
  },
  {
    "objectID": "slides/19-logistic-intro.html#observed-vs.-predicted",
    "href": "slides/19-logistic-intro.html#observed-vs.-predicted",
    "title": "Logistic regression",
    "section": "Observed vs.¬†predicted",
    "text": "Observed vs.¬†predicted\n\nWhat does the following table show?\n\n\npredict(heart_disease_fit, new_data = heart_disease) |&gt;\n  bind_cols(heart_disease) |&gt;\n  count(high_risk, .pred_class)\n\n# A tibble: 2 √ó 3\n  high_risk .pred_class     n\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt;\n1 0         0            3596\n2 1         0             644\n\n\n\n\nThe .pred_class is the class with the highest predicted probability. What is a limitation to using this method to determine the predicted class?"
  },
  {
    "objectID": "slides/19-logistic-intro.html#recap",
    "href": "slides/19-logistic-intro.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUsed logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#announcements",
    "href": "slides/04-slr-evaluation.html#announcements",
    "title": "SLR: Prediction + model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours have started. Click here for full schedule.\nAccept the email invitation to join the sta210-fa22 GitHub organization by today at 11:59pm.\n\nYou may also go to the course organization and click to accept on the banner at the top of the page.\nIf you don‚Äôt see the email or banner invitation, please email Prof.¬†Tackett (maria.tackett@duke.edu).\n\nLab 01 this week - will need access to RStudio and to be a member of the course GitHub organization.\nSee Week 02 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/04-slr-evaluation.html#topics",
    "href": "slides/04-slr-evaluation.html#topics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Topics",
    "text": "Topics\n\nMotivate the importance of model evaluation\nDescribe how \\(R^2\\) and RMSE are used to evaluate models\nAssess model‚Äôs predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#computational-setup",
    "href": "slides/04-slr-evaluation.html#computational-setup",
    "title": "SLR: Prediction + model evaluation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#application-exercise",
    "href": "slides/04-slr-evaluation.html#application-exercise",
    "title": "SLR: Prediction + model evaluation",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#data-source",
    "href": "slides/04-slr-evaluation.html#data-source",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data source",
    "text": "Data source\n\nThe data come from usdata::county_2019\nThese data have been compiled from the 2019 American Community Survey"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#uninsurance-rate",
    "href": "slides/04-slr-evaluation.html#uninsurance-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance rate",
    "text": "Uninsurance rate"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#high-school-graduation-rate",
    "href": "slides/04-slr-evaluation.html#high-school-graduation-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "High school graduation rate",
    "text": "High school graduation rate"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#examining-the-relationship",
    "href": "slides/04-slr-evaluation.html#examining-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Examining the relationship",
    "text": "Examining the relationship\n\n\nThe NC Labor and Economic Analysis Division (LEAD) ‚Äúcollects data, conducts research and analysis and publishes reports about the state‚Äôs economy and labor market. Information and data produced by LEAD help stakeholders make more informed decisions on business recruitment, education and workforce policies and career development, as well as gain a more extensive view of North Carolina‚Äôs economy.‚Äù\nSuppose that an analyst working for LEAD is interested in the relationship between uninsurance and high school graduation rates in NC counties.\n\n\n\n\nWhat type of visualization should the analyst make to examine the relationship between these two variables?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#data-prep",
    "href": "slides/04-slr-evaluation.html#data-prep",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data prep",
    "text": "Data prep\n\ncounty_2019_nc &lt;- county_2019 |&gt;\n  as_tibble() |&gt;\n  filter(state == \"North Carolina\") |&gt;\n  select(name, hs_grad, uninsured)\n\ncounty_2019_nc\n\n# A tibble: 100 √ó 3\n   name             hs_grad uninsured\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n 1 Alamance County     86.3      11.2\n 2 Alexander County    82.4       8.9\n 3 Alleghany County    77.5      11.3\n 4 Anson County        80.7      11.1\n 5 Ashe County         85.1      12.6\n 6 Avery County        83.6      15.9\n 7 Beaufort County     87.7      12  \n 8 Bertie County       78.4      11.9\n 9 Bladen County       81.3      12.9\n10 Brunswick County    91.3       9.8\n# ‚Ä¶ with 90 more rows"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/04-slr-evaluation.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance vs.¬†HS graduation rates",
    "text": "Uninsurance vs.¬†HS graduation rates\n\n\nCode\nggplot(county_2019_nc,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  ) +\n  geom_point(data = county_2019_nc |&gt; filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured), shape = \"circle open\", color = \"#8F2D56\", size = 4, stroke = 2) +\n  geom_text(data = county_2019_nc |&gt; filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured, label = name), color = \"#8F2D56\", fontface = \"bold\", nudge_y = 3, nudge_x = 2)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#modeling-the-relationship",
    "href": "slides/04-slr-evaluation.html#modeling-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Modeling the relationship",
    "text": "Modeling the relationship\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#fitting-the-model",
    "href": "slides/04-slr-evaluation.html#fitting-the-model",
    "title": "SLR: Prediction + model evaluation",
    "section": "Fitting the model",
    "text": "Fitting the model\nWith fit():\n\nnc_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#augmenting-the-data",
    "href": "slides/04-slr-evaluation.html#augmenting-the-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug &lt;- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 √ó 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# ‚Ä¶ with 90 more rows"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-i",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-i",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model I",
    "text": "Visualizing the model I\n\n\n\n\nBlack circles: Observed values (y = uninsured)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-ii",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-ii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model II",
    "text": "Visualizing the model II\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-iii",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-iii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model III",
    "text": "Visualizing the model III\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-iv",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-iv",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model IV",
    "text": "Visualizing the model IV\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\nGray dashed lines: Residuals"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#evaluating-the-model-fit",
    "href": "slides/04-slr-evaluation.html#evaluating-the-model-fit",
    "title": "SLR: Prediction + model evaluation",
    "section": "Evaluating the model fit",
    "text": "Evaluating the model fit\n\nHow can we evaluate whether the model for predicting uninsurance rate from high school graduation rate for NC counties is a good fit?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#two-statistics",
    "href": "slides/04-slr-evaluation.html#two-statistics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#r2",
    "href": "slides/04-slr-evaluation.html#r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.243"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#interpreting-r2",
    "href": "slides/04-slr-evaluation.html#interpreting-r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\nüó≥Ô∏è Vote on Ed Discussion\n\n\nThe \\(R^2\\) of the model for predicting uninsurance rate from high school graduation rate for NC counties is 24.3%. Which of the following is the correct interpretation of this value?\n\n\nHigh school graduation rates correctly predict 24.3% of uninsurance rates in NC counties.\n24.3% of the variability in uninsurance rates in NC counties can be explained by high school graduation rates.\n24.3% of the variability in high school graduation rates in NC counties can be explained by uninsurance rates.\n24.3% of the time uninsurance rates in NC counties can be predicted by high school graduation rates.\n\n\n\nVote - Section 001\nVote - Section 002"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#alternative-approach-for-r2",
    "href": "slides/04-slr-evaluation.html#alternative-approach-for-r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(nc_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.243         0.235  2.09      31.5 0.000000188     1  -214.  435.  443.\n# ‚Ä¶ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nglance(nc_fit)$r.squared\n\n[1] 0.2430694"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#rmse",
    "href": "slides/04-slr-evaluation.html#rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        2.07\n\n\nThe value of RMSE is not very meaningful on its own, but it‚Äôs useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#obtaining-r2-and-rmse",
    "href": "slides/04-slr-evaluation.html#obtaining-r2-and-rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#purpose-of-model-evaluation",
    "href": "slides/04-slr-evaluation.html#purpose-of-model-evaluation",
    "title": "SLR: Prediction + model evaluation",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e.¬†out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#spending-our-data",
    "href": "slides/04-slr-evaluation.html#spending-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Spending our data",
    "text": "Spending our data\n\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we‚Äôve done so far)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#simulation-data-splitting",
    "href": "slides/04-slr-evaluation.html#simulation-data-splitting",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#predictive-performance",
    "href": "slides/04-slr-evaluation.html#predictive-performance",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#bootstrapping-our-data",
    "href": "slides/04-slr-evaluation.html#bootstrapping-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/04-slr-evaluation.html#simulation-bootstrapping",
    "href": "slides/04-slr-evaluation.html#simulation-bootstrapping",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\n\nTake a bootstrap sample ‚Äì sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#predictive-performance-1",
    "href": "slides/04-slr-evaluation.html#predictive-performance-1",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#recap",
    "href": "slides/04-slr-evaluation.html#recap",
    "title": "SLR: Prediction + model evaluation",
    "section": "Recap",
    "text": "Recap\n\nMotivated the importance of model evaluation\nDescribed how \\(R^2\\) and RMSE are used to evaluate models\nAssessed model‚Äôs predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#next-week",
    "href": "slides/04-slr-evaluation.html#next-week",
    "title": "SLR: Prediction + model evaluation",
    "section": "Next week",
    "text": "Next week\nInference on the slope using\n\nSimulation-based methods\nMathematical models\n\n\n\n\nüîó Week 02"
  },
  {
    "objectID": "slides/10-exam-01-review.html#announcements",
    "href": "slides/10-exam-01-review.html#announcements",
    "title": "Exam 01 review",
    "section": "Announcements",
    "text": "Announcements\n\nExam 01: Sep 28 - 30\n\nVideos for Weeks 01 - 05 available until Sep 28 at 11:59pm\n\nNo TA office hours Wed - Fri\nNo labs this week - use time to work on exam\nEd Discussion archived Wed 7pm - Sat 9am\nSee Week 05 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/10-exam-01-review.html#exam-01",
    "href": "slides/10-exam-01-review.html#exam-01",
    "title": "Exam 01 review",
    "section": "Exam 01",
    "text": "Exam 01\n\nReleased today ~ 7pm (will receive email) and due Friday at 11:59pm\nExam instructions can be found in the README of the exam-01 Repo\n\nExercise prompts will be in exam-01.qmd\n\nCovers everything we‚Äôve done thus far - Weeks 01 - 05\nOffice Hours:\n\nThu, 10 - 11am (Prof.¬†Tackett)\n\nHave clarification questions about exam instructions during the exam?\n\nEmail Prof.¬†Tackett with ‚ÄúSTA 210 Exam‚Äù in the subject line"
  },
  {
    "objectID": "slides/15-model-comparison.html#announcements",
    "href": "slides/15-model-comparison.html#announcements",
    "title": "Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due TODAY 11:59pm\nData Science Career Panel - TODAY 12 - 1pm, online\nClick here for Week 08 activities."
  },
  {
    "objectID": "slides/15-model-comparison.html#topics",
    "href": "slides/15-model-comparison.html#topics",
    "title": "Model comparison",
    "section": "Topics",
    "text": "Topics\n\n\nANOVA for Multiple Linear Regression and sum of squares\nComparing models with \\(R^2\\) vs.¬†\\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam‚Äôs razor and parsimony"
  },
  {
    "objectID": "slides/15-model-comparison.html#computational-setup",
    "href": "slides/15-model-comparison.html#computational-setup",
    "title": "Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/15-model-comparison.html#data-restaurant-tips",
    "href": "slides/15-model-comparison.html#data-restaurant-tips",
    "title": "Model comparison",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 √ó 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ‚Ä¶ with 159 more rows"
  },
  {
    "objectID": "slides/15-model-comparison.html#variables",
    "href": "slides/15-model-comparison.html#variables",
    "title": "Model comparison",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/15-model-comparison.html#outcome-tip",
    "href": "slides/15-model-comparison.html#outcome-tip",
    "title": "Model comparison",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/15-model-comparison.html#predictors",
    "href": "slides/15-model-comparison.html#predictors",
    "title": "Model comparison",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/15-model-comparison.html#relevel-categorical-predictors",
    "href": "slides/15-model-comparison.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/15-model-comparison.html#predictors-again",
    "href": "slides/15-model-comparison.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/15-model-comparison.html#outcome-vs.-predictors",
    "href": "slides/15-model-comparison.html#outcome-vs.-predictors",
    "title": "Model comparison",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/15-model-comparison.html#fit-and-summarize-model",
    "href": "slides/15-model-comparison.html#fit-and-summarize-model",
    "title": "Model comparison",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n\n\n\n\n\nIs this the best model to explain variation in tips?"
  },
  {
    "objectID": "slides/15-model-comparison.html#another-model-summary",
    "href": "slides/15-model-comparison.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/15-model-comparison.html#analysis-of-variance-anova-1",
    "href": "slides/15-model-comparison.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can‚Äôt be explained by the model (left in the residuals)\n\nIf the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be ‚Äúvaluable‚Äù (at least one of the \\(\\beta\\)s not equal to 0)"
  },
  {
    "objectID": "slides/15-model-comparison.html#anova-output",
    "href": "slides/15-model-comparison.html#anova-output",
    "title": "Model comparison",
    "section": "ANOVA output",
    "text": "ANOVA output\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA\n\n\n\n\n\n\n\n1\nClick here for explanation about the way R calculates sum of squares for each variable."
  },
  {
    "objectID": "slides/15-model-comparison.html#anova-output-with-totals",
    "href": "slides/15-model-comparison.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/15-model-comparison.html#sum-of-squares",
    "href": "slides/15-model-comparison.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y})^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model"
  },
  {
    "objectID": "slides/15-model-comparison.html#r-squared-r2",
    "href": "slides/15-model-comparison.html#r-squared-r2",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]\n\n\n\nglance(tip_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.641         0.635  2.04      98.3 1.56e-36     3  -358.  726.  742.\n# ‚Ä¶ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/15-model-comparison.html#r-squared-r2-1",
    "href": "slides/15-model-comparison.html#r-squared-r2-1",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/15-model-comparison.html#adjusted-r2",
    "href": "slides/15-model-comparison.html#adjusted-r2",
    "title": "Model comparison",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/15-model-comparison.html#r2-and-adjusted-r2",
    "href": "slides/15-model-comparison.html#r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]"
  },
  {
    "objectID": "slides/15-model-comparison.html#using-r2-and-adjusted-r2",
    "href": "slides/15-model-comparison.html#using-r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/15-model-comparison.html#application-exercise",
    "href": "slides/15-model-comparison.html#application-exercise",
    "title": "Model comparison",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 09: Model comparison\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/15-model-comparison.html#comparing-models-with-r2_adj",
    "href": "slides/15-model-comparison.html#comparing-models-with-r2_adj",
    "title": "Model comparison",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n    data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.674         0.664\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.683         0.662"
  },
  {
    "objectID": "slides/15-model-comparison.html#aic-bic",
    "href": "slides/15-model-comparison.html#aic-bic",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike‚Äôs Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz‚Äôs Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/15-model-comparison.html#aic-bic-1",
    "href": "slides/15-model-comparison.html#aic-bic-1",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/15-model-comparison.html#aic-bic-2",
    "href": "slides/15-model-comparison.html#aic-bic-2",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/15-model-comparison.html#aic-bic-3",
    "href": "slides/15-model-comparison.html#aic-bic-3",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/15-model-comparison.html#using-aic-bic",
    "href": "slides/15-model-comparison.html#using-aic-bic",
    "title": "Model comparison",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e.¬†models with fewer terms)"
  },
  {
    "objectID": "slides/15-model-comparison.html#application-exercise-1",
    "href": "slides/15-model-comparison.html#application-exercise-1",
    "title": "Model comparison",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 09: Model comparison\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/15-model-comparison.html#comparing-models-with-aic-and-bic",
    "href": "slides/15-model-comparison.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 √ó 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  714.  736.\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 √ó 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  720.  757."
  },
  {
    "objectID": "slides/15-model-comparison.html#commonalities-between-criteria",
    "href": "slides/15-model-comparison.html#commonalities-between-criteria",
    "title": "Model comparison",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/15-model-comparison.html#parsimony-and-occams-razor",
    "href": "slides/15-model-comparison.html#parsimony-and-occams-razor",
    "title": "Model comparison",
    "section": "Parsimony and Occam‚Äôs razor",
    "text": "Parsimony and Occam‚Äôs razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam‚Äôs razor because he ‚Äúshaved‚Äù his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/15-model-comparison.html#in-pursuit-of-occams-razor",
    "href": "slides/15-model-comparison.html#in-pursuit-of-occams-razor",
    "title": "Model comparison",
    "section": "In pursuit of Occam‚Äôs razor",
    "text": "In pursuit of Occam‚Äôs razor\n\nOccam‚Äôs razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e.¬†parsimonious model"
  },
  {
    "objectID": "slides/15-model-comparison.html#alternate-views",
    "href": "slides/15-model-comparison.html#alternate-views",
    "title": "Model comparison",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/15-model-comparison.html#other-concerns-with-our-approach",
    "href": "slides/15-model-comparison.html#other-concerns-with-our-approach",
    "title": "Model comparison",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we‚Äôre making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n‚Äútrain‚Äù the model on the training data and pick a few models we‚Äôre genuinely considering as potentially good models\ntest those models on the testing set"
  },
  {
    "objectID": "slides/15-model-comparison.html#recap",
    "href": "slides/15-model-comparison.html#recap",
    "title": "Model comparison",
    "section": "Recap",
    "text": "Recap\n\nANOVA for Multiple Linear Regression and sum of squares\nComparing models with \\(R^2\\) vs.¬†\\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam‚Äôs razor and parsimony\n\n\n\n\nüîó Week 08"
  },
  {
    "objectID": "slides/07-slr-math-models.html#announcements",
    "href": "slides/07-slr-math-models.html#announcements",
    "title": "SLR: Mathematical models for inference",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due\n\nToday at 11:59pm (Thursday labs)\nTue, Sep 20 at 11:59pm (Friday labs)\n\nHW 01: due Wed, Sep 21 at 11:59pm\nStatistics experience - due Fri, Dec 09 at 11:59pm\nLab 01 solutions posted in Resources folder in Sakai\nSee Week 04 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/07-slr-math-models.html#topics",
    "href": "slides/07-slr-math-models.html#topics",
    "title": "SLR: Mathematical models for inference",
    "section": "Topics",
    "text": "Topics\n\nDefine mathematical models to conduct inference for the slope\nUse mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope"
  },
  {
    "objectID": "slides/07-slr-math-models.html#computational-setup",
    "href": "slides/07-slr-math-models.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/07-slr-math-models.html#the-regression-model-revisited",
    "href": "slides/07-slr-math-models.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/07-slr-math-models.html#inference-revisited",
    "href": "slides/07-slr-math-models.html#inference-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we‚Äôll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/07-slr-math-models.html#mathematical-representation-of-the-model",
    "href": "slides/07-slr-math-models.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn‚Äôt tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/07-slr-math-models.html#mathematical-representation-visualized",
    "href": "slides/07-slr-math-models.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/07-slr-math-models.html#regression-standard-error",
    "href": "slides/07-slr-math-models.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error (the spread of the distribution of the response, for a given value of the predictor variable):\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/07-slr-math-models.html#standard-error-of-hatbeta_1",
    "href": "slides/07-slr-math-models.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\nor‚Ä¶\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/07-slr-math-models.html#hypothesis-test-for-the-slope",
    "href": "slides/07-slr-math-models.html#hypothesis-test-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test for the slope",
    "text": "Hypothesis test for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/07-slr-math-models.html#hypothesis-test-test-statistic",
    "href": "slides/07-slr-math-models.html#hypothesis-test-test-statistic",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Test statistic",
    "text": "Hypothesis test: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{159.48 - 0}{18.17} = 8.78\n\\]"
  },
  {
    "objectID": "slides/07-slr-math-models.html#hypothesis-test-p-value",
    "href": "slides/07-slr-math-models.html#hypothesis-test-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/07-slr-math-models.html#understanding-the-p-value",
    "href": "slides/07-slr-math-models.html#understanding-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/07-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "href": "slides/07-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest."
  },
  {
    "objectID": "slides/07-slr-math-models.html#confidence-interval-for-the-slope",
    "href": "slides/07-slr-math-models.html#confidence-interval-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/07-slr-math-models.html#confidence-interval-critical-value",
    "href": "slides/07-slr-math-models.html#confidence-interval-critical-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(duke_forest) - 2)\n\n[1] 1.984984\n\n# confidence level: 90%\nqt(0.95, df = nrow(duke_forest) - 2)\n\n[1] 1.660881\n\n# confidence level: 99%\nqt(0.995, df = nrow(duke_forest) - 2)\n\n[1] 2.628016"
  },
  {
    "objectID": "slides/07-slr-math-models.html#ci-for-the-slope-calculation",
    "href": "slides/07-slr-math-models.html#ci-for-the-slope-calculation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 159.48 \\hspace{15mm} t^* = 1.98 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 18.17\\]\n\n\\[\n159.48 \\pm 1.98 \\times 18.17 = (123.50, 195.46)\n\\]"
  },
  {
    "objectID": "slides/07-slr-math-models.html#ci-for-the-slope-computation",
    "href": "slides/07-slr-math-models.html#ci-for-the-slope-computation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(df_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n10847.77\n222456.88\n\n\narea\n159.48\n18.17\n8.78\n0.00\n123.41\n195.55"
  },
  {
    "objectID": "slides/07-slr-math-models.html#intervals-for-predictions-1",
    "href": "slides/07-slr-math-models.html#intervals-for-predictions-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nSuppose we want to answer the question ‚ÄúWhat is the predicted sale price of a Duke Forest house that is 2,800 square feet?‚Äù\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/07-slr-math-models.html#two-types-of-predictions",
    "href": "slides/07-slr-math-models.html#two-types-of-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: ‚ÄúWhat is the average predicted sale price of Duke Forest houses that are 2,800 square feet?‚Äù\nPrediction for an individual observation: ‚ÄúWhat is the predicted sale price of a Duke Forest house that is 2,800 square feet?‚Äù\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/07-slr-math-models.html#uncertainty-in-predictions",
    "href": "slides/07-slr-math-models.html#uncertainty-in-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/07-slr-math-models.html#standard-errors",
    "href": "slides/07-slr-math-models.html#standard-errors",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/07-slr-math-models.html#standard-errors-1",
    "href": "slides/07-slr-math-models.html#standard-errors-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/07-slr-math-models.html#confidence-interval",
    "href": "slides/07-slr-math-models.html#confidence-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_house &lt;- tibble(area = 2800)\n\npredict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95) |&gt;\n  kable()\n\n\n\n\n.pred_lower\n.pred_upper\n\n\n\n\n529351\n597060.1\n\n\n\n\n\n\n\n\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between $529,351 and $597,060."
  },
  {
    "objectID": "slides/07-slr-math-models.html#prediction-interval",
    "href": "slides/07-slr-math-models.html#prediction-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95) |&gt;\n  kable()\n\n\n\n\n.pred_lower\n.pred_upper\n\n\n\n\n226438.3\n899972.7\n\n\n\n\n\n\n\n\nWe are 95% confident that predicted sale price of a Duke Forest house that is 2,800 square feet is between $226,438 and $899,973."
  },
  {
    "objectID": "slides/07-slr-math-models.html#comparing-intervals",
    "href": "slides/07-slr-math-models.html#comparing-intervals",
    "title": "SLR: Mathematical models for inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/07-slr-math-models.html#extrapolation",
    "href": "slides/07-slr-math-models.html#extrapolation",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\n\n\n\nCalculate the prediction interval for the sale price of a ‚Äútiny house‚Äù in Duke Forest that is 225 square feet.\n\n\n\n\n\n\n\n\n\n\nNo, thanks!\n\n\n\nüîó Week 04"
  },
  {
    "objectID": "slides/lab-06.html#goals",
    "href": "slides/lab-06.html#goals",
    "title": "Lab 06",
    "section": "Goals",
    "text": "Goals\n\nComplete AE 13\nLab 06: Logistic regression intro"
  },
  {
    "objectID": "slides/lab-06.html#resources-for-lab-06",
    "href": "slides/lab-06.html#resources-for-lab-06",
    "title": "Lab 06",
    "section": "Resources for Lab 06",
    "text": "Resources for Lab 06\n\nLecture notes\n\nLogistic Regression: Introduction\n\nIMS, Chap 9: Logistic Regression\n\n\n\n\nüîó Week 10"
  },
  {
    "objectID": "slides/16-cross-validation.html#announcements",
    "href": "slides/16-cross-validation.html#announcements",
    "title": "Cross validation",
    "section": "Announcements",
    "text": "Announcements\n\nLab 05\n\ndue TODAY, 11:59pm (Thu labs)\ndue Tuesday, 11:59pm (Fri labs)\n\nOffice hours update:\n\nMonday, 1 - 2pm: in-person only (Old Chem 118B)\n\nClick here for explanation about sum of squares in R ANOVA output.\nSee Week 09 activities."
  },
  {
    "objectID": "slides/16-cross-validation.html#spring-2023-statistics-classes",
    "href": "slides/16-cross-validation.html#spring-2023-statistics-classes",
    "title": "Cross validation",
    "section": "Spring 2023 Statistics classes",
    "text": "Spring 2023 Statistics classes\n\nSTA 211: Mathematics of Regression\n\nPre-req: STA 210 + Math 216/218/221\n\nSTA 240: Probability for Statistical Inference, Modeling, and Data Analysis\n\nPre-req: Calc 2\n\nSTA 313: Advanced Data Visualization\n\nPre-req: STA 199 or STA 210\n\nSTA 323: Statistical Computing\n\nPre-req: STA 210 and STA 230 / 240\n\nSTA 360: Bayesian Inference and Modern Statistical Methods\n\nPre-req: STA 210, STA 230/240, CS 101, Calc 2, Math 216/218/221,\nCo-req: STA 211"
  },
  {
    "objectID": "slides/16-cross-validation.html#topics",
    "href": "slides/16-cross-validation.html#topics",
    "title": "Cross validation",
    "section": "Topics",
    "text": "Topics\n\n\nCross validation for model evaluation\nCross validation for model comparison"
  },
  {
    "objectID": "slides/16-cross-validation.html#computational-setup",
    "href": "slides/16-cross-validation.html#computational-setup",
    "title": "Cross validation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(schrute)"
  },
  {
    "objectID": "slides/16-cross-validation.html#data-goal",
    "href": "slides/16-cross-validation.html#data-goal",
    "title": "Cross validation",
    "section": "Data & goal",
    "text": "Data & goal\n\n\nData: The data come from the shrute package, and has been transformed using instructions from Lab 04\nGoal: Predict imdb_rating from other variables in the dataset\n\n\n\noffice_episodes &lt;- read_csv(here::here(\"slides/data/office_episodes.csv\"))\noffice_episodes\n\n# A tibble: 186 √ó 14\n   season episode episode_n‚Ä¶¬π imdb_‚Ä¶¬≤ total‚Ä¶¬≥ air_date   lines‚Ä¶‚Å¥ lines‚Ä¶‚Åµ lines‚Ä¶‚Å∂\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      1       1 Pilot           7.6    3706 2005-03-24  0.157   0.179    0.354\n 2      1       2 Diversity ‚Ä¶     8.3    3566 2005-03-29  0.123   0.0591   0.369\n 3      1       3 Health Care     7.9    2983 2005-04-05  0.172   0.131    0.230\n 4      1       4 The Allian‚Ä¶     8.1    2886 2005-04-12  0.202   0.0905   0.280\n 5      1       5 Basketball      8.4    3179 2005-04-19  0.0913  0.0609   0.452\n 6      1       6 Hot Girl        7.8    2852 2005-04-26  0.159   0.130    0.306\n 7      2       1 The Dundies     8.7    3213 2005-09-20  0.125   0.160    0.375\n 8      2       2 Sexual Har‚Ä¶     8.2    2736 2005-09-27  0.0565  0.0954   0.353\n 9      2       3 Office Oly‚Ä¶     8.4    2742 2005-10-04  0.196   0.117    0.295\n10      2       4 The Fire        8.4    2713 2005-10-11  0.160   0.0690   0.216\n# ‚Ä¶ with 176 more rows, 5 more variables: lines_dwight &lt;dbl&gt;, halloween &lt;chr&gt;,\n#   valentine &lt;chr&gt;, christmas &lt;chr&gt;, michael &lt;chr&gt;, and abbreviated variable\n#   names ¬π‚Äãepisode_name, ¬≤‚Äãimdb_rating, ¬≥‚Äãtotal_votes, ‚Å¥‚Äãlines_jim, ‚Åµ‚Äãlines_pam,\n#   ‚Å∂‚Äãlines_michael"
  },
  {
    "objectID": "slides/16-cross-validation.html#split-data-into-training-and-testing",
    "href": "slides/16-cross-validation.html#split-data-into-training-and-testing",
    "title": "Cross validation",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\n\nset.seed(123)\noffice_split &lt;- initial_split(office_episodes)\noffice_train &lt;- training(office_split)\noffice_test &lt;- testing(office_split)"
  },
  {
    "objectID": "slides/16-cross-validation.html#specify-model",
    "href": "slides/16-cross-validation.html#specify-model",
    "title": "Cross validation",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/16-cross-validation.html#from-lab-04",
    "href": "slides/16-cross-validation.html#from-lab-04",
    "title": "Cross validation",
    "section": "From Lab 04",
    "text": "From Lab 04\nCreate a recipe that uses the newly generated variables\n\nDenotes episode_name as an ID variable and doesn‚Äôt use air_date or season as predictors\nCreate dummy variables for all nominal predictors\nRemove all zero variance predictors"
  },
  {
    "objectID": "slides/16-cross-validation.html#create-recipe",
    "href": "slides/16-cross-validation.html#create-recipe",
    "title": "Cross validation",
    "section": "Create recipe",
    "text": "Create recipe\n\noffice_rec1 &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  update_role(episode_name, new_role = \"id\") |&gt;\n  step_rm(air_date, season) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\noffice_rec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor         12\n\nOperations:\n\nVariables removed air_date, season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/16-cross-validation.html#preview-recipe",
    "href": "slides/16-cross-validation.html#preview-recipe",
    "title": "Cross validation",
    "section": "Preview recipe",
    "text": "Preview recipe\n\nprep(office_rec1) |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 139\nColumns: 12\n$ episode       &lt;dbl&gt; 20, 16, 8, 7, 23, 3, 16, 21, 18, 14, 27, 28, 12, 1, 23, ‚Ä¶\n$ episode_name  &lt;fct&gt; \"Welcome Party\", \"Moving On\", \"Performance Review\", \"The‚Ä¶\n$ total_votes   &lt;dbl&gt; 1489, 1572, 2416, 1406, 2783, 1802, 2283, 2041, 1445, 14‚Ä¶\n$ lines_jim     &lt;dbl&gt; 0.12703583, 0.05588822, 0.09523810, 0.07482993, 0.078291‚Ä¶\n$ lines_pam     &lt;dbl&gt; 0.10423453, 0.10978044, 0.10989011, 0.15306122, 0.081850‚Ä¶\n$ lines_michael &lt;dbl&gt; 0.0000000, 0.0000000, 0.3772894, 0.0000000, 0.3736655, 0‚Ä¶\n$ lines_dwight  &lt;dbl&gt; 0.07166124, 0.08782435, 0.15384615, 0.18027211, 0.135231‚Ä¶\n$ imdb_rating   &lt;dbl&gt; 7.2, 8.2, 8.2, 7.7, 9.1, 8.2, 8.3, 8.9, 8.0, 7.8, 8.7, 8‚Ä¶\n$ halloween_yes &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ valentine_yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ christmas_yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,‚Ä¶\n$ michael_yes   &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "slides/16-cross-validation.html#create-workflow",
    "href": "slides/16-cross-validation.html#create-workflow",
    "title": "Cross validation",
    "section": "Create workflow",
    "text": "Create workflow\n\noffice_wflow1 &lt;- workflow() |&gt;\n  add_model(office_spec) |&gt;\n  add_recipe(office_rec1)\n\noffice_wflow1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n3 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/16-cross-validation.html#fit-model-to-training-data",
    "href": "slides/16-cross-validation.html#fit-model-to-training-data",
    "title": "Cross validation",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nNot so fast!"
  },
  {
    "objectID": "slides/16-cross-validation.html#spending-our-data",
    "href": "slides/16-cross-validation.html#spending-our-data",
    "title": "Cross validation",
    "section": "Spending our data",
    "text": "Spending our data\n\nWe have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\nHowever, we usually need to understand the effectiveness of the model before using the test set.\nTypically we can‚Äôt decide on which final model to take to the test set without making model assessments.\nRemedy: Resampling to make model assessments on training data in a way that can generalize to new data."
  },
  {
    "objectID": "slides/16-cross-validation.html#resampling-for-model-assessment",
    "href": "slides/16-cross-validation.html#resampling-for-model-assessment",
    "title": "Cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set. Model fit statistics such as \\(R^2_{Adj}\\), AIC, and BIC are calculated based on this fit.\nThe model is evaluated with the assessment set."
  },
  {
    "objectID": "slides/16-cross-validation.html#resampling-for-model-assessment-1",
    "href": "slides/16-cross-validation.html#resampling-for-model-assessment-1",
    "title": "Cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\nSource: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/16-cross-validation.html#analysis-and-assessment-sets",
    "href": "slides/16-cross-validation.html#analysis-and-assessment-sets",
    "title": "Cross validation",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/16-cross-validation.html#cross-validation-1",
    "href": "slides/16-cross-validation.html#cross-validation-1",
    "title": "Cross validation",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation ‚Äì commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\nRepeat v times, updating which partition is used for assessment each time\n\n\nLet‚Äôs give an example where v = 3‚Ä¶"
  },
  {
    "objectID": "slides/16-cross-validation.html#cross-validation-step-1",
    "href": "slides/16-cross-validation.html#cross-validation-step-1",
    "title": "Cross validation",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/16-cross-validation.html#split-data",
    "href": "slides/16-cross-validation.html#split-data",
    "title": "Cross validation",
    "section": "Split data",
    "text": "Split data\n\nset.seed(345)\nfolds &lt;- vfold_cv(office_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 √ó 2\n  splits          id   \n  &lt;list&gt;          &lt;chr&gt;\n1 &lt;split [92/47]&gt; Fold1\n2 &lt;split [93/46]&gt; Fold2\n3 &lt;split [93/46]&gt; Fold3"
  },
  {
    "objectID": "slides/16-cross-validation.html#cross-validation-steps-2-and-3",
    "href": "slides/16-cross-validation.html#cross-validation-steps-2-and-3",
    "title": "Cross validation",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\n\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/16-cross-validation.html#fit-resamples",
    "href": "slides/16-cross-validation.html#fit-resamples",
    "title": "Cross validation",
    "section": "Fit resamples",
    "text": "Fit resamples\n\n# Function to get Adj R-sq, AIC, BIC\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}\n\nset.seed(456)\n\n# Fit model and calculate statistics for each fold\noffice_fit_rs1 &lt;- office_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n\noffice_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 √ó 5\n  splits          id    .metrics         .notes           .extracts       \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [92/47]&gt; Fold1 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n2 &lt;split [93/46]&gt; Fold2 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;\n3 &lt;split [93/46]&gt; Fold3 &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1 √ó 2]&gt;"
  },
  {
    "objectID": "slides/16-cross-validation.html#cross-validation-now-what",
    "href": "slides/16-cross-validation.html#cross-validation-now-what",
    "title": "Cross validation",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe‚Äôve fit a bunch of models\nNow it‚Äôs time to use them to collect metrics (e.g., $R^2$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/16-cross-validation.html#collect-r2-and-rmse-from-cv",
    "href": "slides/16-cross-validation.html#collect-r2-and-rmse-from-cv",
    "title": "Cross validation",
    "section": "Collect \\(R^2\\) and RMSE from CV",
    "text": "Collect \\(R^2\\) and RMSE from CV\n\n# Produces summary across all CV\ncollect_metrics(office_fit_rs1)\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.353     3  0.0117 Preprocessor1_Model1\n2 rsq     standard   0.539     3  0.0378 Preprocessor1_Model1\n\n\n\nNote: These are calculated using the assessment data"
  },
  {
    "objectID": "slides/16-cross-validation.html#deeper-look-into-r2-and-rmse",
    "href": "slides/16-cross-validation.html#deeper-look-into-r2-and-rmse",
    "title": "Cross validation",
    "section": "Deeper look into \\(R^2\\) and RMSE",
    "text": "Deeper look into \\(R^2\\) and RMSE\n\ncv_metrics1 &lt;- collect_metrics(office_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 √ó 5\n  id    .metric .estimator .estimate .config             \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Fold1 rmse    standard       0.355 Preprocessor1_Model1\n2 Fold1 rsq     standard       0.525 Preprocessor1_Model1\n3 Fold2 rmse    standard       0.373 Preprocessor1_Model1\n4 Fold2 rsq     standard       0.481 Preprocessor1_Model1\n5 Fold3 rmse    standard       0.332 Preprocessor1_Model1\n6 Fold3 rsq     standard       0.610 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/16-cross-validation.html#better-tabulation-of-r2-and-rmse-from-cv",
    "href": "slides/16-cross-validation.html#better-tabulation-of-r2-and-rmse-from-cv",
    "title": "Cross validation",
    "section": "Better tabulation of \\(R^2\\) and RMSE from CV",
    "text": "Better tabulation of \\(R^2\\) and RMSE from CV\n\ncv_metrics1 |&gt;\n  mutate(.estimate = round(.estimate, 3)) |&gt;\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |&gt;\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n0.355\n0.525\n\n\nFold2\n0.373\n0.481\n\n\nFold3\n0.332\n0.610"
  },
  {
    "objectID": "slides/16-cross-validation.html#how-does-rmse-compare-to-y",
    "href": "slides/16-cross-validation.html#how-does-rmse-compare-to-y",
    "title": "Cross validation",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\n\n\nCross validation RMSE stats:\n\ncv_metrics1 |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 √ó 4\n    min   max  mean     sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 0.332 0.373 0.353 0.0202\n\n\n\nTraining data IMDB score stats:\n\noffice_episodes |&gt;\n  \n  summarise(\n    min = min(imdb_rating),\n    max = max(imdb_rating),\n    mean = mean(imdb_rating),\n    sd = sd(imdb_rating)\n  )\n\n# A tibble: 1 √ó 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7  8.25 0.535"
  },
  {
    "objectID": "slides/16-cross-validation.html#collect-r2_adj-aic-bic-from-cv",
    "href": "slides/16-cross-validation.html#collect-r2_adj-aic-bic-from-cv",
    "title": "Cross validation",
    "section": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV",
    "text": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV\n\nmap_df(office_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  bind_cols(Fold = office_fit_rs1$id)\n\n# A tibble: 3 √ó 4\n  adj.r.squared   AIC   BIC Fold \n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1         0.585  70.3 101.  Fold1\n2         0.615  63.0  93.4 Fold2\n3         0.553  77.6 108.  Fold3\n\n\n\nNote: These are based on the model fit from the analysis data"
  },
  {
    "objectID": "slides/16-cross-validation.html#cross-validation-jargon",
    "href": "slides/16-cross-validation.html#cross-validation-jargon",
    "title": "Cross validation",
    "section": "Cross validation jargon",
    "text": "Cross validation jargon\n\nReferred to as v-fold or k-fold cross validation\nAlso commonly abbreviated as CV"
  },
  {
    "objectID": "slides/16-cross-validation.html#cross-validation-in-practice",
    "href": "slides/16-cross-validation.html#cross-validation-in-practice",
    "title": "Cross validation",
    "section": "Cross validation in practice",
    "text": "Cross validation in practice\n\n\nTo illustrate how CV works, we used v = 3:\n\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/16-cross-validation.html#recap",
    "href": "slides/16-cross-validation.html#recap",
    "title": "Cross validation",
    "section": "Recap",
    "text": "Recap\n\n\nCross validation for model evaluation\nCross validation for model comparison\n\n\n\n\n\nüîó Week 09"
  },
  {
    "objectID": "slides/14-transform-predictor.html#announcements",
    "href": "slides/14-transform-predictor.html#announcements",
    "title": "Variable transformations",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due Wed, Oct 19, 11:59pm\nData Science Career Panel - October 19, 12 - 1pm, online\nClick here for Week 08 activities."
  },
  {
    "objectID": "slides/14-transform-predictor.html#mid-semester-survey",
    "href": "slides/14-transform-predictor.html#mid-semester-survey",
    "title": "Variable transformations",
    "section": "Mid-semester survey",
    "text": "Mid-semester survey\nThank you to everyone who filled out a mid-semester survey!\nMost helpful with learning\n\nLectures / application exercises\nOffice hours\n\nSomething to do more of to help with learning\n\nReviewing assignments / common mistakes\nMore examples /application exercises\n\nSomething the students can do more of / keep doing\n\nReview lecture notes\nDo assigned readings\nAttend office hours"
  },
  {
    "objectID": "slides/14-transform-predictor.html#mid-semester-survey-1",
    "href": "slides/14-transform-predictor.html#mid-semester-survey-1",
    "title": "Variable transformations",
    "section": "Mid-semester survey",
    "text": "Mid-semester survey\nOther notes:\n\nWe will review office hours schedule to make sure they are scheduled during times that don‚Äôt have major conflicts\nGrading\n\nWording in statistics matters! For example - these are two different statements:\n\nCorrect: For every one month in age, we expect the respiratory rate to decrease by 0.659 breaths per minute, on average.\nIncorrect: For every one month in age, the respiratory rate will decrease by 0.659 breaths per minute.\n\nFull credit is awarded for (1) using the most appropriate methods (e.g., appropriate summary statistics given a distribution), (2) comprehensively and accurately justifying response, (3) consistency in response and explanation.\nThere is an example in the lecture notes, application exercises, and/or readings."
  },
  {
    "objectID": "slides/14-transform-predictor.html#regrade-requests",
    "href": "slides/14-transform-predictor.html#regrade-requests",
    "title": "Variable transformations",
    "section": "Regrade requests",
    "text": "Regrade requests\n\nDates they are available in email from Gradescope\nReview solutions and ask during office hours first\nDo not submit regrade requests to dispute point values\nQuestion is completely regraded by Prof.¬†Tackett or Head TA\nPolicy in syllabus"
  },
  {
    "objectID": "slides/14-transform-predictor.html#topics",
    "href": "slides/14-transform-predictor.html#topics",
    "title": "Variable transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/14-transform-predictor.html#computational-set-up",
    "href": "slides/14-transform-predictor.html#computational-set-up",
    "title": "Variable transformations",
    "section": "Computational set up",
    "text": "Computational set up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Sleuth3) \nlibrary(patchwork)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/14-transform-predictor.html#respiratory-rate-vs.-age",
    "href": "slides/14-transform-predictor.html#respiratory-rate-vs.-age",
    "title": "Variable transformations",
    "section": "Respiratory Rate vs.¬†Age",
    "text": "Respiratory Rate vs.¬†Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a ‚Äúhigh‚Äù rate, we first want to understand the relationship between a child‚Äôs age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication ‚ÄúReference Values for Respiratory Rate in the First 3 Years of Life‚Äù.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/14-transform-predictor.html#rate-vs.-age",
    "href": "slides/14-transform-predictor.html#rate-vs.-age",
    "title": "Variable transformations",
    "section": "Rate vs.¬†Age",
    "text": "Rate vs.¬†Age"
  },
  {
    "objectID": "slides/14-transform-predictor.html#training-test-sets",
    "href": "slides/14-transform-predictor.html#training-test-sets",
    "title": "Variable transformations",
    "section": "Training + test sets",
    "text": "Training + test sets\n\nset.seed(101222)\n# iniital split \nresp_split &lt;- initial_split(respiratory)\n\n# training set\nresp_train &lt;- training(resp_split)\n\n# test set\nresp_test &lt;- testing(resp_split)"
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-1-rate-vs.-age",
    "href": "slides/14-transform-predictor.html#model-1-rate-vs.-age",
    "title": "Variable transformations",
    "section": "Model 1: Rate vs.¬†Age",
    "text": "Model 1: Rate vs.¬†Age\n\nresp_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Rate ~ Age, data = resp_train)\n\ntidy(resp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n46.458\n0.589\n78.924\n0\n\n\nAge\n-0.659\n0.034\n-19.498\n0"
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-1-residuals",
    "href": "slides/14-transform-predictor.html#model-1-residuals",
    "title": "Variable transformations",
    "section": "Model 1: Residuals",
    "text": "Model 1: Residuals"
  },
  {
    "objectID": "slides/14-transform-predictor.html#consider-different-transformations",
    "href": "slides/14-transform-predictor.html#consider-different-transformations",
    "title": "Variable transformations",
    "section": "Consider different transformations‚Ä¶",
    "text": "Consider different transformations‚Ä¶"
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-2-lograte-vs.-age",
    "href": "slides/14-transform-predictor.html#model-2-lograte-vs.-age",
    "title": "Variable transformations",
    "section": "Model 2: log(Rate) vs.¬†Age",
    "text": "Model 2: log(Rate) vs.¬†Age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.831\n0.015\n259.086\n0\n\n\nAge\n-0.018\n0.001\n-21.243\n0\n\n\n\n\n\n\n\nSlope: For each additional month in a child‚Äôs age, the median respiratory rate is expected to multiply by a factor of 0.982 [exp(-0.018)].\nIntercept: The median respiratory rate for children who are 0 months old is expected to be 29.4 [exp(3.381)]."
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-2-residuals",
    "href": "slides/14-transform-predictor.html#model-2-residuals",
    "title": "Variable transformations",
    "section": "Model 2: Residuals",
    "text": "Model 2: Residuals"
  },
  {
    "objectID": "slides/14-transform-predictor.html#compare-residual-plots",
    "href": "slides/14-transform-predictor.html#compare-residual-plots",
    "title": "Variable transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/14-transform-predictor.html#log-transformation-on-x",
    "href": "slides/14-transform-predictor.html#log-transformation-on-x",
    "title": "Variable transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/14-transform-predictor.html#rate-vs.-logage",
    "href": "slides/14-transform-predictor.html#rate-vs.-logage",
    "title": "Variable transformations",
    "section": "Rate vs.¬†log(Age)",
    "text": "Rate vs.¬†log(Age)"
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-with-transformation-on-x",
    "href": "slides/14-transform-predictor.html#model-with-transformation-on-x",
    "title": "Variable transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e.¬†the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-3-rate-vs.-logage",
    "href": "slides/14-transform-predictor.html#model-3-rate-vs.-logage",
    "title": "Variable transformations",
    "section": "Model 3: Rate vs.¬†log(Age)",
    "text": "Model 3: Rate vs.¬†log(Age)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n49.397\n0.755\n65.436\n0\n\n\nlog_age\n-5.668\n0.311\n-18.248\n0\n\n\n\n\n\n\n\nInterpret the slope and intercept in the context of the data.\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/14-transform-predictor.html#model-3-residuals",
    "href": "slides/14-transform-predictor.html#model-3-residuals",
    "title": "Variable transformations",
    "section": "Model 3: Residuals",
    "text": "Model 3: Residuals"
  },
  {
    "objectID": "slides/14-transform-predictor.html#choose-a-model",
    "href": "slides/14-transform-predictor.html#choose-a-model",
    "title": "Variable transformations",
    "section": "Choose a model",
    "text": "Choose a model\nRecall the goal of the analysis:\nIn order to determine what indicates a ‚Äúhigh‚Äù rate, we first want to understand the relationship between a child‚Äôs age and their respiratory rate.\n\n\nWhich is the preferred metric to compare the models - \\(R^2\\) or RMSE?"
  },
  {
    "objectID": "slides/14-transform-predictor.html#compare-models-on-testing-data",
    "href": "slides/14-transform-predictor.html#compare-models-on-testing-data",
    "title": "Variable transformations",
    "section": "Compare models on testing data",
    "text": "Compare models on testing data\n\n\n\n\n\n\n\n\nRate vs.¬†Age\nlog(Rate) vs.¬†Age\nRate vs.¬†log(Age)\n\n\n\n\n0.549\n0.596\n0.559\n\n\n\n\n\nWhich model would you choose?"
  },
  {
    "objectID": "slides/14-transform-predictor.html#learn-more",
    "href": "slides/14-transform-predictor.html#learn-more",
    "title": "Variable transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables.\n\n\n\nüîó Week 08"
  },
  {
    "objectID": "slides/lab-04.html#goals",
    "href": "slides/lab-04.html#goals",
    "title": "Lab 04",
    "section": "Goals",
    "text": "Goals\n\nMeet your team!\nTeam agreement\nLab 04: The Office\nSTA 210 Mid Semester Survey"
  },
  {
    "objectID": "slides/lab-04.html#meet-your-team",
    "href": "slides/lab-04.html#meet-your-team",
    "title": "Lab 04",
    "section": "Meet your team!",
    "text": "Meet your team!\n\nClick here to find your team.\nSit with your team."
  },
  {
    "objectID": "slides/lab-04.html#team-name-agreement",
    "href": "slides/lab-04.html#team-name-agreement",
    "title": "Lab 04",
    "section": "Team name + agreement",
    "text": "Team name + agreement\n\nCome up with a team name. You can‚Äôt have the same name as another group in the class, so be creative!\n\nYour TA will get your team name by the end of lab.\n\nFill out the team agreement. The goals of the agreement are to‚Ä¶\n\nGain a common understanding of the team‚Äôs goals and expectations for collaboration\nMake a plan for team communication\nMake a plan for working outside of lab"
  },
  {
    "objectID": "slides/lab-04.html#team-workflow",
    "href": "slides/lab-04.html#team-workflow",
    "title": "Lab 04",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are markers in today‚Äôs lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it‚Äôs not your turn type.\n\nDon‚Äôt forget to pull to get your teammates‚Äô updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-04.html#team-workflow-in-action",
    "href": "slides/lab-04.html#team-workflow-in-action",
    "title": "Lab 04",
    "section": "Team workflow, in action",
    "text": "Team workflow, in action\n\nComplete the ‚ÄúWorkflow: Using Git and GitHub as a team‚Äù section of the lab in your teams.\nRaise your hand if you have any questions about the workflow.\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lab-04.html#tips-for-working-on-a-team",
    "href": "slides/lab-04.html#tips-for-working-on-a-team",
    "title": "Lab 04",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other."
  },
  {
    "objectID": "slides/lab-04.html#sta-210-mid-semester-survey",
    "href": "slides/lab-04.html#sta-210-mid-semester-survey",
    "title": "Lab 04",
    "section": "STA 210 Mid Semester Survey",
    "text": "STA 210 Mid Semester Survey\nPlease take a few minutes to complete the STA 210 Mid Semester Survey.\n\nClick here to access the survey."
  },
  {
    "objectID": "slides/lab-04.html#resources-for-lab-04",
    "href": "slides/lab-04.html#resources-for-lab-04",
    "title": "Lab 04",
    "section": "Resources for Lab 04",
    "text": "Resources for Lab 04\n\nLecture notes:\n\nFeature engineering\nFeature engineering: Model workflow\n\nRecipes package function reference\nTidy Modeling in R - Chapter 8: Feature engineering with recipes\n\n\n\n\n\n\n\nNote\n\n\nIf you haven‚Äôt already, you will need to install the schrute R package for the lab. Run the code below in the console.\n\n\n\n\ninstall.packages(\"schrute\")\n\n\n\n\nüîó Week 06"
  },
  {
    "objectID": "slides/lab-01.html#computing-toolkit-for-reproducibility",
    "href": "slides/lab-01.html#computing-toolkit-for-reproducibility",
    "title": "Lab 01",
    "section": "Computing toolkit for reproducibility",
    "text": "Computing toolkit for reproducibility\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\n\n\nYou discussed R and RStudio in class this week, so we will focus on version control using Git and GitHub."
  },
  {
    "objectID": "slides/lab-01.html#git-and-github",
    "href": "slides/lab-01.html#git-and-github",
    "title": "Lab 01",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\n\nGit is a version control system ‚Äì like ‚ÄúTrack Changes‚Äù features from Microsoft Word.\nGitHub is the home for your Git-based projects on the internet (like DropBox but much better).\nWe will use GitHub as the home for course assignments and activities and for collaboration"
  },
  {
    "objectID": "slides/lab-01.html#what-is-versioning",
    "href": "slides/lab-01.html#what-is-versioning",
    "title": "Lab 01",
    "section": "What is versioning?",
    "text": "What is versioning?"
  },
  {
    "objectID": "slides/lab-01.html#what-is-versioning-1",
    "href": "slides/lab-01.html#what-is-versioning-1",
    "title": "Lab 01",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages"
  },
  {
    "objectID": "slides/lab-01.html#git-and-github-tips",
    "href": "slides/lab-01.html#git-and-github-tips",
    "title": "Lab 01",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\n\nThere are a lot of Git commands and very few people know them all. 99% of the time you will use git to commit, push, and pull.\nWe will be doing git things and interfacing with GitHub through RStudio\n\nIf you Google for help, skip any methods for using git through the command line.\n\nThere is a great resource for working with git and R: happygitwithr.com.\n\nSome of the content in there is beyond the scope of this course, but it‚Äôs a good place to look for help."
  },
  {
    "objectID": "slides/lab-01.html#do-you-have-the-lab-01-repo",
    "href": "slides/lab-01.html#do-you-have-the-lab-01-repo",
    "title": "Lab 01",
    "section": "Do you have the lab-01 repo?",
    "text": "Do you have the lab-01 repo?\n\nGo to the GitHub course organization: https://github.com/sta210-fa22\nYou should see a repo with the prefix lab-01- followed by your GitHub username\nIf you do not have this repo, please let your TA know!"
  },
  {
    "objectID": "slides/lab-01.html#demo",
    "href": "slides/lab-01.html#demo",
    "title": "Lab 01",
    "section": "Demo",
    "text": "Demo\nFollow along as your TA demonstrates the following:\n\nConfigure Git using SSH\nClone repo (using SSH) and start new project in RStudio\nRender document and produce PDF\nUpdate name in YAML\n\nRender, commit, push changes to GitHub\nSee updates in your GitHub repo"
  },
  {
    "objectID": "slides/lab-01.html#tips-for-working-on-lab",
    "href": "slides/lab-01.html#tips-for-working-on-lab",
    "title": "Lab 01",
    "section": "Tips for working on lab",
    "text": "Tips for working on lab\n\nYou do not have to finish the lab in class, they will always be due the following Monday (Thursday labs) or Tuesday (Friday labs). One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nDo not pressure each other to finish early (particularly once you start working on teams); use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-01.html#when-youre-done-with-lab",
    "href": "slides/lab-01.html#when-youre-done-with-lab",
    "title": "Lab 01",
    "section": "When you're done with lab",
    "text": "When you're done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\nSubmit the PDF of your responses to Gradescope\n\nYou can access Gradescope through Sakai or the course website\nLogin using your Duke NetID credentials\nSee Lab 01 instructions for details on submitting an assignment on Gradescope\n\n\n\n\n\n\n\nüîó Week 02"
  },
  {
    "objectID": "slides/08-mlr.html#computational-setup",
    "href": "slides/08-mlr.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modelingt\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # for laying out plots\nlibrary(GGally)      # for pairwise plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/08-mlr.html#house-prices-in-levittown",
    "href": "slides/08-mlr.html#house-prices-in-levittown",
    "title": "Multiple linear regression (MLR)",
    "section": "House prices in Levittown",
    "text": "House prices in Levittown\n\nThe data set contains the sales price and characteristics of 85 homes in Levittown, NY that sold between June 2010 and May 2011.\nLevittown was built right after WWII and was the first planned suburban community built using mass production techniques.\nThe article ‚ÄúLevittown, the prototypical American suburb ‚Äì a history of cities in 50 buildings, day 25‚Äù gives an overview of Levittown‚Äôs controversial history."
  },
  {
    "objectID": "slides/08-mlr.html#analysis-goals",
    "href": "slides/08-mlr.html#analysis-goals",
    "title": "Multiple linear regression (MLR)",
    "section": "Analysis goals",
    "text": "Analysis goals\n\nWe would like to use the characteristics of a house to understand variability in the sales price.\nTo do so, we will fit a multiple linear regression model.\nUsing our model, we can answers questions such as\n\n\nWhat is the relationship between the characteristics of a house in Levittown and its sale price?\nGiven its characteristics, what is the expected sale price of a house in Levittown?"
  },
  {
    "objectID": "slides/08-mlr.html#the-data",
    "href": "slides/08-mlr.html#the-data",
    "title": "Multiple linear regression (MLR)",
    "section": "The data",
    "text": "The data\n\nlevittown &lt;- read_csv(here::here(\"slides/data/homeprices.csv\"))\nlevittown\n\n# A tibble: 85 √ó 7\n   bedrooms bathrooms living_area lot_size year_built property_tax sale_price\n      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1        4       1          1380     6000       1948         8360     350000\n 2        4       2          1761     7400       1951         5754     360000\n 3        4       2          1564     6000       1948         8982     350000\n 4        5       2          2904     9898       1949        11664     375000\n 5        5       2.5        1942     7788       1948         8120     370000\n 6        4       2          1830     6000       1948         8197     335000\n 7        4       1          1585     6000       1948         6223     295000\n 8        4       1           941     6800       1951         2448     250000\n 9        4       1.5        1481     6000       1948         9087     299990\n10        3       2          1630     5998       1948         9430     375000\n# ‚Ä¶ with 75 more rows"
  },
  {
    "objectID": "slides/08-mlr.html#variables",
    "href": "slides/08-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nliving_area: Total living area of the house (in square feet)\nlot_size: Total area of the lot (in square feet)\nyear_built: Year the house was built\nproperty_tax: Annual property taxes (in USD)\n\n\nResponse: sale_price: Sales price (in USD)"
  },
  {
    "objectID": "slides/08-mlr.html#eda-response-variable",
    "href": "slides/08-mlr.html#eda-response-variable",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Response variable",
    "text": "EDA: Response variable"
  },
  {
    "objectID": "slides/08-mlr.html#eda-predictor-variables",
    "href": "slides/08-mlr.html#eda-predictor-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Predictor variables",
    "text": "EDA: Predictor variables"
  },
  {
    "objectID": "slides/08-mlr.html#eda-response-vs.-predictors",
    "href": "slides/08-mlr.html#eda-response-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Response vs.¬†Predictors",
    "text": "EDA: Response vs.¬†Predictors"
  },
  {
    "objectID": "slides/08-mlr.html#eda-all-variables",
    "href": "slides/08-mlr.html#eda-all-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: All variables",
    "text": "EDA: All variables\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggpairs(levittown) +\n  theme(\n    axis.text.y = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, size = 10),\n    strip.text.y = element_text(angle = 0, hjust = 0)\n    )"
  },
  {
    "objectID": "slides/08-mlr.html#single-vs.-multiple-predictors",
    "href": "slides/08-mlr.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs.¬†multiple predictors",
    "text": "Single vs.¬†multiple predictors\nSo far we‚Äôve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/08-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/08-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{sale_price}} ~ = & ~\n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{bedrooms} + \\hat{\\beta}_2 \\text{bathrooms} + \\hat{\\beta}_3 \\text{living_area} \\\\\n&+ \\hat{\\beta}_4 \\text{lot_size} + \\hat{\\beta}_5 \\text{year_built} + \\hat{\\beta}_6 \\text{property_tax}\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values sale_price follow a Normal distribution."
  },
  {
    "objectID": "slides/08-mlr.html#regression-model",
    "href": "slides/08-mlr.html#regression-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Regression Model",
    "text": "Regression Model\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/08-mlr.html#the-mlr-model",
    "href": "slides/08-mlr.html#the-mlr-model",
    "title": "Multiple linear regression (MLR)",
    "section": "The MLR model",
    "text": "The MLR model\nFor a given observation \\((x_{i1}, x_{i2} \\ldots, x_{ip}, y_i)\\)\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{8mm} \\epsilon_i \\sim N(0,\\sigma_\\epsilon^2)\n\\]"
  },
  {
    "objectID": "slides/08-mlr.html#prediction",
    "href": "slides/08-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/08-mlr.html#model-fit",
    "href": "slides/08-mlr.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7148818.957\n3820093.694\n-1.871\n0.065\n\n\nbedrooms\n-12291.011\n9346.727\n-1.315\n0.192\n\n\nbathrooms\n51699.236\n13094.170\n3.948\n0.000\n\n\nliving_area\n65.903\n15.979\n4.124\n0.000\n\n\nlot_size\n-0.897\n4.194\n-0.214\n0.831\n\n\nyear_built\n3760.898\n1962.504\n1.916\n0.059\n\n\nproperty_tax\n1.476\n2.832\n0.521\n0.604"
  },
  {
    "objectID": "slides/08-mlr.html#model-equation",
    "href": "slides/08-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{price}} = & -7148818.957 - 12291.011 \\times \\text{bedrooms}\\\\[5pt]  \n&+ 51699.236 \\times \\text{bathrooms}  + 65.903 \\times \\text{living area}\\\\[5pt]\n&- 0.897 \\times \\text{lot size} +  3760.898 \\times \\text{year built}\\\\[5pt]\n&+ 1.476 \\times \\text{property tax}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/08-mlr.html#interpreting-hatbeta_j",
    "href": "slides/08-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for living_area is 65.90. This means for each additional square foot of living area, we expect the sale price of a house in Levittown, NY to increase by $65.90, on average, holding all other predictor variables constant."
  },
  {
    "objectID": "slides/08-mlr.html#prediction-1",
    "href": "slides/08-mlr.html#prediction-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1,050 square feet of living area, 6,000 square foot lot size, built in 1948 with $6,306 in property taxes?\n\n\n\n-7148818.957 - 12291.011 * 3 + 51699.236 * 1 + \n  65.903 * 1050 - 0.897 * 6000 + 3760.898 * 1948 + \n  1.476 * 6306\n\n[1] 265360.4\n\n\n\nThe predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes is $265,360."
  },
  {
    "objectID": "slides/08-mlr.html#prediction-revisit",
    "href": "slides/08-mlr.html#prediction-revisit",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisit",
    "text": "Prediction, revisit\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_house &lt;- tibble(\n  bedrooms = 3, bathrooms = 1, \n  living_area = 1050, lot_size = 6000, \n  year_built = 1948, property_tax = 6306\n  )\n\npredict(price_fit, new_house)\n\n# A tibble: 1 √ó 1\n    .pred\n    &lt;dbl&gt;\n1 265360."
  },
  {
    "objectID": "slides/08-mlr.html#confidence-interval-for-hatmu_y",
    "href": "slides/08-mlr.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 95% confidence interval for the estimated mean price of houses in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     238482.     292239."
  },
  {
    "objectID": "slides/08-mlr.html#prediction-interval-for-haty",
    "href": "slides/08-mlr.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 95% prediction interval for an individual house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     167277.     363444."
  },
  {
    "objectID": "slides/08-mlr.html#cautions",
    "href": "slides/08-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/08-mlr.html#recap",
    "href": "slides/08-mlr.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\n\nIntroduced multiple linear regression\nInterpreted a coefficient \\(\\hat{\\beta}_j\\)\nUsed the model to calculate predicted values and the corresponding intervals\n\n\n\n\n\nüîó Week 04"
  },
  {
    "objectID": "slides/lab-03.html#reminders",
    "href": "slides/lab-03.html#reminders",
    "title": "Lab 03",
    "section": "Reminders",
    "text": "Reminders\nMake sure to do the following as you complete the assignment:\n\nWrite all narrative in complete sentences.\nUse informative axis titles and labels on all graphs.\nImplement version control in your reproducible workflow.\n\nThroughout the assignment periodically render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\nBenchmark: Push changes to GitHub at least three times as you work on the assignment."
  },
  {
    "objectID": "slides/lab-03.html#todays-lab",
    "href": "slides/lab-03.html#todays-lab",
    "title": "Lab 03",
    "section": "Today‚Äôs lab",
    "text": "Today‚Äôs lab\n\nFocused on mathematical inference for simple linear regression and checking model conditions.\nRemember to mark all pages in your Gradescope submission. The first page should be marked for the ‚ÄúWorkflow & formatting‚Äù and ‚ÄúReproducible report‚Äù sections.\nUse lectures and AEs from Week 04 as reference as you complete the lab.\n\n\n\n\n\nüîó Week 04"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 210: Regression Analysis",
    "section": "",
    "text": "Learn approaches for analyzing multivariate data sets, emphasizing analysis of variance, linear regression, and logistic regression. Learn techniques for checking the appropriateness of proposed models, such as residual analyses and case influence diagnostics, and techniques for selecting models. Gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory."
  },
  {
    "objectID": "index.html#teaching-assistants",
    "href": "index.html#teaching-assistants",
    "title": "STA 210: Regression Analysis",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\n\n\nClick here for schedule of office hours.\n\n\n\n\n\n\nName\nRole\nLab section\n\n\n\n\nSam Rosen\nHead TA\nLab 01: Tue 10:05 - 11:20am\n\n\nDonald Cayton\nTA\nLab 02: Tue 11:45am - 1pm\n\n\nLinxuan Wang\nTA\nLab 03: Thu 10:05 - 11:20am\n\n\nXiaojun Zheng\nTA\nLab 04: Thu 11:45am - 1pm\n\n\nBethany Astor\nTA\n\n\n\nJon Campbell\nTA\n\n\n\nAllison Li\nTA\n\n\n\nMitchelle Mojekwu\nTA\n\n\n\nBen Thorpe\nTA"
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge"
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge"
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\nTutorials most relevant to this course:\n\nThe Basics\nWork with Data\nVisualize Data\nTidy your Data\n\nClick here for full list of tutorials."
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences."
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help."
  },
  {
    "objectID": "support.html#lectures-and-labs",
    "href": "support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "support.html#ed-discussion",
    "href": "support.html#ed-discussion",
    "title": "Course support",
    "section": "Ed Discussion",
    "text": "Ed Discussion\nOutside of class and office hours, any general questions about course content or assignments should be posted on Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g.¬†illness, accommodations, etc.), you may email Professor Tackett at maria.tackett@duke.edu. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "support.html#academic-support",
    "href": "support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "support.html#mental-health-and-wellness",
    "href": "support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. Go to studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS): CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000 or students.duke.edu/wellness/caps\nTimelyCare (formerly known as Blue Devils Care): An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu"
  },
  {
    "objectID": "support.html#technology-accommodations",
    "href": "support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nTechnology Accommodations\nHighly aided students who have limited access to computers may request loaner laptops through the DukeLIFE Technology Assistance Program. Please note that supplies are limited.\nNote that we will be using Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "support.html#course-materials-costs",
    "href": "support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "support.html#assistance-with-zoom-or-sakai",
    "href": "support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "ae/ae-15-multilevel-models.html",
    "href": "ae/ae-15-multilevel-models.html",
    "title": "AE 15: Introduction to Multilevel Models",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-15- to get started.\nThe AE is due on GitHub by Thursday, December 01, 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)\nmusic &lt;- read_csv(\"data/musicdata.csv\") |&gt;\n  mutate(orchestra = if_else(instrument == \"orchestral instrument\", 1, 0), \n         large_ensemble = if_else(perform_type == \"Large Ensemble\", 1, 0))"
  },
  {
    "objectID": "ae/ae-15-multilevel-models.html#part-1-univariate-eda",
    "href": "ae/ae-15-multilevel-models.html#part-1-univariate-eda",
    "title": "AE 15: Introduction to Multilevel Models",
    "section": "Part 1: Univariate EDA",
    "text": "Part 1: Univariate EDA\n\nPlot the distribution of the response variable negative affect (na) using individual observations.\n\n\nggplot(data  = music, aes(x = na)) + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Individual scores\", \n       title =\"Distribution of Negative Affect Score\")\n\n\n\n\n\nPlot the distribution of the response variable using an aggregated value (or single observation) for each Level Two observation (musician).\n\n\nmusic |&gt;\n  group_by(id) |&gt;\n  summarise(mean_na = mean(na)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = mean_na)) + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Mean scores\", \n       title = \"Distribution of Mean Negative Affect Scores\",\n       subtitle = \"By Musician\")\n\n\n\n\n\nHow do the plots compare? How do they differ?\nWhat are some advantages of each plot? What are some disadvantages?"
  },
  {
    "objectID": "ae/ae-15-multilevel-models.html#part-2-bivariate-eda",
    "href": "ae/ae-15-multilevel-models.html#part-2-bivariate-eda",
    "title": "AE 15: Introduction to Multilevel Models",
    "section": "Part 2: Bivariate EDA",
    "text": "Part 2: Bivariate EDA\n\nMake a single scatterplot of the negative affect versus number of previous performances (previous) using the individual observations. Use geom_smooth() to add a linear regression line to the plot.\n\n\n# Add code\n\n\nMake a separate scatterplot of the negative affect versus number of previous performances (previous) faceted by musician (id). Use geom_smooth() to add a linear regression line to each plot.\n\n\n# Add code\n\n\nHow do the plots compare? How do they differ?\nWhat are some advantages of each plot? What are some disadvantages?"
  },
  {
    "objectID": "ae/ae-15-multilevel-models.html#part-3-level-one-models",
    "href": "ae/ae-15-multilevel-models.html#part-3-level-one-models",
    "title": "AE 15: Introduction to Multilevel Models",
    "section": "Part 3: Level One Models",
    "text": "Part 3: Level One Models\nCode to fit and display the Level One model for Observation 22 is below.\n\nid_22 &lt;- music |&gt;\n  filter(id == 22)\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(na ~ large_ensemble, data = id_22) |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.500\n1.96\n12.503\n0.000\n\n\nlarge_ensemble\n-7.833\n2.53\n-3.097\n0.009\n\n\n\n\n\nCode to fit the Level One model and get the fitted slope, intercept, and \\(R^2\\) values for all musicians is below.\n\n# set up tibble for fitted values \n\nmodel_stats &lt;- tibble(slopes = rep(0,37), \n               intercepts = rep(0,37), \n               r.squared = rep(0, 37))\n\n\nids &lt;- music |&gt; distinct(id) |&gt; pull()\n\n# counter to keep track of row number to store model_stats\n\ncount &lt;- 1\n\nfor(i in ids){\n  \nid_data &lt;- music |&gt;\n  filter(id == i)\n\nlevel_one_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(na ~ large_ensemble, data = id_data) \n\nlevel_one_model_tidy &lt;- tidy(level_one_model)\n\nmodel_stats$slopes[count] &lt;- level_one_model_tidy$estimate[2]\nmodel_stats$intercepts[count] &lt;- level_one_model_tidy$estimate[1]\nmodel_stats$r.squared[count] &lt;- glance(level_one_model)$r.squared\n\ncount = count + 1\n}"
  },
  {
    "objectID": "ae/ae-15-multilevel-models.html#part-4-level-two-models",
    "href": "ae/ae-15-multilevel-models.html#part-4-level-two-models",
    "title": "AE 15: Introduction to Multilevel Models",
    "section": "Part 4: Level Two Models",
    "text": "Part 4: Level Two Models\n\n# Make a Level Two data set\nmusicians &lt;- music |&gt;\n  distinct(id, orchestra) |&gt;\n  bind_cols(model_stats)\n\nModel for intercepts\n\na &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(intercepts ~ orchestra, data = musicians) \n\ntidy(a) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n16.283\n0.671\n24.249\n0.000\n\n\norchestra\n1.411\n0.991\n1.424\n0.163\n\n\n\n\n\nModel for slopes\n\nb &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(slopes ~ orchestra, data = musicians) \n\ntidy(b) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.771\n0.851\n-0.906\n0.373\n\n\norchestra\n-1.406\n1.203\n-1.168\n0.253"
  },
  {
    "objectID": "ae/ae-15-multilevel-models.html#part-5-distribution-of-r2-values",
    "href": "ae/ae-15-multilevel-models.html#part-5-distribution-of-r2-values",
    "title": "AE 15: Introduction to Multilevel Models",
    "section": "Part 5: Distribution of \\(R^2\\) values",
    "text": "Part 5: Distribution of \\(R^2\\) values\n\nggplot(data = model_stats, aes(x = r.squared)) +\n  geom_dotplot(fill = \"steelblue\",  color = \"black\") +\n  labs(x = \"R-squared values\", \n       title = \"Distribution of R-squared of Level One models\")"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html",
    "href": "ae/ae-04-sim-testing.html",
    "title": "AE 04: Simulation-based hypothesis testing",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-04-bootstrap- to get started.\nThe AE is due on GitHub by Saturday, September 17 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#data",
    "href": "ae/ae-04-sim-testing.html#data",
    "title": "AE 04: Simulation-based hypothesis testing",
    "section": "Data",
    "text": "Data\nThe data are on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. It was originally scraped from Zillow, and can be found in the duke_forest data set in the openintro R package.\n\nglimpse(duke_forest)\n\nRows: 98\nColumns: 13\n$ address    &lt;chr&gt; \"1 Learned Pl, Durham, NC 27705\", \"1616 Pinecrest Rd, Durha‚Ä¶\n$ price      &lt;dbl&gt; 1520000, 1030000, 420000, 680000, 428500, 456000, 1270000, ‚Ä¶\n$ bed        &lt;dbl&gt; 3, 5, 2, 4, 4, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 5, 3, 4, 4, 3,‚Ä¶\n$ bath       &lt;dbl&gt; 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 3.0,‚Ä¶\n$ area       &lt;dbl&gt; 6040, 4475, 1745, 2091, 1772, 1950, 3909, 2841, 3924, 2173,‚Ä¶\n$ type       &lt;chr&gt; \"Single Family\", \"Single Family\", \"Single Family\", \"Single ‚Ä¶\n$ year_built &lt;dbl&gt; 1972, 1969, 1959, 1961, 2020, 2014, 1968, 1973, 1972, 1964,‚Ä¶\n$ heating    &lt;chr&gt; \"Other, Gas\", \"Forced air, Gas\", \"Forced air, Gas\", \"Heat p‚Ä¶\n$ cooling    &lt;fct&gt; central, central, central, central, central, central, centr‚Ä¶\n$ parking    &lt;chr&gt; \"0 spaces\", \"Carport, Covered\", \"Garage - Attached, Covered‚Ä¶\n$ lot        &lt;dbl&gt; 0.97, 1.38, 0.51, 0.84, 0.16, 0.45, 0.94, 0.79, 0.53, 0.73,‚Ä¶\n$ hoa        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ url        &lt;chr&gt; \"https://www.zillow.com/homedetails/1-Learned-Pl-Durham-NC-‚Ä¶"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#exploratory-data-analysis",
    "href": "ae/ae-04-sim-testing.html#exploratory-data-analysis",
    "title": "AE 04: Simulation-based hypothesis testing",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#model",
    "href": "ae/ae-04-sim-testing.html#model",
    "title": "AE 04: Simulation-based hypothesis testing",
    "section": "Model",
    "text": "Model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#hypothesis-test",
    "href": "ae/ae-04-sim-testing.html#hypothesis-test",
    "title": "AE 04: Simulation-based hypothesis testing",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\n\n\n\n\n\nTip\n\n\n\nFor code chunks with fill-in-the-blank code, change code chunk option to #| eval: true once you‚Äôve filled in the code.\n\n\n\nState the null and alternative hypotheses\n[Add hypotheses in mathematical notation]\n\n\nGenerate null distribution using permutation\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(09142022)\n\nnull_dist &lt;- _____ |&gt;\n  specify(______) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = _____, type = \"permute\") |&gt;\n  fit()\n\n\n\nVisualize distribution\n\n# Code for histogram of null distribution\n\n\n\nCalculate the p-value.\n\n# get observed fit \nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\n# calculate p-value\nget_p_value(\n  ____,\n  obs_stat = ____,\n  direction = \"two-sided\"\n)\n\n\nWhat does the warning message mean?\n\n\n\nState conclusion\n[Write your conclusion in the context of the data.]\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-04- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-09-model-comparison.html",
    "href": "ae/ae-09-model-comparison.html",
    "title": "AE 09: Model comparison",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-09- to get started.\nThe AE is due on GitHub by Saturday, October 22 at 11:59pm."
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#packages",
    "href": "ae/ae-09-model-comparison.html#packages",
    "title": "AE 09: Model comparison",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(viridis)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#load-data",
    "href": "ae/ae-09-model-comparison.html#load-data",
    "title": "AE 09: Model comparison",
    "section": "Load data",
    "text": "Load data\n\ntips &lt;- read_csv(\"data/tip-data.csv\") |&gt;\n  filter(!is.na(Party))\n\n\n# relevel factors\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#exploratory-data-analysis",
    "href": "ae/ae-09-model-comparison.html#exploratory-data-analysis",
    "title": "AE 09: Model comparison",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nResponse variable\n\nggplot(tips, aes(x = Tip)) +\n  geom_histogram(binwidth = 1) +\n  labs(title = \"Distribution of tips\")\n\n\n\n\n\n\nPredictor variables\n\np1 &lt;- ggplot(tips, aes(x = Party)) +\n  geom_histogram(binwidth = 1) +\n  labs(title = \"Number of people in party\")\n\np2 &lt;- ggplot(tips, aes(x = Meal, fill = Meal)) +\n  geom_bar() +\n  labs(title = \"Meal type\") +\n  scale_fill_viridis_d()\n\np3 &lt;- ggplot(tips, aes(x = Age, fill = Age)) +\n  geom_bar() +\n  labs(title = \"Age of payer\") +\n  scale_fill_viridis_d(option = \"E\", end = 0.8)\n\np1 + (p2 / p3)\n\n\n\n\n\n\nResponse vs.¬†predictors\n\np4 &lt;- ggplot(tips, aes(x = Party, y = Tip)) +\n  geom_point(color = \"#5B888C\")\n\np5 &lt;- ggplot(tips, aes(x = Meal, y = Tip, fill = Meal)) +\n  geom_boxplot(show.legend = FALSE) +\n  scale_fill_viridis_d()\n\np6 &lt;- ggplot(tips, aes(x = Age, y = Tip, fill = Age)) +\n  geom_boxplot(show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"E\", end = 0.8)\n\np4 + p5 + p6"
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#models",
    "href": "ae/ae-09-model-comparison.html#models",
    "title": "AE 09: Model comparison",
    "section": "Models",
    "text": "Models\n\nModel 1: Tips vs.¬†Age & Party\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n\n\nModel 2: Tips vs.¬†Age, Party, Meal & Day\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party +  Age + Meal +  Day, \n      data = tips)\n\ntidy(tip_fit_2) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.354\n0.968\n-0.365\n0.715\n\n\nParty\n1.792\n0.126\n14.179\n0.000\n\n\nAgeMiddle\n0.506\n0.427\n1.185\n0.238\n\n\nAgeSenCit\n1.017\n0.494\n2.058\n0.041\n\n\nMealDinner\n0.636\n0.457\n1.390\n0.167\n\n\nMealLate Night\n-0.729\n0.754\n-0.967\n0.335\n\n\nDaySaturday\n0.812\n0.783\n1.038\n0.301\n\n\nDaySunday\n0.097\n0.877\n0.111\n0.912\n\n\nDayThursday\n0.069\n0.897\n0.077\n0.939\n\n\nDayTuesday\n0.414\n0.670\n0.618\n0.537\n\n\nDayWednesday\n0.936\n1.098\n0.853\n0.395\n\n\n\n\n\n\n\n\n\n\n\nWhy did we not use the full recipe() workflow to fit Model 1 or Model 2?"
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#r2-and-adjusted-r2",
    "href": "ae/ae-09-model-comparison.html#r2-and-adjusted-r2",
    "title": "AE 09: Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\nFill in the code below to calculate \\(R^2\\) and Adjusted \\(R^2\\) for Model 1. Put eval: true once the code is updated.\n\nglance(______) |&gt;\n  select(r.squared, adj.r.squared)\n\nCalculate \\(R^2\\) and Adjusted \\(R^2\\) for Model 2.\n\n# r-sq and adj. r-sq for model 2\n\n\n\n\n\n\n\nWe would like to choose the model that better fits the data.\n\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?"
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#aic-bic",
    "href": "ae/ae-09-model-comparison.html#aic-bic",
    "title": "AE 09: Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nUse the glance() function to calculate AIC and BIC for Models 1 and 2.\n\n## AIC and BIC for Model 1\n\n\n## AIC and BIC for Model 2\n\n\n\n\n\n\n\nWe would like to choose the model that better fits the data.\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "ae/ae-09-model-comparison.html#evaluating-analysis-process",
    "href": "ae/ae-09-model-comparison.html#evaluating-analysis-process",
    "title": "AE 09: Model comparison",
    "section": "Evaluating analysis process",
    "text": "Evaluating analysis process\n\n\n\n\n\n\nWe fit and evaluated these models using the entire data set. What is a limitation to using the entire data set to fit and evaluate models?"
  },
  {
    "objectID": "ae/ae-06-prediction.html",
    "href": "ae/ae-06-prediction.html",
    "title": "AE 06: Prediction for MLR",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-06- to get started.\nThe AE is due on GitHub by Thursday, September 29 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nThe data set contains the sales price and characteristics of 85 homes in Levittown, NY that sold between June 2010 and May 2011. Levittown was built right after WWII and was the first planned suburban community built using mass production techniques.\nlevittown &lt;- read_csv(\"data/homeprices.csv\")\nThe variables used in this analysis are\nThe goal of the analysis is to use the characteristics of a house to understand variability in the sales price."
  },
  {
    "objectID": "ae/ae-06-prediction.html#linear-model",
    "href": "ae/ae-06-prediction.html#linear-model",
    "title": "AE 06: Prediction for MLR",
    "section": "Linear model",
    "text": "Linear model\n\nprice_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(sale_price ~ bedrooms + bathrooms + living_area + lot_size +\n        year_built + property_tax, data = levittown)\n\ntidy(price_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7148818.957\n3820093.694\n-1.871\n0.065\n\n\nbedrooms\n-12291.011\n9346.727\n-1.315\n0.192\n\n\nbathrooms\n51699.236\n13094.170\n3.948\n0.000\n\n\nliving_area\n65.903\n15.979\n4.124\n0.000\n\n\nlot_size\n-0.897\n4.194\n-0.214\n0.831\n\n\nyear_built\n3760.898\n1962.504\n1.916\n0.059\n\n\nproperty_tax\n1.476\n2.832\n0.521\n0.604"
  },
  {
    "objectID": "ae/ae-06-prediction.html#prediction",
    "href": "ae/ae-06-prediction.html#prediction",
    "title": "AE 06: Prediction for MLR",
    "section": "Prediction",
    "text": "Prediction\nWhat is the predicted sale price for an individual house in Levittown, NY with 4 bedrooms, 2 bathrooms, 1,800 square feet of living area, 6,000 square foot lot size, built in 1947 with $7,403 in property taxes?\nReport the predicted value and appropriate interval.\n\n\n\n\n\n\nNote\n\n\n\nFill in the code, then make #| eval: true before rendering the document.\n\n\n\n# create tibble for new observation \nnew_house &lt;- tibble(\n  bedrooms = ____, \n  bathrooms = ____, \n  _____\n  )\n\n# prediction + interval\nprediction(_________)\n\n\nInterpret the interval in the context of the data.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-06- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-03-bootstrap.html",
    "href": "ae/ae-03-bootstrap.html",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-03-bootstrap- to get started.\nThe AE is due on GitHub by Thursday, September 15 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#data",
    "href": "ae/ae-03-bootstrap.html#data",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Data",
    "text": "Data\nThe data are on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. It was originally scraped from Zillow, and can be found in the duke_forest data set in the openintro R package.\n\nglimpse(duke_forest)\n\nRows: 98\nColumns: 13\n$ address    &lt;chr&gt; \"1 Learned Pl, Durham, NC 27705\", \"1616 Pinecrest Rd, Durha‚Ä¶\n$ price      &lt;dbl&gt; 1520000, 1030000, 420000, 680000, 428500, 456000, 1270000, ‚Ä¶\n$ bed        &lt;dbl&gt; 3, 5, 2, 4, 4, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 5, 3, 4, 4, 3,‚Ä¶\n$ bath       &lt;dbl&gt; 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 3.0,‚Ä¶\n$ area       &lt;dbl&gt; 6040, 4475, 1745, 2091, 1772, 1950, 3909, 2841, 3924, 2173,‚Ä¶\n$ type       &lt;chr&gt; \"Single Family\", \"Single Family\", \"Single Family\", \"Single ‚Ä¶\n$ year_built &lt;dbl&gt; 1972, 1969, 1959, 1961, 2020, 2014, 1968, 1973, 1972, 1964,‚Ä¶\n$ heating    &lt;chr&gt; \"Other, Gas\", \"Forced air, Gas\", \"Forced air, Gas\", \"Heat p‚Ä¶\n$ cooling    &lt;fct&gt; central, central, central, central, central, central, centr‚Ä¶\n$ parking    &lt;chr&gt; \"0 spaces\", \"Carport, Covered\", \"Garage - Attached, Covered‚Ä¶\n$ lot        &lt;dbl&gt; 0.97, 1.38, 0.51, 0.84, 0.16, 0.45, 0.94, 0.79, 0.53, 0.73,‚Ä¶\n$ hoa        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ url        &lt;chr&gt; \"https://www.zillow.com/homedetails/1-Learned-Pl-Durham-NC-‚Ä¶"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#exploratory-data-analysis",
    "href": "ae/ae-03-bootstrap.html#exploratory-data-analysis",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#model",
    "href": "ae/ae-03-bootstrap.html#model",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Model",
    "text": "Model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#bootstrap-confidence-interval",
    "href": "ae/ae-03-bootstrap.html#bootstrap-confidence-interval",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Bootstrap confidence interval",
    "text": "Bootstrap confidence interval\n\n1. Calculate the observed fit (slope)\n\nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159.\n\n\n\n\n2 Take n bootstrap samples and fit models to each one.\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(091222)\n\nboot_fits &lt;- ______ |&gt;\n  specify(______) |&gt;\n  generate(reps = ____, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n\nWhy do we set a seed before taking the bootstrap samples?\nMake a histogram of the bootstrap samples to visualize the bootstrap distribution.\n\n# Code for histogram\n\n\n\n\n3 Compute the 95% confidence interval as the middle 95% of the bootstrap distribution\nFill in the code, then set eval: true .\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = _____, \n  level = ____,\n  type = \"percentile\"\n)"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#changing-confidence-level",
    "href": "ae/ae-03-bootstrap.html#changing-confidence-level",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nModify the code from Step 3 to create a 90% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\n\nModify the code from Step 3 to create a 99% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\nWhich confidence level produces the most accurate confidence interval (90%, 95%, 99%)? Explain\nWhich confidence level produces the most precise confidence interval (90%, 95%, 99%)? Explain\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-03- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-13-logistic-intro.html",
    "href": "ae/ae-13-logistic-intro.html",
    "title": "AE 13: Logistic regression introduction",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-13- to get started.\nThe AE is due on GitHub by Saturday, November 05 , 11:59pm."
  },
  {
    "objectID": "ae/ae-13-logistic-intro.html#packages",
    "href": "ae/ae-13-logistic-intro.html#packages",
    "title": "AE 13: Logistic regression introduction",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(\"data/framingham.csv\") |&gt;\n  select(totChol, TenYearCHD) |&gt;\n  drop_na() |&gt;\n  mutate(high_risk = as.factor(TenYearCHD)) |&gt;\n  select(totChol, high_risk)"
  },
  {
    "objectID": "ae/ae-13-logistic-intro.html#linear-regression-vs.-logistic-regression",
    "href": "ae/ae-13-logistic-intro.html#linear-regression-vs.-logistic-regression",
    "title": "AE 13: Logistic regression introduction",
    "section": "Linear regression vs.¬†logistic regression",
    "text": "Linear regression vs.¬†logistic regression\nState whether a linear regression model or logistic regression model is more appropriate for each scenario:\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie‚Äôs total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year."
  },
  {
    "objectID": "ae/ae-13-logistic-intro.html#heart-disease",
    "href": "ae/ae-13-logistic-intro.html#heart-disease",
    "title": "AE 13: Logistic regression introduction",
    "section": "Heart disease",
    "text": "Heart disease\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\nheart_disease |&gt;\n  count(high_risk)\n\n# A tibble: 2 √ó 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635\n\n\n\n\nCalculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease?\n\n\n\nLogistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet \\(\\pi\\) be the probability an adult is high risk of heart disease. The statistical model is\n\\[\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\\]\n\nheart_disease_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits.\n\n\n\nCalculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease?\nWhat are the odds they are high risk for heart disease?\nWhat is the probability they are high risk for heart disease? Use the odds to calculate your answer.\n\n\n\nComparing observations\nSuppose a person‚Äôs cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease to change?\nHow do you expect the odds that this person is high risk for heart disease to change?"
  },
  {
    "objectID": "ae/ae-14-model-compare.html",
    "href": "ae/ae-14-model-compare.html",
    "title": "AE 14: Logistic regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-14- to get started.\nThe AE is due on GitHub by Thursday, November 17 , 11:59pm."
  },
  {
    "objectID": "ae/ae-14-model-compare.html#packages",
    "href": "ae/ae-14-model-compare.html#packages",
    "title": "AE 14: Logistic regression",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-14-model-compare.html#data",
    "href": "ae/ae-14-model-compare.html#data",
    "title": "AE 14: Logistic regression",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a data set of 25,000 randomly sampled flights that departed one of three NYC airports (JFK, LGA, EWR) in 2013.\n\nflight_data &lt;- read_csv(\"data/flight-data.csv\")\n\nThe goal of this analysis is to fit a model that could be used to predict whether a flight will arrive on time (up to 30 minutes past the scheduled arrival time) or late (more than 30 minutes past the scheduled arrival time).\n\nConvert arr_delay to factor with levels \"late\" (first level) and \"on_time\" (second level). This variable is our outcome and it indicates whether the flight‚Äôs arrival was more than 30 minutes.\n\n\n# add code"
  },
  {
    "objectID": "ae/ae-14-model-compare.html#modeling-prep",
    "href": "ae/ae-14-model-compare.html#modeling-prep",
    "title": "AE 14: Logistic regression",
    "section": "Modeling prep",
    "text": "Modeling prep\n\nSplit the data into testing (75%) and training (25%), and save each subset.\n\n\nset.seed(222)\n\n# add code\n\n\nSpecify a logistic regression model that uses the \"glm\" engine.\n\n\n# add code\n\nNext, we‚Äôll create two recipes and workflows and compare them to each other."
  },
  {
    "objectID": "ae/ae-14-model-compare.html#model-1-everything-and-the-kitchen-sink",
    "href": "ae/ae-14-model-compare.html#model-1-everything-and-the-kitchen-sink",
    "title": "AE 14: Logistic regression",
    "section": "Model 1: Everything and the kitchen sink",
    "text": "Model 1: Everything and the kitchen sink\n\nDefine a recipe that predicts arr_delay using all variables except for flight and time_hour, which, in combination, can be used to identify a flight, and dest. Also make sure this recipe handles dummy coding as well as issues that can arise due to having categorical variables with some levels apparent in the training set but not in the testing set. Call this recipe flights_rec1.\n\n\n# add code\n\n\nCreate a workflow that uses flights_rec1 and the model you specified.\n\n\n# add code\n\n\nFit the this model to the training data using your workflow and display a tidy summary of the model fit.\n\n\n# add code\n\n\nPredict arr_delay for the testing data using this model.\n\n\n# add code\n\n\nPlot the ROC curve and find the area under the curve. Comment on how well you think this model has done for predicting arrival delay.\n\n\n# add code"
  },
  {
    "objectID": "ae/ae-14-model-compare.html#model-2-lets-be-a-bit-more-thoughtful",
    "href": "ae/ae-14-model-compare.html#model-2-lets-be-a-bit-more-thoughtful",
    "title": "AE 14: Logistic regression",
    "section": "Model 2: Let‚Äôs be a bit more thoughtful",
    "text": "Model 2: Let‚Äôs be a bit more thoughtful\n\nDefine a new recipe, flights_rec2, that, in addition to what was done in flights_rec1, adds features for day of week and month based on date and also adds indicators for all US holidays (also based on date). A list of these holidays can be found in timeDate::listHolidays(\"US\"). Once these features are added, date should be removed from the data. Then, create a new workflow, fit the same model (logistic regression) to the training data, and do predictions on the testing data. Finally, draw another ROC curve and find the area under the curve.\n\n\n# add code"
  },
  {
    "objectID": "ae/ae-14-model-compare.html#putting-it-altogether",
    "href": "ae/ae-14-model-compare.html#putting-it-altogether",
    "title": "AE 14: Logistic regression",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\nCreate an ROC curve that plots both models, in different colors, and adds a legend indicating which model is which.\n\n\n# add code\n\n\nCompare the predictive performance of this new model to the previous one. Based on the ROC curves and area under the curve statistic, which model does better?"
  },
  {
    "objectID": "ae/ae-14-model-compare.html#acknowledgement",
    "href": "ae/ae-14-model-compare.html#acknowledgement",
    "title": "AE 14: Logistic regression",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by tidymodels.org/start/recipes and adapted from sta210-s22.github.io/website/ae/ae-10-flight-delays."
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html",
    "href": "ae/ae-07-exam-01-review.html",
    "title": "AE 07: Exam 01 review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-07- to get started.\nThe AE is due on GitHub by Saturday, October 01 at 11:59pm."
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#packages",
    "href": "ae/ae-07-exam-01-review.html#packages",
    "title": "AE 07: Exam 01 review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#restaurant-tips",
    "href": "ae/ae-07-exam-01-review.html#restaurant-tips",
    "title": "AE 07: Exam 01 review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St.¬†Olaf who worked at a local restaurant.1\nThe variables we‚Äôll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\nAlcohol: whether alcohol was purchased with meal\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#exploratory-analysis",
    "href": "ae/ae-07-exam-01-review.html#exploratory-analysis",
    "title": "AE 07: Exam 01 review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#modeling",
    "href": "ae/ae-07-exam-01-review.html#modeling",
    "title": "AE 07: Exam 01 review",
    "section": "Modeling",
    "text": "Modeling\nLet‚Äôs start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and neatly display the results with 3 digits and the 95% confidence interval for the coefficients.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#inference",
    "href": "ae/ae-07-exam-01-review.html#inference",
    "title": "AE 07: Exam 01 review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we‚Äôll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips |&gt;\n  specify(Tip ~ Party) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation with 100 reps. State the hypotheses and the significance level you‚Äôre using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\nset.seed(1234)\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models. You can reference output from previous exercises and/or write new code as needed.\nCheck the relevant conditions for Exercise 10. Are there any violations in conditions that make you reconsider your inferential findings? You can reference previous graphs / conditions and add any new code as needed.\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you‚Äôre asked to construct a confidence and a prediction interval for your finding in the previous exercise. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#multiple-linear-regression",
    "href": "ae/ae-07-exam-01-review.html#multiple-linear-regression",
    "title": "AE 07: Exam 01 review",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nMake a plot to visualize the relationship between Party and Tip with the points colored by Alcohol. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and three digits.\n\n\n# add your code here\n\n\nInterpret the coefficients of Party and Alcohol.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-07- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-07-exam-01-review.html#footnotes",
    "href": "ae/ae-07-exam-01-review.html#footnotes",
    "title": "AE 07: Exam 01 review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. ‚ÄúThe Effects of Credit Cards on Tipping.‚Äù Project for Statistics 212-Statistics for the Sciences, St.¬†Olaf College.‚Ü©Ô∏é"
  },
  {
    "objectID": "prepare/week-15.html",
    "href": "prepare/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important\n\n\n\n\nExam 02: Mon, Dec 05 (evening) - Thu, Dec 08 at 12pm (noon)\nTeam Feedback #2 due Tue, Dec 06, 11:59pm\nNo class Wed, Dec 07\n\nOnline exam office hours (Zoom link in Sakai)\n\nStatistics experience due Fri, Dec 09, 11:59pm\nProject written report due Fri, Dec 09, 11:59pm (accepted until Sun, Dec 11, 11:59pm)\n\n\n\n\nPrepare\nNo readings this week.\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Exam 02 Review\nDec 05\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 16: Exam 02 Review\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nüìù Exam 02\nMon, Dec 05 evening - Thu, Dec 08, 12pm (noon)\n\n\nüìù Project written report\ndue Fri, Dec 09, 11:59pm (accepted until Sun, Dec 11, 11:59pm)\n\n\nüìù Statistics experience\ndue Fri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-03.html",
    "href": "prepare/week-03.html",
    "title": "Week 03",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nAE 03: due Thu, Sep 15 at 11:59pm (on GitHub)\nAE 04: due Sat, Sep 17 at 11:59pm (on GitHub)\nLab 02:\n\ndue Mon, Sep 19, 11:59pm (Thu labs)\ndue Tue, Sep 20, 11:59pm (Fri labs)\n\nHW 01: due Wed, Sep 21, 11:59pm\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ IMS, Sec 24.2: Randomization test for the slope\nSep 12\n\n\nüìñ IMS, Sec 24.3: Bootstrap confidence interval for the slope\nüìñ IMS, Sec 24.4: Mathematical model for testing the slope\nSep 14\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª SLR: Simulation-based inference (bootstrap confidence intervals)\nSep 12\n\n\nüíª SLR: Simulation-based inference (hypothesis testing)\nSep 14\n\n\nüíª Lab 02\nSep 15 & 16\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 03: Bootstrap confidence intervals\n\n\nüìã AE 04: Simulation-based hypothesis testing\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nüìù Lab 02\nMon, Sep 19, 11:59pm (Thu labs)\nTue, Sep 20, 11:59pm (Fri labs)\n\n\nüìù HW 01\nWed, Sep 21, 11:59pm\n\n\nüìù Statistics experience\nFri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-16.html",
    "href": "prepare/week-16.html",
    "title": "Week 16",
    "section": "",
    "text": "Prepare\nNo reading assignments.\n\n\nParticipate\nNo lecture slides.\n\n\nPractice\nNo AEs.\n\n\nPerform\n\n\n\n\n\n\n\nüìù Project: Video presentation + slides\ndue Wed, Dec 14, 11:59pm\n\n\nüìù Project: Organized GitHub repo\ndue Wed, Dec 14, 11:59pm\n\n\nüìù Project: Presentation comments\ndue Fri, Dec 16, 11:59pm"
  },
  {
    "objectID": "prepare/week-06.html",
    "href": "prepare/week-06.html",
    "title": "Week 06",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nLab 04:\n\ndue Thu, Oct 13, 11:59pm (Thu lab)\ndue Fri, Oct 14, 11:59pm (Fri lab)\n\nHW 02:\n\ndue Wed, Oct 19, 11:59pm\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ Tidy Modeling in R - Chapter 8: Feature engineering with recipes\nOct 05\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Feature engineering\nOct 03\n\n\nüíª Feature engineering: Model workflow\nOct 05\n\n\nüíª Lab 04\nOct 06 & 07\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 08: Feature engineering\n\n\n\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nüìù Lab 04\nThu, Oct 13, 11:59pm (Thu lab)\nFri, Oct 14, 11:59pm (Fri lab)"
  },
  {
    "objectID": "prepare/week-13.html",
    "href": "prepare/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important\n\n\n\n\nNo office hours Nov 21 - 25.\nNo class Nov 21 or Nov 23.\nNo labs Nov 24 - 25.\n\nüçÅ Have a good Thanksgiving break! üçÅ\n\n\n\nPrepare\nNo readings.\n\n\nParticipate\nNo lecture slides.\n\n\nPractice\nNo application exercises.\n\n\nPerform\n\n\n\n\n\n\n\nüìù Project written report\ndue Fri, Dec 09, 11:59pm (accepted until Sun, Dec 11, 11:59pm)\n\n\nüìù Statistics experience\ndue Fri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-11.html",
    "href": "prepare/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\n\nLab 07\n\ndue Mon, Nov 14, 11:59pm (Thu labs)\ndue Tue, Nov 15, 11:59pm (Fri labs)\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ IMS, Chap 9: Logistic Regression\nNov 07 & Nov 09\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Logistic regression: Odds ratios\nNov 07\n\n\nüíª Logistic regression: Prediction + classification\nNov 09\n\n\n\n\n\nPractice\nNo AEs this week.\n\n\nPerform\n\n\n\n\n\n\n\nüìù Lab 07\n\ndue Mon, Nov 14, 11:59pm (Thu labs)\ndue Tue, Nov 15, 11:59pm (Fri labs)\n\n\n\nüìù Statistics Experience\ndue Fri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-04.html",
    "href": "prepare/week-04.html",
    "title": "Week 04",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nAE 05: due Sat, Sep 24, 11:59pm\nHW 01: due Wed, Sep 21, 11:59pm\nLab 03:\n\ndue Mon, Sep 26, 11:59pm (Thu labs)\ndue Tue, Sep 27, 11:59pm (Fri labs)\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ IMS, Sec 24.4: Mathematical model for testing the slope\nüìñ IMS, Sec 24.5: Mathematical model, interval for the slope\nüìñ IMS, Sec 24.6: Checking model conditions\nüìñ IMS, Sec 24.7: Chapter review\nSep 19\n\n\nüìñ IMS, Sec 8.2: Many predictors in a model\nSep 21\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª SLR: Mathematical models for inference\nSep 19\n\n\nüíª SLR: Model conditions\nSep 21\n\n\nüíª Multiple linear regression\nSep 21\n\n\nüíª Lab 03\nSep 22 & 23\n\n\n\n\n\nPractice\n\n\n\n\n\n\nAE 05: Multiple linear regression\n\n\n\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nüìù HW 01\nWed, Sep 21, 11:59pm\n\n\nüìù Lab 03\nMon, Sep 26, 11:59pm (Thu labs)\nTue, Sep 27, 11:59pm (Fri labs)\n\n\nüìù Statistics experience\nFri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-08.html",
    "href": "prepare/week-08.html",
    "title": "Week 08",
    "section": "",
    "text": "Important\n\n\n\n\nAE 09 due Sat, Oct 22, 11:59pm\nLab 05:\n\ndue Mon, Oct 24, 11:59pm (Thu lab)\ndue Tue, Oct 25 11:59pm (Fri lab)\n\n\n\n\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ Tidy Modeling in R - Chapter 8: Feature engineering with recipes\nOct 19\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Variable transformations: Log-transformed predictor\nOct 17\n\n\nüíª Model comparison\nOct 19\n\n\nüíª Lab 05\nOct 20 - 21\n\n\n\n\n\nPractice\nüìã AE 09: Model comparison\n\n\nPerform\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nLab 05\n\ndue Mon, Oct 24, 11:59pm (Thu lab)\ndue Tue, Oct 25, 11:59pm (Fri lab)"
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02: Multiple linear regression",
    "section": "",
    "text": "In this analysis you will use multiple linear regression to analyze relationships between variables in three different scenarios."
  },
  {
    "objectID": "hw/hw-02.html#exercise-1",
    "href": "hw/hw-02.html#exercise-1",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nOur first goal is to fit a model predicting body mass (which is more difficult to measure) from bill length, bill depth, flipper length, species, and sex.\nWe will start by preparing the data.\n\nUse the drop_na() function to remove any observations from the penguins data frame that has missing values. Your resulting data frame should have 333 observations.\nSplit the data into training (75%) and testing (25%) sets. Use a seed of 123."
  },
  {
    "objectID": "hw/hw-02.html#exercise-2",
    "href": "hw/hw-02.html#exercise-2",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nUse the training data to fit a model predicting body mass (which is more difficult to measure) from the other variables listed above. Only include main effects, i.e., no interaction terms, in this model. Fit the model in a way such that the intercept has a meaningful interpretation.\nNeatly display the model using 3 digits.\nWrite estimated regression equation. Use the variable names in your equation."
  },
  {
    "objectID": "hw/hw-02.html#exercise-3",
    "href": "hw/hw-02.html#exercise-3",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nInterpret each slope coefficient in the context of the data.\nInterpret the intercept in the context of the data."
  },
  {
    "objectID": "hw/hw-02.html#exercise-4",
    "href": "hw/hw-02.html#exercise-4",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nCalculate the residual for a male Adelie penguin that weighs 3750 grams with the following body measurements: bill_length_mm = 39.1, bill_depth_mm = 18.7, flipper_length_mm = 181. Does the model overpredict or underpredict this penguin‚Äôs weight?\nCalculate \\(R^2\\) of this model based on the training data and interpret this value in context of the data and the model."
  },
  {
    "objectID": "hw/hw-02.html#exercise-5",
    "href": "hw/hw-02.html#exercise-5",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext, we will focus on a model using bill length and species to predict body mass.\nUse the training data to make a visualization of the relationship between bill length and body mass by species. Does the visualization give evidence of a potential interaction term? Briefly explain your response."
  },
  {
    "objectID": "hw/hw-02.html#exercise-6",
    "href": "hw/hw-02.html#exercise-6",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the training data to fit a model using bill length, species, and the interaction between the two variables to predict body mass. Fit the model in a way such that the intercept has a meaningful interpretation.\nNeatly display the model using 3 digits.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the step_interact() to add interactions in the recipe."
  },
  {
    "objectID": "hw/hw-02.html#exercise-7",
    "href": "hw/hw-02.html#exercise-7",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nUse the test data to compare the model fit in Exercise 2 to the model fit in Exercise 6. Which model is the ‚Äúbest‚Äù for predicting body mass? Briefly explain your response showing the code and output to support your choice."
  },
  {
    "objectID": "hw/hw-02.html#exercise-8",
    "href": "hw/hw-02.html#exercise-8",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nInterpret the coefficient of Age (0.072) in the context of the analysis.\nInterpret the coefficient of Place of residence in the context of the analysis."
  },
  {
    "objectID": "hw/hw-02.html#exercise-9",
    "href": "hw/hw-02.html#exercise-9",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nThe model includes an interaction between Place of residence and Emotionality (capturing differential tendencies in to worry and be anxious).\n\nWhat does the coefficient for the interaction (0.101) mean in the context of the data?\nInterpret the estimated effect of Emotionality for a person who lives in the US/Canada.\nInterpret the estimated effect of Emotionality for a person who lives in Europe."
  },
  {
    "objectID": "hw/hw-02.html#exercise-10",
    "href": "hw/hw-02.html#exercise-10",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nData on countries‚Äô Gross Domestic Product (GDP) and percentage of urban population was collected and made available by The World Bank in 2020. A description of the variables as defined by The World Bank are provided below.\n\nGDP: ‚ÄúGDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.‚Äù\nUrban Population (% of total): ‚ÄúUrban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects.‚Äù\n\nThe linear model of the relationship between GDP and urban population is as follows\n\\[\n\\widehat{\\log(GDP)} = 6.11 + 0.042 \\times urban\n\\]\n\nInterpret the slope in terms of the GDP in the context of the data.\nInterpret the intercept in terms of the GDP in the context of the data.\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-02.html#footnotes",
    "href": "hw/hw-02.html#footnotes",
    "title": "HW 02: Multiple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Education & median income in US Counties",
    "section": "",
    "text": "In this assignment, you‚Äôll use simple linear regression to examine the association between between the percent of adults with a bachelor‚Äôs degree and the median household income for counties in the United States."
  },
  {
    "objectID": "hw/hw-01.html#part-1-exploratory-data-analysis",
    "href": "hw/hw-01.html#part-1-exploratory-data-analysis",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 1: Exploratory data analysis",
    "text": "Part 1: Exploratory data analysis\n\nExercise 1\nCreate a histogram of the distribution of the response variable median_household_income and calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\n\n\nExercise 2\nLet‚Äôs view the data in another way. Use the code below to make a map of the United States with the color of the counties filled in based on the median household income. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 - 3 observations you have from the map?\nWhat is a feature that is apparent in the map that wasn‚Äôt as easily apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not as easily apparent from the map?\n\n\ncounty_map_data &lt;- left_join(county_data_sample, map_data_sample)\n\nggplot(data = map_data_all) +\n  geom_polygon(aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = county_map_data, aes(x = long, y = lat, group = group,\n    fill = median_household_income)\n    ) +\n  labs(\n    x = \"___\",\n    y = \"___\",\n    fill = \"___\",\n    title = \"___\"\n  ) +\n  scale_fill_viridis_c(labels = label_dollar()) +\n  coord_quickmap()\n\n\n\nExercise 3\nCreate a visualization of the relationship between bachelors and median_household_income . Use the visualization to describe the relationship between the two variables.\n\nIf you haven‚Äôt yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#part-2-modeling",
    "href": "hw/hw-01.html#part-2-modeling",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 2: Modeling",
    "text": "Part 2: Modeling\n\nExercise 4\nWe will use a linear regression model to better quantify the relationship between bachelors and median_household_income.\nWrite the form of the statistical model we will use for this task using mathematical notation. Use variable names (bachelors and median_household_income) in the equation for your model1.\n\n\nExercise 5\n\nFit the linear model to understand variability in the median household income based on the percent of adults age 25 and older in the county with a bachelor‚Äôs degree. Neatly display the model output with 3 digits.\nWrite the estimated regression equation using mathematical notation. Use variable names (bachelors and median_household_income) in the equation.\n\n\n\nExercise 6\nNow let‚Äôs use the model coefficients to describe the relationship between these two variables.\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nIs it meaningful to interpret the intercept for this data? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#part-3-inference-for-the-u.s.",
    "href": "hw/hw-01.html#part-3-inference-for-the-u.s.",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 3: Inference for the U.S.",
    "text": "Part 3: Inference for the U.S.\nWe want to use the data from these 600 randomly selected counties to draw conclusions about the relationship between the percent of adults age 25 and older with a bachelor‚Äôs degree and median household income for the over 3,000 counties in the United States.\n\nExercise 7\n\nWhat is the population of interest? What is the sample?\nIs it reasonable to treat the sample in this analysis as representative of the population? Briefly explain why or why not.\n\n\n\nExercise 8\nConduct a hypothesis test for the slope to assess whether there is sufficient evidence of a linear relationship between the percent of adults age 25 and older with a bachelor‚Äôs degree and the median household income in a county. Use a randomization (permutation) test. In your response:\n\nState the null and alternative hypotheses in words and mathematical notation\nShow all relevant code and output used to conduct the test. Use set.seed(8).\nState the conclusion in the context of the data.\n\n\n\nExercise 9\nNext, construct a 95% confidence interval for the slope using bootstrapping with set.seed(9). Show all relevant code and output used to calculate the interval. Interpret the confidence interval in the context of the data.\n\n\nExercise 10\nComment on whether the hypothesis test and confidence interval support the general consensus that adults who have a completed a bachelor‚Äôs degree generally earn higher income than adults who have not. A brief explanation is sufficient but it should be based on your conclusions from the hypothesis test and confidence interval.\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#part-4-model-comparison",
    "href": "hw/hw-01.html#part-4-model-comparison",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 4: Model comparison",
    "text": "Part 4: Model comparison\n\nExercise 11\nA researcher suggests that knowing the percentage of households in a county with a computer is a better indicator of median household income than the percentage of adults with a bachelor‚Äôs degree.\n\nFit a linear model of the relationship between median_household_income and household_has_computer. Neatly display the model with 3 digits.\nEvaluate this model and the model fit in Exercise 5 to assess the researcher‚Äôs claim.\nDo your analysis results support the researcher‚Äôs claim? Briefly explain why or why not.\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01.html#submission",
    "href": "hw/hw-01.html#submission",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ‚û°Ô∏è Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "hw/hw-01.html#grading-50-points",
    "href": "hw/hw-01.html#grading-50-points",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Grading (50 points)",
    "text": "Grading (50 points)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 11\n47\n\n\nWorkflow & formatting\n32"
  },
  {
    "objectID": "hw/hw-01.html#footnotes",
    "href": "hw/hw-01.html#footnotes",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClick here for a guide on writing mathematical symbols using LaTex.‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-06.html",
    "href": "labs/lab-06.html",
    "title": "Lab 06: Adelie Penguins",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nMonday, November 07 , 11:59pm (Thursday labs)\nTuesday, November 08, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-06.html#introduction",
    "href": "labs/lab-06.html#introduction",
    "title": "Lab 06: Adelie Penguins",
    "section": "Introduction",
    "text": "Introduction\nIn this assignment, you‚Äôll get to put into practice the logistic regression skills you‚Äôve developed to analyze data about Palmer Penguins.\n\nLearning goals\nBy the end of the lab you will be able to\n\nconduct exploratory data analysis for logistic regression\nfit logistic regression models and write the regression equation\nuse the model to calculate predicted probabilities\ncontinue developing a collaborative workflow with your teammates"
  },
  {
    "objectID": "labs/lab-06.html#getting-started",
    "href": "labs/lab-06.html#getting-started",
    "title": "Lab 06: Adelie Penguins",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa22 organization on GitHub. Click on the repo with the prefix lab-06. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-06.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-06.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 06: Adelie Penguins",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group‚Äôs Qmd file at a time. Once that person has finished typing the group‚Äôs responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-06.html#packages",
    "href": "labs/lab-06.html#packages",
    "title": "Lab 06: Adelie Penguins",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-06.html#data-palmer-penguins",
    "href": "labs/lab-06.html#data-palmer-penguins",
    "title": "Lab 06: Adelie Penguins",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nWe will go back to the Palmer penguins data used in HW 02.\nData were collected and made available by Dr.¬†Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\nThese data can be found in the palmerpenguins package. We‚Äôre going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica. Click here to see the codebook.\nWe will focus on the following variables:\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nbill_depth_mm\ninteger\nBill depth in mm"
  },
  {
    "objectID": "labs/lab-06.html#exercises",
    "href": "labs/lab-06.html#exercises",
    "title": "Lab 06: Adelie Penguins",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use logistic regression to understand the relationship between bill depth, island, and whether a penguin is from the Adelie species. First, we need to create a new response variable to identify whether a penguin is from the Adelie species.\n\npenguins &lt;- penguins |&gt;\n  mutate(adelie = factor(if_else(species == \"Adelie\", 1, 0)))\n\nAnd let‚Äôs check to make sure the new variable looks how we would expect before we continue with the analysis.\n\npenguins |&gt; \n  count(adelie, species)\n\n# A tibble: 3 √ó 3\n  adelie species       n\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 0      Chinstrap    68\n2 0      Gentoo      124\n3 1      Adelie      152\n\n\n\nExercise 1\nLet‚Äôs start by examining the relationship between adelie and island.\nVisualize the relationship between adelie and island. What is something you observe about the relationship between these two variables based on the plot?\n\n\n\n\n\n\nTip\n\n\n\nIf you need inspiration, click here for example plots and code to visualize the relationship between two categorical variables.\n\n\n\n\nExercise 2\nWhat does the values_fill argument do in the following chunk? The documentation for the function will be helpful in answering this question.\n\npenguins |&gt;\n  count(island, adelie) |&gt;\n  pivot_wider(names_from = adelie, values_from = n, values_fill = 0)\n\n# A tibble: 3 √ó 3\n  island      `0`   `1`\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 Biscoe      124    44\n2 Dream        68    56\n3 Torgersen     0    52\n\n\n\n\nExercise 3\n\nCalculate the probability a randomly selected penguin is from the Adelie species if it was recorded on Biscoe island.\nCalculate the odds a randomly selected penguin is from the Adelie species if it was recorded on Biscoe island.\n\n\n\nExercise 4\nYou want to fit a model using island to predict the odds of being from the Adelie species. Let \\(\\pi\\) be the probability a penguin is from the Adelie species. The model has the form shown below.\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1~ Dream + \\beta_2 ~ Torgersen\n\\]\n\nFit the model and neatly display the model output using three digits.\n\n\n\nWhat are the predicted odds of a penguin being from the Adelie species if it was recorded on Biscoe island?\nWhat are the predicted odds of a penguin being from the Adelie species if it was recorded on Dream island?\n\n\n\nExercise 5\nNext, we‚Äôd like to add bill depth to the model. We‚Äôll start by examining the relationship between these two variables.\nVisualize the relationship between bill_depth_mm and adelie. What is something you observe about the relationship between these two variables based on the plot?\n\n\nExercise 6\n\nAdd bill depth to the previous model so that there are two predictors, island and bill_depth_mm. Neatly display the model output using three digits.\nWrite the estimated regression equation.\n\n\n\nExercise 7\nUse the model from Exercise 6.\n\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with bill depth 17 mm to a penguin with bill depth 20 mm? Assume both penguins were recorded on the Dream island.\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with bill depth 17 mm to a penguin with bill depth 20 mm? Assume both penguins were recorded on the Dream island.\n\n\n\nExercise 8\nUse the model from Exercise 6.\n\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with bill depth 18 mm recorded on Biscoe island to a penguin with bill depth 21 mm recorded on Dream island?\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with bill depth 18 mm recorded on Biscoe island to a penguin with bill depth 21 mm recorded on Dream island?"
  },
  {
    "objectID": "labs/lab-06.html#submission",
    "href": "labs/lab-06.html#submission",
    "title": "Lab 06: Adelie Penguins",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nSelect all team members‚Äô names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "labs/lab-06.html#grading",
    "href": "labs/lab-06.html#grading",
    "title": "Lab 06: Adelie Penguins",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 8\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-06.html#footnotes",
    "href": "labs/lab-06.html#footnotes",
    "title": "Lab 06: Adelie Penguins",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-05.html",
    "href": "labs/lab-05.html",
    "title": "Lab 05: Candy Competition",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nMonday, October 24 , 11:59pm (Thursday labs)\nTuesday, October 25, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-05.html#introduction",
    "href": "labs/lab-05.html#introduction",
    "title": "Lab 05: Candy Competition",
    "section": "Introduction",
    "text": "Introduction\nIn today‚Äôs lab you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\nLearning goals\nBy the end of the lab you will be able to\n\ndescribe the components of a recipe\nfit a model using recipes\ncompare models\ncontinue developing a collaborative workflow with your teammates"
  },
  {
    "objectID": "labs/lab-05.html#getting-started",
    "href": "labs/lab-05.html#getting-started",
    "title": "Lab 05: Candy Competition",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa22 organization on GitHub. Click on the repo with the prefix lab-05. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-05.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-05.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 05: Candy Competition",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group‚Äôs Qmd file at a time. Once that person has finished typing the group‚Äôs responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-05.html#packages",
    "href": "labs/lab-05.html#packages",
    "title": "Lab 05: Candy Competition",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fivethirtyeight)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-05.html#data-candy",
    "href": "labs/lab-05.html#data-candy",
    "title": "Lab 05: Candy Competition",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data from this lab comes from the the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g.¬†Reeses vs.¬†Skittles). Click here to check out some of the match ups.\nThe data set contains the characteristics and win percentage from 85 candies in the experiment. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nchocolate\nDoes it contain chocolate?\n\n\nfruity\nIs it fruit flavored?\n\n\ncaramel\nIs there caramel in the candy?\n\n\npeanutalmondy\nDoes it contain peanuts, peanut butter or almonds?\n\n\nnougat\nDoes it contain nougat?\n\n\ncrispedricewafer\nDoes it contain crisped rice, wafers, or a cookie component?\n\n\nhard\nIs it a hard candy?\n\n\nbar\nIs it a candy bar?\n\n\npluribus\nIs it one of many candies in a bag or box?\n\n\nsugarpercent\nThe percentile of sugar it falls under within the data set. Values 0 - 1.\n\n\npricepercent\nThe unit price percentile compared to the rest of the set. Values 0 - 1.\n\n\nwinpercent\nThe overall win percentage according to 269,000 matchups. Values 0 - 100.\n\n\n\nUse the code below to get a glimpse of the candy_rankings data frame in the fivethirtyeight R package.\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter‚Ä¶\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F‚Ä¶\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE‚Ä¶\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,‚Ä¶\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, ‚Ä¶\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,‚Ä¶\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS‚Ä¶\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F‚Ä¶\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE‚Ä¶\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31‚Ä¶\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51‚Ä¶\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.‚Ä¶"
  },
  {
    "objectID": "labs/lab-05.html#exercises",
    "href": "labs/lab-05.html#exercises",
    "title": "Lab 05: Candy Competition",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to multiple linear regression to understand the factors that make a good candy.\n\nExercise 1\nSplit the data into training (80%) and test (20%) sets. Name the data sets candy_train and candy_test, respectively. Use set.seed(1).\n\n\n\n\n\n\nTip\n\n\n\nUse the prop = argument in the initial_split() function to specify the proportion of observations in the training data.\n\n\n\n\nExercise 2\nBelow is a recipe for a model that uses the characteristics of candy to understand variability in the win percentage. The lines of the recipe code are labeled Line 1 - Line 8. Describe what each line of code does. The explanation should be written comprehensively and specifically enough that someone could replicate the data manipulation steps based on your description.\nFor example, if a line of code is step_center(X), a comprehensive and specific explanation something similar to the following: ‚ÄúThis line of code mean centers the variable \\(X\\) by subtracting \\(\\bar{X}\\) from each value of \\(X\\) in the training data.‚Äù\n\n\n\n\n\n\nTip\n\n\n\nUse the Recipes Function Reference page as a resource to learn more about the step_ functions.\n\n\n\n#Line 1\ncandy_rec &lt;- recipe(winpercent ~ ., data = candy_train) |&gt; \n#Line 2\n  update_role(competitorname, new_role = \"ID\") |&gt; \n# Line 3\n  step_cut(sugarpercent, breaks = c(0, 0.25, 0.5, 0.75,1)) |&gt; \n#Line 4\n  step_mutate(pricepercent = pricepercent * 100) |&gt; \n#Line 5\n  step_dummy(all_nominal_predictors()) |&gt; \n#Line 6\n  step_interact(terms =~ pricepercent:chocolate) |&gt; \n#Line 7\n  step_rm(fruity, caramel, peanutyalmondy, nougat, hard, bar, pluribus, crispedricewafer) |&gt; \n#Line 8\n  step_zv(all_predictors()) \n\n\n\nExercise 3\nFill in the code to use prep and bake for a preview of what will happen when the recipe in Exercise 2 is applied.\n\ncandy_rec |&gt;\n  prep() |&gt;\n  bake(_____) |&gt;\n  glimpse()\n\nHow many terms (not including the intercept) will be in the model produced by this recipe?\n\n\nExercise 4\nSpecify the model, build the model workflow using the recipe in Exercise 2, and fit the model to the training data. Neatly display the model using 3 digits.\n\n\nExercise 5\nInterpret the following in the context of the data:\n\nIntercept\nCoefficient of sugarpercent_X.0.75.1.\nCoefficient of pricepercent_x_chocolateTRUE\n\n\n\nExercise 6\nLet‚Äôs consider another model. Use the recipe() workflow to fit a new model that meets the following criteria:\n\nIncludes variables chocolate, pricepercent, crispedricewafer, pluribus, sugarpercent\nUpdate pricepercent so it ranges from 0 to 100 (instead of 0 to 1)\nMakes sugarpercent a factor where the levels equal the four quartiles: 0 - 0.25, 0.25 - 0.50, 0.50 - 0.75, 0.75 - 1\nIncludes the interaction between pricepercent and pluribus\n\nNeatly display the model using 3 digits.\n\n\nExercise 7\nCalculate \\(R^2\\), Adjusted \\(R^2\\), AIC, and BIC for the models fit in Exercise 4 and the Exercise 6.\n\n\nExercise 8\nConsider the model from Exercise 4 ‚ÄúModel 1‚Äù and the model fit in Exercise 6 ‚ÄúModel 2‚Äù. State the model you would select based on each of the criteria below. Briefly explain your response.\n\n\\(R^2\\)\nAdjusted \\(R^2\\)\nAIC\nBIC\n\n\n\nExercise 9\nUse RMSE to evaluate the predictive performance of each model on the testing data. Which model would you choose based on RMSE?\n\n\nExercise 10\n\nWhich model do you choose after taking into account all the model evaluation statistics in Exercises 9 and 10? Briefly explain your response.\nUse the model you selected to describe what generally makes a good candy, i.e., one with a high win percentage."
  },
  {
    "objectID": "labs/lab-05.html#submission",
    "href": "labs/lab-05.html#submission",
    "title": "Lab 05: Candy Competition",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nSelect all team members‚Äô names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "labs/lab-05.html#grading",
    "href": "labs/lab-05.html#grading",
    "title": "Lab 05: Candy Competition",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-05.html#footnotes",
    "href": "labs/lab-05.html#footnotes",
    "title": "Lab 05: Candy Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "Lab 01: Ikea furniture",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nMonday, September 12, 11:59pm (Thursday labs)\nTuesday, September 13, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-01.html#learning-goals",
    "href": "labs/lab-01.html#learning-goals",
    "title": "Lab 01: Ikea furniture",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2 and use those visualizations to describe distributions\nBe gain to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 01: Ikea furniture",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-fa22 organization on GitHub. Click on the repo with the prefix lab-01-ikea-. It contains the starter documents you need to complete the lab.\n\n\n\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\n\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\n\n\n\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab-01.html#r-and-r-studio",
    "href": "labs/lab-01.html#r-and-r-studio",
    "title": "Lab 01: Ikea furniture",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of an Quarto (.Rmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.Rmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-01.html#footnotes",
    "href": "labs/lab-01.html#footnotes",
    "title": "Lab 01: Ikea furniture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLearn more about density plots and see code examples at the ggplot2 reference page for density plots.‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-02.html",
    "href": "labs/lab-02.html",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nMonday, September 19, 11:59pm (Thursday labs)\nTuesday, September 20, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-02.html#learning-goals",
    "href": "labs/lab-02.html#learning-goals",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will‚Ä¶\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use simulation-based inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-02.html#footnotes",
    "href": "labs/lab-02.html#footnotes",
    "title": "Lab 02: Ice duration and air temperature in Madison, WI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClick here for a guide on writing mathematical symbols using LaTex.‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-03.html",
    "href": "labs/lab-03.html",
    "title": "Lab 03: Coffee grades",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nMonday, September 26 , 11:59pm (Thursday labs)\nTuesday, September 27, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-03.html#learning-goals",
    "href": "labs/lab-03.html#learning-goals",
    "title": "Lab 03: Coffee grades",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will‚Ä¶\n\nbe able to use mathematical models to conduct inference for the slope\nbe able to assess conditions for simple linear regression"
  },
  {
    "objectID": "labs/lab-03.html#packages",
    "href": "labs/lab-03.html#packages",
    "title": "Lab 03: Coffee grades",
    "section": "Packages",
    "text": "Packages\nThe follow packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-03.html#the-data",
    "href": "labs/lab-03.html#the-data",
    "title": "Lab 03: Coffee grades",
    "section": "The Data",
    "text": "The Data\nThe dataset for this lab comes from the Coffee Quality Database and was obtained from the #TidyTuesday GitHub repo. It includes information about the origin, producer, measures of various characteristics, and the quality measure for over 1,000 coffees. The coffees can be reasonably be treated as a random sample.\nThis lab will focus on the following variables:\n\nsweetness: Sweetness grade, 0 (least sweet) - 10 (most sweet)\nflavor: Flavor grade, 0 (worst flavor) - 10 (best flavor)\n\nClick here for the definitions of all variables in the data set. Click here for more details about how these measures are obtained.\n\ncoffee &lt;- read_csv(\"data/coffee-grades.csv\")"
  },
  {
    "objectID": "labs/lab-03.html#grading-50-pts",
    "href": "labs/lab-03.html#grading-50-pts",
    "title": "Lab 03: Coffee grades",
    "section": "Grading (50 pts)",
    "text": "Grading (50 pts)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n31\n\n\nReproducible report\n22"
  },
  {
    "objectID": "labs/lab-03.html#footnotes",
    "href": "labs/lab-03.html#footnotes",
    "title": "Lab 03: Coffee grades",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes implementing version control with at least at least 3 informative commit messages and having a neatly formatted PDF document that is easily readable with an updated name and date.‚Ü©Ô∏é\nThis means we will be able to render the .qmd file in your GitHub repo and exactly reproduce the .pdf file in the Github repo and on Gradescope.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-04.html",
    "href": "labs/lab-04.html",
    "title": "Lab 04: The Office",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nThursday, October 13 , 11:59pm (Thursday labs)\nFriday, October 14, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-04.html#introduction",
    "href": "labs/lab-04.html#introduction",
    "title": "Lab 04: The Office",
    "section": "Introduction",
    "text": "Introduction\nIn today‚Äôs lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\nThis is a different data source than the one we‚Äôve used in class last week.\n\nLearning goals\nBy the end of the lab you will‚Ä¶\n\nengineer features based on episode scripts\ntrain a model\nmake predictions\nevaluate model performance on training and testing data\npractice collaborating with others using a single Github repo"
  },
  {
    "objectID": "labs/lab-04.html#getting-started",
    "href": "labs/lab-04.html#getting-started",
    "title": "Lab 04: The Office",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa22 organization on GitHub. Click on the repo with the prefix lab-04. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-04.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-04.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 04: The Office",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nAssign each person on your team a number 1 through 4. For teams of three, Team Member 1 can take on the role of Team Member 4.\nThe following exercises must be done in order. Only one person should type in the .qmd file, commit, and push updates at a time. When it is not your turn to type, you should still share ideas and contribute to the team‚Äôs discussion.\n\n\n\n\n\n\n\n\n‚å®Ô∏è Team Member 1: Hands on the keyboard.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!1\n\n\n\nChange the author to your team name and include each team member‚Äôs name in the author field of the YAML in the following format: Team Name: Member 1, Member 2, Member 3, Member 4.\n\nTeam Member 1: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub.\n\n\nTeam Members 2, 3, 4: Once Team Member 1 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the updated name in your .qmd file."
  },
  {
    "objectID": "labs/lab-04.html#packages",
    "href": "labs/lab-04.html#packages",
    "title": "Lab 04: The Office",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(schrute) #install.packages(\"schrute\")\nlibrary(lubridate)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-04.html#data-the-office",
    "href": "labs/lab-04.html#data-the-office",
    "title": "Lab 04: The Office",
    "section": "Data: The Office",
    "text": "Data: The Office\nThe data for this lab comes from the schrute package and it‚Äôs in the a data set called theoffice. This data set contains the entire script transcriptions from The Office.\nLet‚Äôs start by taking a peek at the data.\n\nglimpse(theoffice)\n\nRows: 55,130\nColumns: 12\n$ index            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16‚Ä¶\n$ season           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ episode          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ episode_name     &lt;chr&gt; \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",‚Ä¶\n$ director         &lt;chr&gt; \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis‚Ä¶\n$ writer           &lt;chr&gt; \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky‚Ä¶\n$ character        &lt;chr&gt; \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha‚Ä¶\n$ text             &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How ‚Ä¶\n$ text_w_direction &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How ‚Ä¶\n$ imdb_rating      &lt;dbl&gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6‚Ä¶\n$ total_votes      &lt;int&gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,‚Ä¶\n$ air_date         &lt;chr&gt; \"2005-03-24\", \"2005-03-24\", \"2005-03-24\", \"2005-03-24‚Ä¶\n\n\nThere are 55130 observations and 12 columns in this data set. The variable names are as follows.\n\nnames(theoffice)\n\n [1] \"index\"            \"season\"           \"episode\"          \"episode_name\"    \n [5] \"director\"         \"writer\"           \"character\"        \"text\"            \n [9] \"text_w_direction\" \"imdb_rating\"      \"total_votes\"      \"air_date\"        \n\n\nEach row in the data set is a line spoken by a character in a given episode of the show. This means some information at the episode level (e.g., imdb_rating, air_date, etc. are repeated across the rows that belong to a single episode.\nThe air_date variable is coded as a factor, which is undesirable for the analysis. We‚Äôll want to parse that variable later into its components during feature engineering. So, for now, let‚Äôs convert it to date.\n\ntheoffice &lt;- theoffice |&gt;\n  mutate(air_date = ymd(as.character(air_date)))\n\nLet‚Äôs take a look at the data to confirm we‚Äôre happy with how each of the variables are encoded.\n\nglimpse(theoffice)\n\nRows: 55,130\nColumns: 12\n$ index            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16‚Ä¶\n$ season           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ episode          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ episode_name     &lt;chr&gt; \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",‚Ä¶\n$ director         &lt;chr&gt; \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis‚Ä¶\n$ writer           &lt;chr&gt; \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky‚Ä¶\n$ character        &lt;chr&gt; \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha‚Ä¶\n$ text             &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How ‚Ä¶\n$ text_w_direction &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How ‚Ä¶\n$ imdb_rating      &lt;dbl&gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6‚Ä¶\n$ total_votes      &lt;int&gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,‚Ä¶\n$ air_date         &lt;chr&gt; \"2005-03-24\", \"2005-03-24\", \"2005-03-24\", \"2005-03-24‚Ä¶"
  },
  {
    "objectID": "labs/lab-04.html#exercises",
    "href": "labs/lab-04.html#exercises",
    "title": "Lab 04: The Office",
    "section": "Exercises",
    "text": "Exercises\n\nData prep\n\n\n\n\n\n\n‚å®Ô∏è Team Member 1: Hands still on the keyboard. Write the answers to Exercises 1 and 2.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\nExercise 1\nIdentify episodes where Halloween, Valentine‚Äôs Day, and Christmas are mentioned.\n\nFirst, convert all text to lowercase with the function str_to_lower().\nThen, create three new variables (halloween_mention, valentine_mention, and christmas_mention) that take the value 1 if the character string \"halloween\", \"valentine\", or \"christmas\" appears in the text, respectively, and 0 otherwise.\n\nBelow is code to help you get started.\n\ntheoffice &lt;- theoffice |&gt;\n  mutate(\n    text = ___(text),\n    halloween_mention = if_else(str_detect(text, \"___\"), ___, ___),\n    valentine_mention = ___,\n    ___ = ___\n  )\n\n\n\nExercise 2\nIn this exercise we‚Äôll accomplish two separate tasks. We‚Äôre doing both tasks all at once, because we‚Äôre going to drastically change our data frame, from one row per line spoken to one row per episode. We‚Äôll call the resulting data frame office_episodes.\nThe two tasks are as follows:\n\nTask 1. Identify episodes where the word ‚Äúhalloween‚Äù, ‚Äúvalentine‚Äù, or ‚Äúchristmas‚Äù were ever mentioned, using variables you created in Exercise 1.\nTask 2. Calculate the percentage of lines spoken by Jim, Pam, Michael, and Dwight for each episode of The Office.\n\nBelow are instructions and starter code to get you started with these tasks.\n\nStart by grouping theoffice data by season, episode, episode_name, imdb_rating, total_votes, and air_date. (These variables, except for season have the same value for each given episode, hence grouping by them allows us to make sure they appear in the output of this pipeline.)\nUse summarize() to calculate the desired features at the season-episode level.\nTask 1:\n\nCalculate the number of lines per season per episode. You can might name this new variable n_lines.\nThen, calculate the proportion of lines in that episode spoken by each of the four characters Jim, Pam, Michael, and Dwight. Name these new variables lines_jim, lines_pam, lines_michael, and lines_dwight, respectively.\n\nTask 2:\n\nCreate a variable called halloween that sums up the 1s in halloween_mention at the season-episode level and takes on the value \"yes\" if the sum is greater than or equal to 1, or \"no\" otherwise.\nDo something similar for new variables valentine and christmas as well based on values from valentine_mention and christmas_mention.\n\nFinish up your summarize() statement by dropping the groups, so the resulting data frame is no longer grouped. Additionally, remove n_lines (we won‚Äôt use that variable in our analysis, we only calculated it as an intermediary step).\n\n\noffice_episodes &lt;- theoffice |&gt;\n  group_by(___) |&gt;\n  summarize(\n    n_lines = n(),\n    lines_jim = sum(character == \"___\") / n_lines,\n    lines_pam = ___,\n    lines_michael = ___,\n    lines_dwight = ___,\n    halloween = if_else(sum(___) &gt;= 1, \"yes\", \"no\"),\n    valentine = if_else(___, \"___\", \"___\"),\n    christmas = if_else(___, \"___\", \"___\"),\n    .groups = \"drop\"\n  ) |&gt;\n  select(-n_lines)\n\n\n\n\n\n\n\nNote\n\n\n\nWhy summarize() and not mutate()? We use mutate() to add / modify a column of a data frame. The output data frame always has the same number of rows as the input data frame. On the other hand, we use summarize() to reduce the data frame to either a single row (single summary statistic) or one row per each group (summary statistics at the group level).\nAnd what about that .groups argument in summarize? You could try running your summarize() step without it first. You‚Äôll see that R print out a message saying ‚Äúsummarize() has grouped output by season, episode. You can override using the .groups argument.‚Äù summarize() will only drop the last group. So if you want a data frame that doesn‚Äôt have a grouping structure as a result of a summarize(), you can explicitly ask for that with .groups = \"drop\". Before you proceed, read the documentation for summarize(), and specifically the explanation for the .groups argument to prepare yourself for future instances where you might see this type of message.\n\n\n\nTeam Member 1: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 2, 3, 4: Once Team Member 1 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 1 and 2 in your .qmd file.\n\nNow it‚Äôs time for a hand off‚Ä¶\n\n\n\n\n\n\n‚å®Ô∏è Team Member 2: Hands on the keyboard. Write the answers to Exercises 3 - 5.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 3\nThe Michael Scott character (played by Steve Carrell) left the show at the end of Season 7. Add an indicator variable, michael, that takes on the value \"yes\" if Michael Scott (Steve Carrell) was in the show, and \"no\" if not.\n\noffice_episodes &lt;- office_episodes |&gt;\n  mutate(michael = if_else(season &gt; ___, \"___\", \"___\"))\n\n\n\nExercise 4\nPrint out the dimensions (dim()) of the new data set you created as well as the names() of the columns in the data set.\nYour new data set, office_episodes, should have 186 rows and 14 columns. The column names should be season, episode, episode_name, imdb_rating, total_votes, air_date, lines_jim, lines_pam, lines_michael, lines_dwight, halloween, valentine, christmas, and michael. If you are not matching these numbers or columns, go back and try to figure out where you went wrong. Or ask your TA for help!\n\n\n\nExploratory data analysis\nThis would be a good place to conduct some exploratory data analysis (EDA). For example, plot the proportion of lines spoken by each character over time. Or calculate the percentage of episodes that mention Halloween, or Valentine‚Äôs Day, or Christmas. Given we have limited time in the lab we‚Äôre not going to ask you to report EDA results as part of this lab, but we‚Äôre noting this here to provide suggestions for how you might go about structuring your project.\n\n\nModeling prep\n\nExercise 5\nSplit the data into training (75%) and testing (25%). Save the training and testing data as office_train and office_test respectively. Use seed 123.\nNaming suggestion: Call the initial split office_split, the training data office_train, and testing data office_test.\n\nset.seed(123)\noffice_split &lt;- ___(office_episodes)\noffice_train &lt;- ___(office_split)\noffice_test &lt;- ___(___)\n\n\nTeam Member 2: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 3, 4: Once Team Member 2 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 3 - 5 in your .qmd file.\n\n\nNow it‚Äôs time for another hand off‚Ä¶\n\n\n\n\n\n\n‚å®Ô∏è Team Member 3: Hands on the keyboard. Write the answers to Exercises 6 - 8.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 6\nSpecify a linear regression model with engine \"lm\" and call it office_spec.\nNaming suggestion: Call the model specification office_spec.\n\noffice_spec &lt;- ___\n\n\n\nExercise 7\nCreate a recipe that performs feature engineering using the following steps (in the given order):\n\nupdate_role(): updates the role of episode_name to not be a predictor (be an ID)\nstep_rm(): removes air_date and season as predictors\nstep_dummy(): creates dummy variables for all_nominal_predictors()\nstep_zv(): removes all zero variance predictors\n\nNaming suggestion: Call the recipe office_rec.\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  ___\n\n\n\nExercise 8\nBuild a model workflow for fitting the model specified earlier and using the recipe you developed to preprocess the data.\nNaming suggestion: Call the model workflow office_wflow.\n\noffice_wflow &lt;- workflow() |&gt;\n  add_model(___) |&gt;\n  add_recipe(___)\n\n\n\n\nModel fit and evaluation\n\nTeam Member 3: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 4: Once Team Member 3 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercise 6 - 8 in your .qmd file.\n\n\nNow it‚Äôs time for another hand off‚Ä¶\n\n\n\n\n\n\n‚å®Ô∏è Team Member 4: Hands on the keyboard. Write the answers to Exercises 9 - 11.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\nExercise 9\nFit the model to training data, neatly display the model output, and interpret two of the slope coefficients.\nNaming suggestion: Call the model fit office_fit.\n\noffice_fit &lt;- office_wflow |&gt;\n  fit(data = ___)\n\n___\n\n\n\nExercise 10\nCalculate predicted imdb_rating for the training data using the predict() function. Then, bind the columns from the training data to this result.\nUsing this data frame, create a scatterplot of predicted and observed IMDB ratings for the training data.\nNaming suggestion: Call the resulting data frame office_train_pred.\nStretch goal. Add episode names, using geom_text(), for episodes with much higher and much lower observed IMDB ratings compared to others.\n\n\nExercise 11\nCalculate the R-squared and RMSE for this model for predictions on the training data.\n\nTeam Member 4: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 3: Once Team Member 4 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercise 9 - 11 in your .qmd file.\n\n\nNow it‚Äôs time for another hand off‚Ä¶\n\n\n\n\n\n\n‚å®Ô∏è Team Member 2: Hands on the keyboard. Write the answers to Exercises 12 - 14.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 12\nRepeat Exercise 10, but with testing data.\nNaming suggestion: Call the resulting data frame office_test_pred.\n\n\nExercise 13\nBased on your visualization on Exercise 12, speculate on whether you expect the R-squared and RMSE for this model to be higher or lower for predictions on the testing data compared to those on the training data, or do you expect them to be the same? Explain your reasoning.\n\n\nExercise 14\nCheck your intuition in Exercise 13 by actually calculating the R-squared and RMSE for this model for predictions on the training data. Comment on whether your intuition is confirmed or not.\n\nTeam Member 2: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 3, 4: Once Team Member 2 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 12 - 14 in your .qmd file."
  },
  {
    "objectID": "labs/lab-04.html#wrapping-up",
    "href": "labs/lab-04.html#wrapping-up",
    "title": "Lab 04: The Office",
    "section": "Wrapping up",
    "text": "Wrapping up\n\n\n\n\n\n\n‚å®Ô∏è Team Member 3: Hands on the keyboard. Make any edits as needed.\nüôÖüèΩ All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\nTeam Member 3: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 4: Once Team Member 3 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team‚Äôs lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the final version of your .qmd file."
  },
  {
    "objectID": "labs/lab-04.html#submission",
    "href": "labs/lab-04.html#submission",
    "title": "Lab 04: The Office",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nSelect all team members‚Äô names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "labs/lab-04.html#grading",
    "href": "labs/lab-04.html#grading",
    "title": "Lab 04: The Office",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 14\n42\n\n\nWorkflow & formatting\n52\n\n\nComplete team contract\n3"
  },
  {
    "objectID": "labs/lab-04.html#footnotes",
    "href": "labs/lab-04.html#footnotes",
    "title": "Lab 04: The Office",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon‚Äôt trust yourself to keep your hands off the keyboard? Put them in your picket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!‚Ü©Ô∏é\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab-07.html",
    "href": "labs/lab-07.html",
    "title": "Lab 07: General Social Survey",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nMonday, November 14 , 11:59pm (Thursday labs)\nTuesday, November 15, 11:59pm (Friday labs)"
  },
  {
    "objectID": "labs/lab-07.html#introduction",
    "href": "labs/lab-07.html#introduction",
    "title": "Lab 07: General Social Survey",
    "section": "Introduction",
    "text": "Introduction\nIn this assignment, you‚Äôll analyze data from the 2016 General Social Survey using logistic regression for interpretation and prediction.\n\nLearning goals\nBy the end of the lab you will be able to‚Ä¶\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model\nUse the logistic regression model for prediction and classification"
  },
  {
    "objectID": "labs/lab-07.html#getting-started",
    "href": "labs/lab-07.html#getting-started",
    "title": "Lab 07: General Social Survey",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa22 organization on GitHub. Click on the repo with the prefix lab-07. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-07.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-07.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 07: General Social Survey",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group‚Äôs Qmd file at a time. Once that person has finished typing the group‚Äôs responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-07.html#packages",
    "href": "labs/lab-07.html#packages",
    "title": "Lab 07: General Social Survey",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-07.html#part-i-exploratory-data-analysis",
    "href": "labs/lab-07.html#part-i-exploratory-data-analysis",
    "title": "Lab 07: General Social Survey",
    "section": "Part I: Exploratory data analysis",
    "text": "Part I: Exploratory data analysis\n\nExercise 1\nLet‚Äôs begin by making a binary variable for respondents‚Äô views on spending on mass transportation. Create a new variable that is equal to ‚Äú1‚Äù if a respondent said spending on mass transportation is about right and ‚Äú0‚Äù otherwise. Then make a plot of the new variable, using informative labels for each category.\n\n\nExercise 2\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey.\n\n\n\n\n\n\nTip\n\n\n\nNote how the categories are spelled in the data.\n\n\n\nMake a plot of the distribution of polviews.\nWhich political view occurs most frequently in this data set?\n\n\n\nExercise 3\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\n\n\nExercise 4\nWe‚Äôd like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as \"89 or older\".\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values \"89 or older\" with a single value.\nThen plot the distribution of age."
  },
  {
    "objectID": "labs/lab-07.html#part-ii-logistic-regression-model",
    "href": "labs/lab-07.html#part-ii-logistic-regression-model",
    "title": "Lab 07: General Social Survey",
    "section": "Part II: Logistic regression model",
    "text": "Part II: Logistic regression model\n\nExercise 5\nBriefly explain why we should use a logistic regression model to predict the odds a randomly selected person is satisfied with spending on mass transportation.\n\n\nExercise 6\nSplit the data into training (75%) and testing sets (25%). Use a seed of 6.\n\n\nExercise 7\nLet‚Äôs start by fitting a model using the demographic factors - age, sex, sei10, and region - to predict the odds a person is satisfied with spending on mass transportation.\nUse the training data to fit the model described above. Use a recipe to make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Then, neatly display the model using 3 digits.\n\n\nExercise 8\n\nInterpret the intercept in terms of the odds in the context of the data.\nConsider the relationship between age and one‚Äôs opinion about spending on mass transportation. Interpret the coefficient of age in terms of the odds of being satisfied with spending on mass transportation.\n\n\n\nExercise 9\nNow let‚Äôs see whether a person‚Äôs political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors. Use the training data to fit a model using all the variables from Exercise 7 along with polviews. Neatly display the model using 3 digits.\n\n\nExercise 10\nUse the testing data to produce the ROC curve and calculate the area under curve (AUC) for the model fit in Exercise 7 and the one fit in Exercise 9. Which model is a better fit for the data? Briefly explain your choice.\n\n\nExercise 11\nYou have been tasked by a local political organization to identify adults who are satisfied with current spending on mass transportation. These adults will receive targeted political mailings that differ from adults who are not currently satisfied with spending on mass transportation. You will use the model selected in the previous exercise to predict which adults are satisfied with spending on mass transportation.\nWhat cutoff probability will you use to classify observations in ‚Äúsatisfied with mass transportation spending‚Äù versus ‚Äúnot satisfied‚Äù? Briefly explain how you determined that cutoff probability.\n\n\nExercise 12\nMake a confusion matrix using the cutoff probability from Exercise 11. Use the confusion matrix to calculate the following:\n\nSensitivity\nSpecificity\nFalse negative rate\nFalse positive rate"
  },
  {
    "objectID": "labs/lab-07.html#submission",
    "href": "labs/lab-07.html#submission",
    "title": "Lab 07: General Social Survey",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember ‚Äì you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nSelect all team members‚Äô names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù).\nSelect the first page of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù section."
  },
  {
    "objectID": "labs/lab-07.html#grading",
    "href": "labs/lab-07.html#grading",
    "title": "Lab 07: General Social Survey",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 12\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-07.html#footnotes",
    "href": "labs/lab-07.html#footnotes",
    "title": "Lab 07: General Social Survey",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/hw-04.html",
    "href": "hw/hw-04.html",
    "title": "HW 04: Logistic regression",
    "section": "",
    "text": "In this assignment you will analyze results from multiple studies that utilize logistic regression."
  },
  {
    "objectID": "hw/hw-04.html#packages",
    "href": "hw/hw-04.html#packages",
    "title": "HW 04: Logistic regression",
    "section": "Packages",
    "text": "Packages\nNo R packages are needed for this assignment."
  },
  {
    "objectID": "hw/hw-04.html#impacts-of-tea-consumption",
    "href": "hw/hw-04.html#impacts-of-tea-consumption",
    "title": "HW 04: Logistic regression",
    "section": "Impacts of tea consumption",
    "text": "Impacts of tea consumption\nThe 2016 article ‚ÄúTea consumption reduces the incidence of neurocognitive disorders: Findings from the Singapore longitudinal aging study‚Äù (Feng et al. 2016) examined the association between tea consumption habits and neurocognitive disorders (NCD), such as Alzheimer‚Äôs disease, in adults age 55 and older. Portions of the abstract are below:\n\nObjectives\nTo examine the relationships between tea consumption habits and incident neurocognitive disorders (NCD) and explore potential effect modification by gender and the apolipoprotein E (APOE) genotype.\nParticipants\n\n\n957 community-living Chinese elderly who were cognitively intact at baseline.\n\n\nMeasurements\n\n\nWe collected tea consumption information at baseline from 2003 to 2005 and ascertained incident cases of neurocognitive disorders (NCD) from 2006 to 2010. Odds ratio (OR) of association were calculated in logistic regression models that adjusted for potential confounders.\n\n\nResults\n\n\nA total of 72 incident NCD cases were identified from the cohort. Tea intake was associated with lower risk of incident NCD, independent of other risk factors. Reduced NCD risk was observed for both green tea (OR=0.43) and black/oolong tea (OR=0.53) and appeared to be influenced by the changing of tea consumption habit at follow-up. Using consistent nontea consumers as the reference, only consistent tea consumers had reduced risk of NCD (OR=0.39). Stratified analyses indicated that tea consumption was associated with reduced risk of NCD among females (OR=0.32) and APOE e4 carriers (OR=0.14) but not males and non APOE e4 carriers.\n\n\nExercise 1\nThe odds ratios reported in the abstract are the adjusted odds ratios, i.e., the odds ratios after adjusting for potential confounders such as age, pre-existing health conditions, diet, and behavioral factors. Interpret the following odds ratios from the abstract. Write the interpretations in the context of the data.\n\nOR = 0.39\nOR = 0.32\n\n\n\nExercise 2\nAn online article based on the results of Feng et al.¬†states the following:\n\n‚ÄúAnd for people who carry a gene that puts them at higher risk for Alzheimer‚Äôs disease (the APOE e4 gene), enjoying the beverage is even more important: Daily tea consumption could reduce their risk of cognitive decline by up to 86 percent.‚Äù\n\nIs this statement supported by the results of the study? Briefly explain why or why not."
  },
  {
    "objectID": "hw/hw-04.html#understanding-unemployment",
    "href": "hw/hw-04.html#understanding-unemployment",
    "title": "HW 04: Logistic regression",
    "section": "Understanding unemployment",
    "text": "Understanding unemployment\nIn the 2014 article ‚ÄúThe Biggest Predictor of How Long You‚Äôll Be Unemployed Is When You Lose Your Job‚Äù, author Ben Casselman analyzes the relationship between numerous factors such as age, race, and education and the odds an adult is unemployed for over a year.\n\nExercise 3\nAccording to the article, among those unemployed for over a year, 16% are under 25 years old, 62% are 25 to 54 years old, and 22% are 55 and up. Based on this data‚Ä¶\n\nWhat are the odds a randomly selected person who has been unemployed over a year is 55 and up?\nWhat are the odds a randomly selected person who has been unemployed over a year is not 25 to 54 years old?\n\n\n\nExercise 4\nCasselman fits a logistic regression model using the unemployment rate at the time the person lost their job to predict whether an adult is unemployed for over a year. He states the following from the model:\n\n‚ÄúA one-point increase in the unemployment rate raises an individual‚Äôs odds of becoming long-term unemployed by 35 percent.‚Äù\n\nWhat is the coefficient for unemployment rate in this model? Show how you calculated the answer."
  },
  {
    "objectID": "hw/hw-04.html#rearrest-risk-algorithms",
    "href": "hw/hw-04.html#rearrest-risk-algorithms",
    "title": "HW 04: Logistic regression",
    "section": "Rearrest risk algorithms",
    "text": "Rearrest risk algorithms\nIn the paper ‚ÄúEmploying Standardized Risk Assessment in Pretrial Release Decisions: Association With Criminal Justice Outcomes and Racial Equity‚Äù Marlowe et al. (2020) analyze the risk predictions produced by a black-box algorithm used to determine whether a defendant is considered ‚Äúhigh risk‚Äù of being rearrested if they are released while awaiting trial. Such algorithms are used by judges in some states to help determine whether or not defendants are released while awaiting trial.\nThe authors examine the algorithm‚Äôs risk predictions and whether a person was rearrested for over 500 defendants released pretrial in a southern state. For each person, the algorithm produced one of the following predictions: ‚ÄúHigh Risk‚Äù or ‚ÄúLow Risk‚Äù. The observed outcome was ‚ÄúRearrested‚Äù or ‚ÄúNot Rearrested‚Äù. Below are some results from the analysis:\n\nSensitivity: 86%\nSpecificity: 24%\nPositive predictive power: 57%\nNegative predictive power: 60%\n\n\n\n\n\n\n\nTip\n\n\n\n\nPositive Predictive Power: P(Y = 1 | Y classified as 1 from the model)\nNegative Predictive Power: P(Y = 0 | Y classified as 0 from the model)\n\n\n\n\nExercise 5\nExplain what each of the following mean in the context of the analysis:\n\nSensitivity\nSpecificity\nPositive predictive power\nNegative predictive power\n\n\n\nExercise 6\nWhat is the false positive rate? What does this value mean in the context of the analysis?\n\n\nExercise 7\nThe AUC for this algorithm is 0.55. Based on this value, do you think this algorithm a good fit for the population examined in the paper? Why or why not?"
  },
  {
    "objectID": "hw/hw-04.html#identifying-spam-emails",
    "href": "hw/hw-04.html#identifying-spam-emails",
    "title": "HW 04: Logistic regression",
    "section": "Identifying spam emails",
    "text": "Identifying spam emails\nSuppose you fit a logistic regression to aid in spam classification for individual emails. The output from the logistic regression model is below:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.81\n0.09\n-9.34\n&lt;0.0001\n\n\nto_multiple1\n-2.64\n0.30\n-8.68\n&lt;0.0001\n\n\nwinneryes\n1.63\n0.32\n5.11\n&lt;0.0001\n\n\nformat1\n-1.59\n0.12\n-13.28\n&lt;0.0001\n\n\nre_subj1\n-3.05\n0.36\n-8.40\n&lt;0.0001\n\n\n\n\n\n\nExercise 8\nUse the model to answer the following:\n\nWrite down the model using the coefficients from the model fit.\nSuppose we have an observation where to_muliple = 0, winner  = 1 , format = 1, and re_subj = 0. What is the predicted probability that this message is spam?\nSuppose you are a data scientist working on a spam filter. For a given message, how high must the probability a message is spam be before you think it would be reasonable to put it in a spambox/ junk folder (which the user is unlikely to check)? What are 2 tradeoffs you might consider?\n\nExercise 8 was adapted from an exercise in Introduction to Modern Statistics\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-04.html#footnotes",
    "href": "hw/hw-04.html#footnotes",
    "title": "HW 04: Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "",
    "text": "In this analysis you will use multiple linear regression to fit and evaluate models using characteristics of LEGO sets to understand variability in the price."
  },
  {
    "objectID": "hw/hw-03.html#exercise-1",
    "href": "hw/hw-03.html#exercise-1",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn this analysis, we dropped observations that have missing values for any of the relevant variables. What is a disadvantage of dropping observations that have missing values, instead of using a method to impute, i.e., fill in, the missing data? How might dropping these observations impact the generalizability of conclusions?"
  },
  {
    "objectID": "hw/hw-03.html#exercise-2",
    "href": "hw/hw-03.html#exercise-2",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distributions of the predictor variables Pieces, Size, Year, and Pages. Neatly arrange the plots using the patchwork package."
  },
  {
    "objectID": "hw/hw-03.html#exercise-3",
    "href": "hw/hw-03.html#exercise-3",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat are some feature engineering steps you might use to prepare the variables in the previous exercise for the model? Describe th step and the function you would use. The list should incorporate at least three different step_ functions.\n\n\n\n\n\n\nTip\n\n\n\nUse the recipes reference page for a list of step_ functions."
  },
  {
    "objectID": "hw/hw-03.html#exercise-4",
    "href": "hw/hw-03.html#exercise-4",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe distribution of Theme is shown below. The bars are ordered by the frequency they occur in the data set.\n\nlegos |&gt;\n  count(Theme) |&gt;\nggplot(aes(x = fct_reorder(Theme, n), y = n)) +\n  geom_col() + \n    labs(title = \"Lego Set Theme\", \n         x = \"Theme\", \n         y = \"Number of LEGO sets\") + \n  coord_flip()\n\n\n\n\nWhat is one reason we should avoid putting the variable Theme in a model as is?"
  },
  {
    "objectID": "hw/hw-03.html#exercise-5",
    "href": "hw/hw-03.html#exercise-5",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 5",
    "text": "Exercise 5\nWe will use v-fold cross validation to compare two models. We‚Äôll start by preparing the data, creating the folds, and defining the model specification that will be used for both models.\n\nSplit the data into training (75%) and testing (25%) sets. Use a seed of 5.\nSplit the training data into 10 folds. Use a seed of 5.\nSpecify the model."
  },
  {
    "objectID": "hw/hw-03.html#exercise-6",
    "href": "hw/hw-03.html#exercise-6",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the training data to create a recipe for the first model. The model uses the variables Size, Theme, and Pages to predict Amazon_Price. Conduct the following feature engineering steps:\n\nUse step_other() to collapse Theme into fewer categories. Define the threshold such that any levels of Theme with fewer than 20 observations is defined as ‚ÄúOther‚Äù.\nMean-center Pages.\nMake dummy variables for all categorical predictors.\nRemove any predictors with zero variance.\n\nThen create the workflow that brings together this recipe and the model specification from the previous exercise."
  },
  {
    "objectID": "hw/hw-03.html#exercise-7",
    "href": "hw/hw-03.html#exercise-7",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 7",
    "text": "Exercise 7\nConduct 10-fold cross validation using the workflow from the previous exercise. Calculate and display mean RMSE across the 10 folds.\n\n\n\n\n\n\nNote\n\n\n\nWe will just use RMSE to compare models for this assignment; however, in practice, it is best to take into account multiple model fit statistics to get a more holistic evaluation and comparison of the models."
  },
  {
    "objectID": "hw/hw-03.html#exercise-8",
    "href": "hw/hw-03.html#exercise-8",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs consider a new model that includes all the variables used in model from Exercise 6 along with Year and Pieces.\n\nUse the training data to create a recipe that uses all the feature engineering steps in Exercise 6 with the addition of the following steps:\n\nCreate a new variable called since2018 that calculates the number of years since 2018.\nRemove Year as a potential predictor.\nMean-center Pieces.\n\nCreate the workflow that brings together this recipe and the model specification from Exercise 5.\nConduct 10-fold cross validation using this model workflow. Calculate and display mean RMSE across the 10 folds."
  },
  {
    "objectID": "hw/hw-03.html#exercise-9",
    "href": "hw/hw-03.html#exercise-9",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 9",
    "text": "Exercise 9\nCompare the cross validation results from Exercises 7 and 8. Which model do you select based on RMSE? Briefly explain your choice."
  },
  {
    "objectID": "hw/hw-03.html#exercise-10",
    "href": "hw/hw-03.html#exercise-10",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 10",
    "text": "Exercise 10\nRefit the selected model on the entire training data. Neatly display the model using 3 digits.\nThen, calculate VIF for the model and use it to comment on whether there are potential issues with multicollinearity.\n\n\n\n\n\n\nTip\n\n\n\nWhen we fit a model using recipe and workflow, we need to extract the model object before using augment or vif functions. Fill in the name of the selected model in both blanks in the code below to extract the model object and calculate VIF\n\nlegos_fit_model &lt;- extract_fit_parsnip(______)\nvif(legos_fit_model$fit)"
  },
  {
    "objectID": "hw/hw-03.html#exercise-11",
    "href": "hw/hw-03.html#exercise-11",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 11",
    "text": "Exercise 11\nCalculate RMSE on the training data and on the testing data. Use it to comment on how well the model performs on new data and whether there are signs of model overfit."
  },
  {
    "objectID": "hw/hw-03.html#exercise-12",
    "href": "hw/hw-03.html#exercise-12",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 12",
    "text": "Exercise 12\nThough we do not check the model conditions in this assignment, complete this exercise assuming the model conditions are met.\nDescribe the effect of Theme on the price of LEGO sets, including an indication of which levels are statistically significant. Use a threshold of 0.05 to determine significance.\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-03.html#footnotes",
    "href": "hw/hw-03.html#footnotes",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/stats-experience.html",
    "href": "hw/stats-experience.html",
    "title": "HW: Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1Ô∏è‚É£ Have a statistics experience\n2Ô∏è‚É£ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW: Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you‚Äôd like to do but you‚Äôre not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf talks\n\n2022 conference\n2021 conference\n2020 conference\n\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn‚Äôt on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O‚ÄôNeil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don‚Äôt by Nate Silver\nList of books about data science ethics\n\nThis list is not exhaustive.\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n‚úÖ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n‚úÖ The visualization should include features or customization that are beyond what we‚Äôve done in class .\n‚úÖ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: Coding out loud\nWatch an episode of Coding out loud (either live or pre-recorded) and work through the project.\nA few guidelines:\n‚úÖ Create a GitHub repo for your Coding out loud submission. Your repo should include\n\nThe Quarto file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n‚úÖ The final product (visualuzation, table, etc.) should include features or customization that are beyond what was achieved in the Coding out loud episode.\n‚úÖ Include the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/stats-experience.html#part-2-summarize-your-experience",
    "href": "hw/stats-experience.html#part-2-summarize-your-experience",
    "title": "HW: Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we‚Äôve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e.¬†use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/stats-experience.html#submission",
    "href": "hw/stats-experience.html#submission",
    "title": "HW: Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Statistics Experience assignment on Gradescope by Fri, Dec 09 at 11:59pm. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "prepare/week-09.html",
    "href": "prepare/week-09.html",
    "title": "Week 09",
    "section": "",
    "text": "Important\n\n\n\n\nAE 10 due Thu, Oct 27, 11:59pm\nProject Proposal: due Fri, Nov 04: 11:59pm\n\nHW 03 moved to next week\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ Tidy Modeling in R - Chapter 10: Resampling for evaluating performance\nOct 24\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nüíª MLR: Cross validation\nOct 24\n\n\nüíª MLR: Inference\nOct 26\n\n\nüíª MLR: Conditions\nOct 26\n\n\nüíª Lab: Project proposal\nOct 27 & 28\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 10: Cross validation\n\n\nüìã AE 11: MLR Inference + conditions\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nüìù Project Proposal\ndue Fri, Nov 04, 11:59pm"
  },
  {
    "objectID": "prepare/week-10.html",
    "href": "prepare/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\n\nAE 12 due Thu, Nov 03, 11:59pm\nAE 13 due Sat, Nov 05, 11:59pm\nProject Proposal due Fri, Nov 04: 11:59pm\nHW 03 due Mon, Nov 07, 11:59pm\nLab 06:\n\ndue Mon, Nov 07, 11:59pm (Thu labs)\ndue Tue, Nov 08, 11:59pm (Fri labs)\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ IMS, Chap 9: Logistic Regression\nNov 02\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª MLR: Conditions\nOct 31\n\n\nüíª Logistic Regression: Introduction\nNov 02\n\n\nüíª Lab 06\nNov 03 - 04\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 12: Multiple Linear Regression Review\n\n\nüìã AE 13: Logistic Regression Intro\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nüìù Project Proposal\ndue Fri, Nov 04, 11:59pm\n\n\nüìù HW 03\ndue Mon, Nov 07, 11:59pm\n\n\nüìù Lab 06\n\ndue Mon, Nov 07, 11:59pm (Thu labs)\ndue Tue, Nov 08, 11:59pm (Fri labs)\n\n\n\nüìù Statistics Experience\ndue Fri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-05.html",
    "href": "prepare/week-05.html",
    "title": "Week 05",
    "section": "",
    "text": "Important\n\n\n\nAnnouncements:\n\nNo labs this week - work on Exam 01\nNo TA office hours Wed - Fri\n\nProf.¬†Tackett office hours Thu 10 - 11am\n\nNo Ed Discussion Wed - Fri\n\nDue dates:\n\nAE 06: due Thu, Sep 29, 11:59pm\nAE 07: due Sat, Oct 01, 11:59pm\nExam 01: Wed, Sep 28 - Fri, Sep 30 at 11:59pm\n\n\n\n\nPrepare\nNo readings this week.\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª MLR: Types of predictors\nSep 26\n\n\nüíª Exam 01 review\nSep 28\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 06: Prediction for MLR\n\n\nüìã AE 07: Exam 01 review\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nüìù Exam 01\nFri, Sep 30, 11:59pm"
  },
  {
    "objectID": "prepare/week-07.html",
    "href": "prepare/week-07.html",
    "title": "Week 07",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nLab 04:\n\ndue Thu, Oct 13, 11:59pm (Thu lab)\ndue Fri, Oct 14, 11:59pm (Fri lab)\n\nProject topic ideas:\n\ndue Mon, Oct 17, 11:59pm (Thu lab)\ndue Tue, Oct 18, 11:59pm (Fri lab)\n\nHW 02:\n\ndue Wed, Oct 19, 11:59pm\n\n\n\n\n\nPrepare\nNo readings this week.\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Variable transformations\nOct 12\n\n\n\n\n\nPractice\nNo Application Exercises this week.\n\n\nPerform\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nProject topic ideas\n\ndue Mon, Oct 17, 11:59pm (Thu lab)\ndue Tue, Oct 18, 11:59pm (Fri lab)\n\n\n\nüìù HW 02\nWed, Oct 19, 11:59pm"
  },
  {
    "objectID": "prepare/week-12.html",
    "href": "prepare/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\n\nAE 14: due Thu, Nov 17, 11:59pm\nHW 04: due Mon, Nov 21, 11:59pm\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ IMS, Chp 26: Inference for logistic regression\nNov 16\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nüíª Logistic regression: Model comparison\nNov 14\n\n\nüíª Logistic Regression: Inference + conditions\nNov 16\n\n\nüíª Lab: Project analysis + write up\nNov 17 + 18\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 14: Logistic regression - Model Comparison\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nüìù HW 04\ndue Mon, Nov 21, 11:59pm\n\n\nüìù Statistics experience\ndue Fri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-02.html",
    "href": "prepare/week-02.html",
    "title": "Week 02",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nNo AEs due this week\nLab 01:\n\ndue Mon, Sep 12, 11:59pm (Thu labs)\ndue Tue, Sep 13, 11:59pm (Fri labs)\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ Introduction to Modern Statistics (IMS), Ch 7: Linear regression with a single predictor\nSep 05 & 07\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª SLR: Fitting models in R with tidymodels\nSep 05\n\n\nüíª SLR: Prediction + model evaluation\nSep 07\n\n\nüíª Lab 01\nSep 08 & 09\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 02 - Bike rentals in Washigton, DC\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nAssignment\nDue date\n\n\n\n\nüìù Lab 01\nMon, Sep 12, 11:59pm (Thu labs)\nTue, Sep 13, 11:59pm (Fri labs)\n\n\nüìù Statistics experience\nFri, Dec 09, 11:59pm"
  },
  {
    "objectID": "prepare/week-01.html",
    "href": "prepare/week-01.html",
    "title": "Week 01",
    "section": "",
    "text": "Prepare\n\n\n\n\n\n\nüìñ Syllabus\n\n\nüìñ Course support\n\n\nüìñ Introduction to Modern Statistics (IMS), Ch 7: Linear regression with a single predictor\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Welcome to STA 210!\nAug 29\n\n\nüíª Simple Linear Regression\nAug 31\n\n\nüíª Lab 00\nSep 01 & 02\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 01-Movie Budgets\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nüìù Complete STA 210 Student Survey\ndue Fri, Sep 02 at 11:59pm\n\n\nüìù Reserve STA 210 RStudio Docker Container\nMon, Sep 05 at the beginning of class"
  },
  {
    "objectID": "prepare/week-14.html",
    "href": "prepare/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important\n\n\n\n\nAE 15 due Sat, Dec 03, 11:59pm\nTeam Feedback #2 due Tue, Dec 06, 11:59pm\nLooking ahead\n\nExam 02: Dec 05 (evening) - Dec 08, 12pm (noon)\n\nClick here for all lecture recordings\n\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\nReading\nCorresponding lecture\n\n\n\n\nüìñ Beyond Multiple Linear Regression, Chapter 7: Correlated Data (optional)\nNov 28\n\n\nüìñ Beyond Multiple Linear Regression, Chapter 8:Introduction to Multilevel Models (optional)\nNov 28 & Nov 30\n\n\n\n\n\nParticipate\n\n\n\n\n\n\n\nTopic\nDate\n\n\n\n\nüíª Introduction to Multilevel Models\nNov 28\n\n\nüíª Fitting Multilevel Models\nNov 30\n\n\nüíª Lab: Written report\nDec 01 & 02\n\n\n\n\n\nPractice\n\n\n\n\n\n\nüìã AE 15: Introduction to Multilevel Models\n\n\n\n\n\nPerform\n\n\n\n\n\n\n\nüìù Project written report\ndue Fri, Dec 09, 11:59pm (accepted until Sun, Dec 11, 11:59pm)\n\n\nüìù Statistics experience\ndue Fri, Dec 09, 11:59pm"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html",
    "href": "ae/ae-16-exam-02-review.html",
    "title": "AE 16: Exam 02 Review",
    "section": "",
    "text": "Important\n\n\n\nThe AE is due on GitHub by Thursday, December 08, 11:59pm.\n\nNote: This in-class review is not exhaustive. Use lecture notes notes, application exercises, labs, homework, and readings for a comprehensive exam review."
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#packages",
    "href": "ae/ae-16-exam-02-review.html#packages",
    "title": "AE 16: Exam 02 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-1",
    "href": "ae/ae-16-exam-02-review.html#exercise-1",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nSplit the data into training (75%) and testing (25%) sets. Use seed 1205.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-2",
    "href": "ae/ae-16-exam-02-review.html#exercise-2",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nWrite the equation of the statistical model for predicting interest rate (interest_rate) from debt to income ratio (debt_to_income), the term of loan (term), the number of inquiries (credit checks) into the applicant‚Äôs credit during the last 12 months (inquiries_last_12m), whether there are any bankruptcies listed in the public record for this applicant (bankrupt), and the type of application (application_type). The model should allow for the effect of debt to income ratio on interest rate to vary by application type.\n[Add model here]"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-3",
    "href": "ae/ae-16-exam-02-review.html#exercise-3",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nSpecify a linear regression model. Call it loans_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-4",
    "href": "ae/ae-16-exam-02-review.html#exercise-4",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse the training data to build the following recipe:\n\nPredict interest_rate from debt_to_income, term, inquiries_last_12m, public_record_bankrupt, and application_type.\nMean center debt_to_income.\nMake term a factor.\nCreate a new variable: bankrupt that takes on the value ‚Äúno‚Äù if public_record_bankrupt is 0 and the value ‚Äúyes‚Äù if public_record_bankrupt is 1 or higher. Then, remove public_record_bankrupt.\nInteract application_type with debt_to_income.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-5",
    "href": "ae/ae-16-exam-02-review.html#exercise-5",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate the workflow that brings together the model specification and recipe.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-6",
    "href": "ae/ae-16-exam-02-review.html#exercise-6",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nConduct 10-fold cross validation. Use the seed 1205. You will only collect the default metrics, \\(R^2\\) and RMSE. You do not need to collect AIC, BIC or Adj. \\(R^2\\).\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-7",
    "href": "ae/ae-16-exam-02-review.html#exercise-7",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nCollect and summarize \\(R^2\\) and RMSE metrics from your CV resamples.\n\n# add code here\n\nWhy are we focusing on \\(R^2\\) and RMSE instead of adjusted \\(R^2\\), AIC, BIC?\n[Add response here]"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-8",
    "href": "ae/ae-16-exam-02-review.html#exercise-8",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nRefit the model on the entire training data.\n\n# add code here\n\nThen, interpret the following in the context of the data:\n\nIntercept\ndebt_to_income for joint applications\nterm"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#data",
    "href": "ae/ae-16-exam-02-review.html#data",
    "title": "AE 16: Exam 02 Review",
    "section": "Data",
    "text": "Data\nAs part of a study of the effects of predatory invasive crab species on snail populations, researchers measured the mean closing forces and the propodus heights of the claws on several crabs of three species.\n\n\n\n\n\n\nclaws &lt;- read_csv(\"data/claws.csv\") |&gt;\n  mutate(lb = as_factor(lb))\n\nThe data set contains following variables:\n\nforce: Closing force of claw (newtons)\nheight: Propodus height (mm)\nspecies: Crab species - Cp(Cancer productus), Hn (Hemigrapsus nudus), Lb(Lophopanopeus bellus)\nlb: 1 if Lophopanopeus bellus species, 0 otherwise\nhn: 1 if Hemigrapsus nudus species, 0 otherwise\ncp: 1 if Cancer productus species, 0 otherwise\nforce_cent: mean centered force\nheight_cent: mean centered height"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#getting-started",
    "href": "ae/ae-16-exam-02-review.html#getting-started",
    "title": "AE 16: Exam 02 Review",
    "section": "Getting started",
    "text": "Getting started\n\nWhy do we use the log-odds as the response variable?\n\n[Add response here]\n\nFill in the blanks:\n\nUse log-odds to ‚Ä¶\nUse odds to ‚Ä¶\nUse probabilities to ‚Ä¶\n\nSuppose we want to use force to determine whether or not a crab is from the Lophopanopeus bellus (Lb) species. Why should we use a logistic regression model for this analysis?"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-9",
    "href": "ae/ae-16-exam-02-review.html#exercise-9",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\nWe will use force_cent, the mean-centered variable for force in the model. The model output is below. Write the equation of the model produced by R. Don‚Äôt forget to fill in the blanks for ‚Ä¶.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.798\n0.358\n-2.233\n0.026\n-1.542\n-0.123\n\n\nforce_cent\n0.043\n0.039\n1.090\n0.276\n-0.034\n0.123\n\n\n\n\n\nLet \\(\\pi\\) be‚Ä¶\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = \\]"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-10",
    "href": "ae/ae-16-exam-02-review.html#exercise-10",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the intercept in the context of the data."
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-11",
    "href": "ae/ae-16-exam-02-review.html#exercise-11",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 11",
    "text": "Exercise 11\nInterpret the effect of force in the context of the data."
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-12",
    "href": "ae/ae-16-exam-02-review.html#exercise-12",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 12",
    "text": "Exercise 12\nNow let‚Äôs consider adding height to the model. Fit the model that includes height_cent. Then use AIC to choose the model that best fits the data.\n\nlb_fit_2 &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(lb ~ force_cent + height_cent, data = claws)\n\ntidy(lb_fit_2, conf.int = TRUE) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-1.130\n0.463\n-2.443\n0.015\n-2.167\n-0.306\n\n\nforce_cent\n0.211\n0.092\n2.279\n0.023\n0.056\n0.424\n\n\nheight_cent\n-0.895\n0.398\n-2.249\n0.025\n-1.815\n-0.234"
  },
  {
    "objectID": "ae/ae-16-exam-02-review.html#exercise-13",
    "href": "ae/ae-16-exam-02-review.html#exercise-13",
    "title": "AE 16: Exam 02 Review",
    "section": "Exercise 13",
    "text": "Exercise 13\nWhat do the following mean in the context of this data. Explain and calculate them.\n\nSensitivity: ‚Ä¶\nSpecificity: ‚Ä¶\nNegative predictive power: ‚Ä¶"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html",
    "href": "ae/ae-08-feature-engineering.html",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-08- to get started.\nThe AE is due on GitHub by Saturday, October 08 at 11:59pm."
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#packages",
    "href": "ae/ae-08-feature-engineering.html#packages",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(viridis)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#load-data",
    "href": "ae/ae-08-feature-engineering.html#load-data",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Load data",
    "text": "Load data\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#exploratory-data-analysis",
    "href": "ae/ae-08-feature-engineering.html#exploratory-data-analysis",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nBelow are two of the exploratory data analysis plots from lecture.\n\nggplot(office_ratings, aes(x = imdb_rating)) +\n  geom_histogram(binwidth = 0.25) +\n  labs(\n    title = \"The Office ratings\",\n    x = \"IMDB rating\"\n  )\n\n\n\n\n\noffice_ratings |&gt;\n  mutate(season = as_factor(season)) |&gt;\n  ggplot(aes(x = season, y = imdb_rating, color = season)) +\n  geom_boxplot() +\n  geom_jitter() +\n  guides(color = \"none\") +\n  labs(\n    title = \"The Office ratings\",\n    x = \"Season\",\n    y = \"IMDB rating\"\n  ) +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#testtrain-split",
    "href": "ae/ae-08-feature-engineering.html#testtrain-split",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Test/train split",
    "text": "Test/train split\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\noffice_train &lt;- training(office_split)\noffice_test  &lt;- testing(office_split)"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#build-a-recipe",
    "href": "ae/ae-08-feature-engineering.html#build-a-recipe",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Build a recipe",
    "text": "Build a recipe\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())\n\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#workflows-and-model-fitting",
    "href": "ae/ae-08-feature-engineering.html#workflows-and-model-fitting",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\n\nSpecify model\n\noffice_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\nBuild workflow\n\noffice_wflow &lt;- workflow() |&gt;\n  add_model(office_spec) |&gt;\n  add_recipe(office_rec)\n\n\noffice_wflow\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_date()\n‚Ä¢ step_holiday()\n‚Ä¢ step_num2factor()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\nFit model to training data\n\noffice_fit &lt;- office_wflow |&gt;\n  fit(data = office_train)\n\ntidy(office_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.396\n0.510\n12.532\n0.000\n\n\nepisode\n-0.004\n0.017\n-0.230\n0.818\n\n\ntotal_votes\n0.000\n0.000\n9.074\n0.000\n\n\nseason_X2\n0.811\n0.327\n2.482\n0.014\n\n\nseason_X3\n1.042\n0.343\n3.040\n0.003\n\n\nseason_X4\n1.090\n0.295\n3.695\n0.000\n\n\nseason_X5\n1.082\n0.348\n3.109\n0.002\n\n\nseason_X6\n1.004\n0.367\n2.735\n0.007\n\n\nseason_X7\n1.018\n0.352\n2.894\n0.005\n\n\nseason_X8\n0.497\n0.348\n1.430\n0.155\n\n\nseason_X9\n0.621\n0.345\n1.802\n0.074\n\n\nair_date_dow_Tue\n0.382\n0.422\n0.904\n0.368\n\n\nair_date_dow_Thu\n0.284\n0.389\n0.731\n0.466\n\n\nair_date_month_Feb\n-0.060\n0.132\n-0.452\n0.652\n\n\nair_date_month_Mar\n-0.075\n0.156\n-0.481\n0.631\n\n\nair_date_month_Apr\n0.095\n0.177\n0.539\n0.591\n\n\nair_date_month_May\n0.156\n0.213\n0.734\n0.464\n\n\nair_date_month_Sep\n-0.078\n0.223\n-0.348\n0.728\n\n\nair_date_month_Oct\n-0.176\n0.174\n-1.014\n0.313\n\n\nair_date_month_Nov\n-0.156\n0.149\n-1.046\n0.298\n\n\nair_date_month_Dec\n0.170\n0.149\n1.143\n0.255"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#evaluate-model-on-training-data",
    "href": "ae/ae-08-feature-engineering.html#evaluate-model-on-training-data",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Evaluate model on training data",
    "text": "Evaluate model on training data\n\nMake predictions\n\n\n\n\n\n\nImportant\n\n\n\nFill in the code and make #| eval: true before rendering the document.\n\n\n\noffice_train_pred &lt;- predict(office_fit, ______) |&gt;\n  bind_cols(_____)\n\n\n\nCalculate \\(R^2\\)\n\n\n\n\n\n\nImportant\n\n\n\nFill in the code and make #| eval: true before rendering the document.\n\n\n\nrsq(office_train_pred, truth = _____, estimate = _____)\n\n\nWhat is preferred - high or low values of \\(R^2\\)?\n\n\n\nCalculate RMSE\n\n\n\n\n\n\nImportant\n\n\n\nFill in the code and make #| eval: true before rendering the document.\n\n\n\nrmse(______, ________, ________)\n\n\nWhat is preferred - high or low values of RMSE?\nIs this RMSE considered high or low? Hint: Consider the range of the response variable to answer this question.\n\noffice_train |&gt;\n  summarise(min = min(imdb_rating), max = max(imdb_rating))\n\n# A tibble: 1 √ó 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#evaluate-model-on-testing-data",
    "href": "ae/ae-08-feature-engineering.html#evaluate-model-on-testing-data",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Evaluate model on testing data",
    "text": "Evaluate model on testing data\nAnswer the following before evaluating the model performance on testing data:\n\nDo you expect \\(R^2\\) on the testing data to be higher or lower than the \\(R^2\\) calculated using training data? Why?\nDo you expect RMSE on the testing data to be higher or lower than the \\(R^2\\) calculated using training data? Why?\n\n\nMake predictions\n\n# fill in code to make predictions from testing data\n\n\n\nCalculate \\(R^2\\)\n\n# fill in code to calculate $R^2$ for testing data\n\n\n\nCalculate RMSE\n\n# fill in code to calculate RMSE for testing data"
  },
  {
    "objectID": "ae/ae-08-feature-engineering.html#compare-training-and-testing-data-results",
    "href": "ae/ae-08-feature-engineering.html#compare-training-and-testing-data-results",
    "title": "AE 08: Feature Engineering- Model workflow",
    "section": "Compare training and testing data results",
    "text": "Compare training and testing data results\n\nCompare the \\(R^2\\) for the training and testing data. Is this what you expected?\nCompare the RMSE for the training and testing data. Is this what you expected?"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html",
    "href": "ae/ae-10-cross-validation.html",
    "title": "AE 10: Cross validation",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-10- to get started.\nThe AE is due on GitHub by Thursday, October 27 at 11:59pm."
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#function",
    "href": "ae/ae-10-cross-validation.html#function",
    "title": "AE 10: Cross validation",
    "section": "Function",
    "text": "Function\n\n# function to calculate model fit statistics\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#packages",
    "href": "ae/ae-10-cross-validation.html#packages",
    "title": "AE 10: Cross validation",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#load-data",
    "href": "ae/ae-10-cross-validation.html#load-data",
    "title": "AE 10: Cross validation",
    "section": "Load data",
    "text": "Load data\n\noffice_episodes &lt;- read_csv(\"data/office_episodes.csv\")"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#split-data-into-training-and-testing",
    "href": "ae/ae-10-cross-validation.html#split-data-into-training-and-testing",
    "title": "AE 10: Cross validation",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\nset.seed(123)\noffice_split &lt;- initial_split(office_episodes)\noffice_train &lt;- training(office_split)\noffice_test &lt;- testing(office_split)"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#specify-model",
    "href": "ae/ae-10-cross-validation.html#specify-model",
    "title": "AE 10: Cross validation",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\noffice_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#create-recipe",
    "href": "ae/ae-10-cross-validation.html#create-recipe",
    "title": "AE 10: Cross validation",
    "section": "Create recipe",
    "text": "Create recipe\nCreate the recipe from class. Call it office_rec1.\n\noffice_rec1 &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  update_role(episode_name, new_role = \"id\") |&gt;\n  step_rm(air_date, season) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\noffice_rec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor         12\n\nOperations:\n\nVariables removed air_date, season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#preview-recipe",
    "href": "ae/ae-10-cross-validation.html#preview-recipe",
    "title": "AE 10: Cross validation",
    "section": "Preview recipe",
    "text": "Preview recipe\n\nprep(office_rec1) |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 139\nColumns: 12\n$ episode       &lt;dbl&gt; 20, 16, 8, 7, 23, 3, 16, 21, 18, 14, 27, 28, 12, 1, 23, ‚Ä¶\n$ episode_name  &lt;fct&gt; \"Welcome Party\", \"Moving On\", \"Performance Review\", \"The‚Ä¶\n$ total_votes   &lt;dbl&gt; 1489, 1572, 2416, 1406, 2783, 1802, 2283, 2041, 1445, 14‚Ä¶\n$ lines_jim     &lt;dbl&gt; 0.12703583, 0.05588822, 0.09523810, 0.07482993, 0.078291‚Ä¶\n$ lines_pam     &lt;dbl&gt; 0.10423453, 0.10978044, 0.10989011, 0.15306122, 0.081850‚Ä¶\n$ lines_michael &lt;dbl&gt; 0.0000000, 0.0000000, 0.3772894, 0.0000000, 0.3736655, 0‚Ä¶\n$ lines_dwight  &lt;dbl&gt; 0.07166124, 0.08782435, 0.15384615, 0.18027211, 0.135231‚Ä¶\n$ imdb_rating   &lt;dbl&gt; 7.2, 8.2, 8.2, 7.7, 9.1, 8.2, 8.3, 8.9, 8.0, 7.8, 8.7, 8‚Ä¶\n$ halloween_yes &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ valentine_yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ christmas_yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,‚Ä¶\n$ michael_yes   &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#create-workflow",
    "href": "ae/ae-10-cross-validation.html#create-workflow",
    "title": "AE 10: Cross validation",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it office_wflow1.\n\noffice_wflow1 &lt;- workflow() |&gt;\n  add_model(office_spec) |&gt;\n  add_recipe(office_rec1)\n\noffice_wflow1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n3 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#create-folds",
    "href": "ae/ae-10-cross-validation.html#create-folds",
    "title": "AE 10: Cross validation",
    "section": "Create folds",
    "text": "Create folds\nCreate 10-folds.\n\n# make 10 folds\nset.seed(345)\nfolds &lt;- vfold_cv(office_train, v = 10)"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#conduct-cross-validation",
    "href": "ae/ae-10-cross-validation.html#conduct-cross-validation",
    "title": "AE 10: Cross validation",
    "section": "Conduct cross validation",
    "text": "Conduct cross validation\nConduct cross validation on the 10 folds.\n\nset.seed(456)\n# Fit model and calculate statistics for each fold\noffice_fit_rs1 &lt;- office_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#summarize-assessment-cv-metrics",
    "href": "ae/ae-10-cross-validation.html#summarize-assessment-cv-metrics",
    "title": "AE 10: Cross validation",
    "section": "Summarize assessment CV metrics",
    "text": "Summarize assessment CV metrics\nSummarize assessment metrics from your CV resamples.\n\ncollect_metrics(office_fit_rs1, summarize = TRUE)\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.339    10  0.0227 Preprocessor1_Model1\n2 rsq     standard   0.567    10  0.0458 Preprocessor1_Model1"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#summarize-model-fit-cv-metrics",
    "href": "ae/ae-10-cross-validation.html#summarize-model-fit-cv-metrics",
    "title": "AE 10: Cross validation",
    "section": "Summarize model fit CV metrics",
    "text": "Summarize model fit CV metrics\n\nmap_df(office_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  summarise(mean_adj_rsq = mean(adj.r.squared), \n            mean_aic = mean(AIC), \n            mean_bic = mean(BIC))\n\n# A tibble: 1 √ó 3\n  mean_adj_rsq mean_aic mean_bic\n         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        0.583     90.8     125."
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#model-2-recipe",
    "href": "ae/ae-10-cross-validation.html#model-2-recipe",
    "title": "AE 10: Cross validation",
    "section": "Model 2: Recipe",
    "text": "Model 2: Recipe\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#model-2-workflow",
    "href": "ae/ae-10-cross-validation.html#model-2-workflow",
    "title": "AE 10: Cross validation",
    "section": "Model 2: Workflow",
    "text": "Model 2: Workflow\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#model-2-conduct-cv",
    "href": "ae/ae-10-cross-validation.html#model-2-conduct-cv",
    "title": "AE 10: Cross validation",
    "section": "Model 2: Conduct CV",
    "text": "Model 2: Conduct CV\n\n\n\n\n\n\nNote\n\n\n\nNote: We will use the same folds as the ones used for Model 1. Why should we use the same folds to evaluate and compare both models?\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#model-2-summarize-assessment-cv-metrics",
    "href": "ae/ae-10-cross-validation.html#model-2-summarize-assessment-cv-metrics",
    "title": "AE 10: Cross validation",
    "section": "Model 2: Summarize assessment CV metrics",
    "text": "Model 2: Summarize assessment CV metrics\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-cross-validation.html#model-2-summarize-model-fit-cv-metrics",
    "href": "ae/ae-10-cross-validation.html#model-2-summarize-model-fit-cv-metrics",
    "title": "AE 10: Cross validation",
    "section": "Model 2: Summarize model fit CV metrics",
    "text": "Model 2: Summarize model fit CV metrics\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-mlr.html",
    "href": "ae/ae-05-mlr.html",
    "title": "AE 05: Multiple linear regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-05- to get started.\nThe AE is due on GitHub by Saturday, September 24 at 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(GGally)\nThe data set contains the sales price and characteristics of 85 homes in Levittown, NY that sold between June 2010 and May 2011. Levittown was built right after WWII and was the first planned suburban community built using mass production techniques.\nlevittown &lt;- read_csv(\"data/homeprices.csv\")\nThe variables used in this analysis are\nThe goal of the analysis is to use the characteristics of a house to understand variability in the sales price."
  },
  {
    "objectID": "ae/ae-05-mlr.html#exploratory-data-analysis",
    "href": "ae/ae-05-mlr.html#exploratory-data-analysis",
    "title": "AE 05: Multiple linear regression",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nggpairs(levittown) +\n  theme(\n    axis.text.y = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, size = 10),\n    strip.text.y = element_text(angle = 0, hjust = 0)\n    )"
  },
  {
    "objectID": "ae/ae-05-mlr.html#linear-model",
    "href": "ae/ae-05-mlr.html#linear-model",
    "title": "AE 05: Multiple linear regression",
    "section": "Linear model",
    "text": "Linear model\nFit a linear model of housing prices versus the house characteristics in Levittown. Neatly display model using 3 digits.\n\n# fit model \n\n# display model with 3 digits"
  },
  {
    "objectID": "ae/ae-05-mlr.html#interpretation",
    "href": "ae/ae-05-mlr.html#interpretation",
    "title": "AE 05: Multiple linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nInterpret the coefficient of bedrooms in the context of the data.\nThe intercept is the estimated sales price for what subset of houses? Be specific."
  },
  {
    "objectID": "ae/ae-05-mlr.html#prediction",
    "href": "ae/ae-05-mlr.html#prediction",
    "title": "AE 05: Multiple linear regression",
    "section": "Prediction",
    "text": "Prediction\nWhat is the predicted sale price for a house in Levittown, NY with 4 bedrooms, 2 bathrooms, 1,000 square feet of living area, 6,000 square foot lot size, built in 1947 with $7,403 in property taxes?\n\nReport the predicted value and appropriate interval.\n\n\n# create tibble for new observation \n\n# prediction + interval\n\n\nInterpret the interval in the context of the data.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-05- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-12-mlr-review.html",
    "href": "ae/ae-12-mlr-review.html",
    "title": "AE 12: Multiple linear regression review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-12- to get started.\nThe AE is due on GitHub by Thursday, November 03, 11:59pm.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(rms)\nlibrary(knitr)\nThe data for this analysis includes information about LEGO sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide.\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are\nThe goal of this analysis is to predict the Amazon price based on the number of pieces, theme, and size of pieces. We will only include observations that have recorded values for all relevant variables.\nlegos &lt;- read_csv(\"data/lego-sample.csv\") |&gt;\n  select(Size, Pieces, Theme, Amazon_Price) |&gt;\n  drop_na()"
  },
  {
    "objectID": "ae/ae-12-mlr-review.html#exploratory-data-analysis",
    "href": "ae/ae-12-mlr-review.html#exploratory-data-analysis",
    "title": "AE 12: Multiple linear regression review",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nResponse variable\n\nggplot(data = legos, aes(x = Amazon_Price)) +\n  geom_histogram() + \n    labs(title = \"Price of Lego sets on Amazon\",\n         x = \"Price in US dollars\")\n\n\n\n\n\n\nPredictor variables\n\np_pieces &lt;- ggplot(data = legos, aes(x = Pieces)) +\n  geom_histogram() + \n    labs(title = \"Number of pieces\",\n         x = \"\")\n\np_size &lt;- ggplot(data = legos, aes(x = Size)) +\n  geom_bar() + \n    labs(title = \"Piece size\",\n         x = \"\") + \n  coord_flip()\n\np_pieces /  p_size\n\n\n\n\n\nWhat (if any) feature engineering might we want to include in a recipe for a model for Pieces and Size?\n\n\nlegos |&gt;\n  count(Theme) |&gt;\nggplot(aes(x = fct_reorder(Theme, n), y = n)) +\n  geom_col() + \n    labs(title = \"Lego Set Theme\", \n         x = \"Theme\", \n         y = \"Number of LEGO sets\") + \n  coord_flip()\n\n\n\n\n\nWhy should we avoid putting Theme in a model as is?\nHow can we use step_other() in the recipe to make Theme more usable in the model?"
  },
  {
    "objectID": "ae/ae-12-mlr-review.html#model-fitting",
    "href": "ae/ae-12-mlr-review.html#model-fitting",
    "title": "AE 12: Multiple linear regression review",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nTraining and test sets\nWrite code to data into training (75%) and testing (25%) sets.\n\nset.seed(1031)\n\n# add code to make training and test sets\n\n\n\nModel workflow\nFit the model using Pieces, Size, and Theme to understand variability in Amazon_Price. To do so, specify the model and use training data to create a recipe applying the feature engineering steps mentioned above. Then, build the model workflow and fit the model.\n\n# add code to specify model \n\n\n# add code to create recipe\n\n\n# This is an optional step to see the outcome of the recipe\n\n# add code to prep and bake \n\n\n# add code to build workflow\n\n\n# add code to fit model"
  },
  {
    "objectID": "ae/ae-12-mlr-review.html#check-conditions-multicollinearity",
    "href": "ae/ae-12-mlr-review.html#check-conditions-multicollinearity",
    "title": "AE 12: Multiple linear regression review",
    "section": "Check conditions + multicollinearity",
    "text": "Check conditions + multicollinearity\nWhen we fit a model using recipe and workflow, we need to extract the model object before using the augment function. Fill in the name of the model fit in the code below.\n\n\n\n\n\n\nUpdate the option to eval: true, so the code chunk evaluates when the document is rendered.\n\n\n\n\nlego_fit_model &lt;- extract_fit_parsnip(______)\nlego_aug &lt;- augment(lego_fit_model)\n\n\nResiduals vs predicted values\nUse the code below to make a plot of residuals vs.¬†predicted values\n\n\n\n\n\n\nUpdate the option to eval: true, so the code chunk evaluates when the document is rendered.\n\n\n\n\nggplot(data = lego_aug, aes(x = .fitted, y= .resid)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") + \n  labs(x = \"Predicted\", \n       y = \"Residuals\", \n       title = \"Residuals vs. Predicted\")\n\n\n\nDistribution of residuals\nFill in the code below to make a histogram of the residuals with an overlay of the normal distribution.\n\n\n\n\n\n\nUpdate the option to eval: true, so the code chunk evaluates when the document is rendered.\n\n\n\n\nggplot(lego_aug, aes(.resid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = mean(lego_aug$_____), sd = sd(_____)), \n    color = \"red\"\n  )\n\nUse the plots and information about the data to comment on whether each condition is satisfied.\n\nLinearity: [Add response]\nConstant variance: [Add response]\nNormality: [Add response]\nIndependence: [Add response]\n\n\n\nMulticollinearity\nFun the code to use the vif() function from the rms package to check multicollinearity.\n\n\n\n\n\n\nUpdate the option to eval: true, so the code chunk evaluates when the document is rendered.\n\n\n\n\nvif(lego_fit_model$fit)\n\n\nAre there issues with multicollinearity in the model?"
  },
  {
    "objectID": "ae/ae-12-mlr-review.html#next-steps",
    "href": "ae/ae-12-mlr-review.html#next-steps",
    "title": "AE 12: Multiple linear regression review",
    "section": "Next steps",
    "text": "Next steps\n\nBased on the assessment of the model conditions and multicollinearity, what is the next step you might take in the model building process?\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your ae-12- repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html",
    "href": "ae/ae-11-mlr-inference.html",
    "title": "AE 11: MLR Inference + conditions",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-11- to get started.\nThe AE is due on GitHub by Saturday, October 29 at 11:59pm."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#packages",
    "href": "ae/ae-11-mlr-inference.html#packages",
    "title": "AE 11: MLR Inference + conditions",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#data",
    "href": "ae/ae-11-mlr-inference.html#data",
    "title": "AE 11: MLR Inference + conditions",
    "section": "Data",
    "text": "Data\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-1",
    "href": "ae/ae-11-mlr-inference.html#exercise-1",
    "title": "AE 11: MLR Inference + conditions",
    "section": "Exercise 1",
    "text": "Exercise 1\nBelow is the model predicting volume from hightemp and season.\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\nAdd an interaction effect between hightemp and season to the model. Do the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms.\n\n## add code"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-2",
    "href": "ae/ae-11-mlr-inference.html#exercise-2",
    "title": "AE 11: MLR Inference + conditions",
    "section": "Exercise 2",
    "text": "Exercise 2\nBelow is the model predicting volume from all available predictors.\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit) |&gt;\nkable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.62\n76.58\n0.23\n0.82\n\n\nhightemp\n7.07\n2.42\n2.92\n0.00\n\n\navgtemp\n-2.04\n3.14\n-0.65\n0.52\n\n\nseasonSpring\n35.91\n32.99\n1.09\n0.28\n\n\nseasonSummer\n24.15\n52.81\n0.46\n0.65\n\n\ncloudcover\n-7.25\n3.84\n-1.89\n0.06\n\n\nprecip\n-95.70\n42.57\n-2.25\n0.03\n\n\nday_typeWeekend\n35.90\n22.43\n1.60\n0.11\n\n\n\n\n\nFill in the code to plot the histogram of residuals with an overlay of the normal distribution based on the results of the model.\n\n\n\n\n\n\nNote\n\n\n\nUpdate to eval: true once the code is updated.\n\n\n\nrt_full_aug &lt;- augment(_______)\n\nggplot(rt_full_aug, aes(.resid)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 50) +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = mean(rt_full_aug$____), sd = ______), \n    lwd = 2, \n    color = \"red\"\n  )"
  },
  {
    "objectID": "ae/ae-01-movies.html",
    "href": "ae/ae-01-movies.html",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is a demo only.\nYou do not have a corresponding repository for it and you‚Äôre not expected to turn in anything for it.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(DT)"
  },
  {
    "objectID": "ae/ae-01-movies.html#data",
    "href": "ae/ae-01-movies.html#data",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\")\n\nView the first 10 rows of data.\n\nmovies |&gt;\n  slice(1:10)\n\n# A tibble: 10 √ó 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S‚Ä¶ R      Drama  1980 June 13‚Ä¶   8.4 9.27e5 Stanley‚Ä¶ Steph‚Ä¶ Jack‚Ä¶ United‚Ä¶\n 2 The B‚Ä¶ R      Adve‚Ä¶  1980 July 2,‚Ä¶   5.8 6.5 e4 Randal ‚Ä¶ Henry‚Ä¶ Broo‚Ä¶ United‚Ä¶\n 3 Star ‚Ä¶ PG     Acti‚Ä¶  1980 June 20‚Ä¶   8.7 1.2 e6 Irvin K‚Ä¶ Leigh‚Ä¶ Mark‚Ä¶ United‚Ä¶\n 4 Airpl‚Ä¶ PG     Come‚Ä¶  1980 July 2,‚Ä¶   7.7 2.21e5 Jim Abr‚Ä¶ Jim A‚Ä¶ Robe‚Ä¶ United‚Ä¶\n 5 Caddy‚Ä¶ R      Come‚Ä¶  1980 July 25‚Ä¶   7.3 1.08e5 Harold ‚Ä¶ Brian‚Ä¶ Chev‚Ä¶ United‚Ä¶\n 6 Frida‚Ä¶ R      Horr‚Ä¶  1980 May 9, ‚Ä¶   6.4 1.23e5 Sean S.‚Ä¶ Victo‚Ä¶ Bets‚Ä¶ United‚Ä¶\n 7 The B‚Ä¶ R      Acti‚Ä¶  1980 June 20‚Ä¶   7.9 1.88e5 John La‚Ä¶ Dan A‚Ä¶ John‚Ä¶ United‚Ä¶\n 8 Ragin‚Ä¶ R      Biog‚Ä¶  1980 Decembe‚Ä¶   8.2 3.3 e5 Martin ‚Ä¶ Jake ‚Ä¶ Robe‚Ä¶ United‚Ä¶\n 9 Super‚Ä¶ PG     Acti‚Ä¶  1980 June 19‚Ä¶   6.8 1.01e5 Richard‚Ä¶ Jerry‚Ä¶ Gene‚Ä¶ United‚Ä¶\n10 The L‚Ä¶ R      Biog‚Ä¶  1980 May 16,‚Ä¶   7   1   e4 Walter ‚Ä¶ Bill ‚Ä¶ Davi‚Ä¶ United‚Ä¶\n# ‚Ä¶ with 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;, company &lt;chr&gt;,\n#   runtime &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-01-movies.html#analysis",
    "href": "ae/ae-01-movies.html#analysis",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Analysis",
    "text": "Analysis\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\n\nmovies |&gt;\n  filter(genre %in% genre_list) |&gt; \n  group_by(genre,year) |&gt;\n  summarise(avg_gross = mean(gross)) |&gt;\n  ggplot(mapping = aes(x = year, y = avg_gross, color=genre)) +\n    geom_point() + \n    geom_line() +\n    ylab(\"Average Gross Revenue (in US Dollars)\") +\n    ggtitle(\"Gross Revenue Over Time\") +\n    scale_color_viridis_d()\n\n\n\n\nWhat do you observe from the plot?\nNext, let‚Äôs see the relationship between a movie‚Äôs budget and its gross revenue.\n\nmovies |&gt;\n  filter(genre %in% genre_list, budget &gt; 0) |&gt; \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point() +\n  geom_smooth(method=\"lm\",se=FALSE) + \n  xlab(\"Log-transformed Budget\")+\n  ylab(\"Log-transformed Gross Revenue\") +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d()"
  },
  {
    "objectID": "ae/ae-01-movies.html#exercises",
    "href": "ae/ae-01-movies.html#exercises",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nPost your response on ED Discussion.\n\nSection 001 (10:15am lecture)\nSection 002 (3:30pm lecture)\n\nIn the remaining time, discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-01-movies.html#references",
    "href": "ae/ae-01-movies.html#references",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "References",
    "text": "References\n\nhttps://github.com/danielgrijalva/movie-stats\nInternet Movie Database"
  },
  {
    "objectID": "ae/ae-01-movies.html#appendix",
    "href": "ae/ae-01-movies.html#appendix",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies |&gt; \n  arrange(genre) |&gt; \n  select(genre) |&gt;\n  distinct() |&gt;\n  datatable()"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html",
    "href": "ae/ae-02-bikeshare.html",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-01-bikehsare to get started.\nThis AE will not be counted as part of the Application Exercises portion of the final course grade.\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#data",
    "href": "ae/ae-02-bikeshare.html#data",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-02-bikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250)\n\n\n\nggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point()\n\n\n\n\n\n\nExercise 2\nDescribe the distribution of daily bike rentals and the distribution of temperature based on the visualizations created in Exercise 1. Include the shape, center, spread, and presence of any potential outliers.\n[Add your answer here]\n\n\nExercise 3\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n[Add your answer here]\n\n\nExercise 4\nDescribe the relationship between daily bike rentals and temperature based on the visualization created in Exercise 1. Comment on how we expect the number of bike rentals to change as the temperature increases.\n[Add your answer here]\n\n\nExercise 5\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\n\\[\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp\\_orig} + \\epsilon\\]\nbe the best fit for the data? Why or why not?\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-02-bikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 6\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 7\nNext, let‚Äôs look at how the daily bike rentals differ by season. Let‚Äôs visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a ‚Äúsmoothed out histogram‚Äù. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 8\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs.¬†temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 9\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#modeling",
    "href": "ae/ae-02-bikeshare.html#modeling",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 10\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 11\nUsing the data you filtered in Exercise 10, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 13\nInterpret the slope in the context of the data.\n[Add your answer here]\n\n\nExercise 14\nInterpret the intercept in the context of the data.\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#synthesis",
    "href": "ae/ae-02-bikeshare.html#synthesis",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 15\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]\n\nThe following exercises will be completed only if time permits.\n\n\nExercise 16\nPick another season. Based on the visualization in Exercise 8, would you expect the slope of the relationship between temperature and daily bike rentals to be smaller or larger than the slope of the model you‚Äôve been working with so far? Explain your reasoning.\n[Add your answer here]\n\n\nExercise 17\nFor this season you picked in Exercise 16, fit a linear model for predicting daily bike rentals from temperature. Note, you will need to filter your data for this season first. Use the output to write out the estimated regression equation and interpret the slope and the intercept of this model.\n\n# add your code here\n\n[Add your answer here]"
  },
  {
    "objectID": "project-instructions.html",
    "href": "project-instructions.html",
    "title": "Final project",
    "section": "",
    "text": "Topic ideas\n\ndue Monday, October 17 (Thursday labs)\ndue Tuesday, October 18 (Friday labs)\n\nProject proposal due Friday, November 4\nRound 1 submission (optional) due Tuesday, November 22\nWritten report due Friday, December 9 (accepted until December 11)\nVideo presentation + slides and Reproducibility + organization due Wednesday, December 14\nPresentation comments due Friday, December 16\n\n\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group‚Äôs interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\n\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team‚Äôs project\n\n\n\n\n\nIdentify 2 - 3 data sets you‚Äôre interested in potentially using for the final project. If you‚Äôre unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point. It may also help to think of topics you‚Äôre interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help set you up for a successful project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as ‚Äúname‚Äù, ‚Äúsocial security number‚Äù, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g.¬†‚Äústate abbreviation‚Äù and ‚Äústate name‚Äù), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTip\n\n\n\nAsk a member of the teaching team if you‚Äôre unsure whether your data set meets the criteria.\n\n\nFor each data set, include the following:\n\n\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\n\n\nDescribe a research question you‚Äôre interested in answering using this data.\n\n\n\n\n\nUse the glimpse function to provide an overview of the data set\n\n\n\n\n\n\n\nImportant\n\n\n\nAll work will go inside the topic-ideas folder. Write your responses in topic-ideas.qmd and put the data sets in the data folder within topic-ideas.\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set.\n\n\n\n\n\nThe Topic Ideas portion of the project is worth 10 points. It will be graded based on meeting the criteria stated above for 2 proposed data sets.\n\n\n\n\nThe purpose of the project proposal is to help you think about your analysis strategy early and thoroughly explore the data.\nInclude the following in the proposal:\n\n\nThe introduction section includes\n\nan introduction to the subject matter you‚Äôre investigating\nthe motivation for your research question (citing any relevant literature)\nthe primary research question you are interested in exploring\nyour team‚Äôs hypotheses regarding the research question\n\n\n\n\nIn this section, you will describe the data set. This includes\n\ndescription of the observations in the data set\ndescription of how the data were originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\n\nIn this section, you will explore the data. This includes using , visualizations and summary statistics to describe the following:\n\nunivariate distribution of the response variable\nunivariate distributions of the potential predictor variables\nrelationships between the response and predictors\nrelationships between predictors\npotential interaction effects you‚Äôre interested in exploring\n\nIn this section, you will also describe any data cleaning you need to do to prepare for modeling, such as imputing missing values, collapsing levels for categorical predictors, creating new variables, summarizing data, etc.\n\n\n\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\ndescription of the response variable and list of potential predictors\nregression model technique (multiple linear regression or logistic regression)\n\n\n\n\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll work will go inside the proposal folder. Write your responses in proposal.qmd and put the data set and the data dictionary in the data folder.\n\nSubmit the PDF of the proposal to Gradescope. Mark all pages of the document.\n\n\n\n\n\nThe anticipated length, including all graphs, tables, narrative, etc., is 3 - 6 pages; it may not exceed 7 pages.\nThe proposal is with 15 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (14 - 15 points) : All required elements are completed and are accurate. There is a thorough exploration of the data and the team has demonstrated a careful and thoughtful approach exploring the data and preparing it for analysis. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong: (11 - 13 points) Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (8 - 10 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (7 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling.\n\n\n\n\n\nThe Round 1 submission is an opportunity to receive detailed feedback on your analysis and written report. The feedback will only be on the content that is submitted, so more ‚Äúcomplete‚Äù drafts will receive more detailed feedback. At this stage, you will also be notified of the grade you would receive at that point. You will have the option to keep the grade (and thus you don‚Äôt need to turn in an updated report) or resubmit the written report by the final submission deadline for grading.\nTo submit the draft:\n1. Push the updated written-report.qmd and written-report.pdf to your GitHub repo.\n2. Open an issue with the title ‚ÄúRound 1 Submission‚Äù. You can use the template issue in the GitHub repo. Make sure I am tagged in the issue (@matackett), so I receive notification of your Round 1 submission. See Creating an issue from a repository for instructions on opening an issue. Please ask a member of the teaching team for assistance if you need help opening the issue.\n\n\n\n\n\n\nImportant\n\n\n\nYou must complete both steps by Tuesday, November 22, 11:59pm to receive preliminary feedback.\n\nReports submitted after that date will not receive preliminary feedback.\n\n\nNote that this is optional, so there is no grading penalty for turning in nothing for the Round 1 submission. Due to time constraints at the end of the semester, only high-level feedback will be given for the reports submitted at the final written report deadline in December.\n\n\n\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\n\n\n\n\n\n\nNote\n\n\n\nBefore you finalize your write up, make sure the code chunks are not visible and all messages and warnings are suppressed.\n\n\nYou will submit the PDF of your final report on GitHub.\nThe PDF you submit must match the .qmd in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. There is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\n\n\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won‚Äôt fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you‚Äôre fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model‚Äôs predictive power is thoroughly assessed.\n\n\n\nIn this section you‚Äôll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nThis is an assessment of the overall presentation and formatting of the written report.\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe written report is due on Friday, December 09, 11:59pm and will be accepted with no late penalty until Sunday, December 11, 11:59pm.\nPush the file written-report.qmd and the rendered written-report.pdf go the GitHub repo by the deadline. You will &lt;u&gt;not submit the report on Gradescope.\nThe version of the report in the repo by the Sunday, December 11, 11:59pm will be the one that is graded.\n\n\n\n\n\n\n\n\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\n\n\n\n\nImportant\n\n\n\nCreate a presentation folder in your GitHub repo. Put a PDF of the slides in the presentation folder. The slides PDF of your slides must be in the GitHub repo by Wednesday, December 14, 11:59pm.\nYou will &lt;u&gt;not submit the slides on Gradescope.\n\n\n\n\n\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 7 minutes. It is fine if the video is shorter than 7 minutes, but it cannot exceed 7 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Sakai.\n\n\n\n\n\n\nNote\n\n\n\nNote: Every team member is expected to speak in the presentation. Part of the grade will be whether every group member had a meaningful speaking role in the presentation.\n\n\n\n\n\nClick the Warpwire tab in the Sakai site for your section.\nClick the ‚Äú+‚Äù and select ‚ÄúUpload files‚Äù.\nLocate the video on your computer and click to upload.\nOnce you‚Äôve uploaded the video to Warpwire, click to share the video and copy the video‚Äôs URL. You will need this when you post the video in the discussion forum.\n\n\n\n\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick ‚ÄúStart a new conversation‚Äù.\nMake the title ‚ÄúYour Team Name: Project Title‚Äù.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click ‚ÄúInsert 1 item.‚Äù This will embed your video in the conversation.\nUnder the video, paste the URL to your video. This is to ensure peers can see the presentation if they‚Äôre unable to view the embedded video.\nYou‚Äôre done!\n\nYou can see the Teaching Team example in Sakai.\n\n\n\n\n\n\nImportant\n\n\n\nThe presentation video must be uploaded to Sakai by Wednesday, December 14, 11:59pm.\n\n\n\n\n\n\n\nEach student will be assigned 2 presentations to watch. Click here to see your viewing assignments.\nWatch the group‚Äôs video, then click ‚ÄúReply‚Äù to post a question for the group. You may not post a question that‚Äôs already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e.¬†it shouldn‚Äôt be ‚ÄúWhy did you use a bar plot instead of a pie chart‚Äù?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group‚Äôs specific presentation, i.e demonstrating that you‚Äôve watched the presentation.\n\n\n\n\n\n\nImportant\n\n\n\nYou may start posting questions and comments on Thursday, December 15. All comments must be posted by Friday, December 16 at 11:59pm.\n\n\nThis portion of the project will be assessed individually.\n\n\n\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas, project-proposal , and written-report-comments.pdf (if applicable) files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n*.Rproj: File specifying the RStudio project\n.gitignore: File listing all files that are in the local RStudio project but not the GitHub repo\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable.\n\n\n\n\n\n\nImportant\n\n\n\nThe repo must be ready for grading by Wednesday, December 14, 11:59pm.\n\n\n\n\n\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\n\n\n\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n10 pts\n\n\nProject proposal\n15 pts\n\n\nWritten report\n45 pts\n\n\nSlides + video presentation\n15 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\n\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\n\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "project-instructions.html#introduction",
    "href": "project-instructions.html#introduction",
    "title": "Final project",
    "section": "",
    "text": "TL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group‚Äôs interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\n\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team‚Äôs project"
  },
  {
    "objectID": "project-instructions.html#topic-ideas",
    "href": "project-instructions.html#topic-ideas",
    "title": "Final project",
    "section": "",
    "text": "Identify 2 - 3 data sets you‚Äôre interested in potentially using for the final project. If you‚Äôre unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point. It may also help to think of topics you‚Äôre interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help set you up for a successful project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as ‚Äúname‚Äù, ‚Äúsocial security number‚Äù, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g.¬†‚Äústate abbreviation‚Äù and ‚Äústate name‚Äù), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTip\n\n\n\nAsk a member of the teaching team if you‚Äôre unsure whether your data set meets the criteria.\n\n\nFor each data set, include the following:\n\n\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\n\n\nDescribe a research question you‚Äôre interested in answering using this data.\n\n\n\n\n\nUse the glimpse function to provide an overview of the data set\n\n\n\n\n\n\n\nImportant\n\n\n\nAll work will go inside the topic-ideas folder. Write your responses in topic-ideas.qmd and put the data sets in the data folder within topic-ideas.\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set.\n\n\n\n\n\nThe Topic Ideas portion of the project is worth 10 points. It will be graded based on meeting the criteria stated above for 2 proposed data sets."
  },
  {
    "objectID": "project-instructions.html#project-proposal",
    "href": "project-instructions.html#project-proposal",
    "title": "Final project",
    "section": "",
    "text": "The purpose of the project proposal is to help you think about your analysis strategy early and thoroughly explore the data.\nInclude the following in the proposal:\n\n\nThe introduction section includes\n\nan introduction to the subject matter you‚Äôre investigating\nthe motivation for your research question (citing any relevant literature)\nthe primary research question you are interested in exploring\nyour team‚Äôs hypotheses regarding the research question\n\n\n\n\nIn this section, you will describe the data set. This includes\n\ndescription of the observations in the data set\ndescription of how the data were originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\n\nIn this section, you will explore the data. This includes using , visualizations and summary statistics to describe the following:\n\nunivariate distribution of the response variable\nunivariate distributions of the potential predictor variables\nrelationships between the response and predictors\nrelationships between predictors\npotential interaction effects you‚Äôre interested in exploring\n\nIn this section, you will also describe any data cleaning you need to do to prepare for modeling, such as imputing missing values, collapsing levels for categorical predictors, creating new variables, summarizing data, etc.\n\n\n\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\ndescription of the response variable and list of potential predictors\nregression model technique (multiple linear regression or logistic regression)\n\n\n\n\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll work will go inside the proposal folder. Write your responses in proposal.qmd and put the data set and the data dictionary in the data folder.\n\nSubmit the PDF of the proposal to Gradescope. Mark all pages of the document.\n\n\n\n\n\nThe anticipated length, including all graphs, tables, narrative, etc., is 3 - 6 pages; it may not exceed 7 pages.\nThe proposal is with 15 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (14 - 15 points) : All required elements are completed and are accurate. There is a thorough exploration of the data and the team has demonstrated a careful and thoughtful approach exploring the data and preparing it for analysis. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong: (11 - 13 points) Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (8 - 10 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (7 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling."
  },
  {
    "objectID": "project-instructions.html#draft-report",
    "href": "project-instructions.html#draft-report",
    "title": "Final project",
    "section": "",
    "text": "The Round 1 submission is an opportunity to receive detailed feedback on your analysis and written report. The feedback will only be on the content that is submitted, so more ‚Äúcomplete‚Äù drafts will receive more detailed feedback. At this stage, you will also be notified of the grade you would receive at that point. You will have the option to keep the grade (and thus you don‚Äôt need to turn in an updated report) or resubmit the written report by the final submission deadline for grading.\nTo submit the draft:\n1. Push the updated written-report.qmd and written-report.pdf to your GitHub repo.\n2. Open an issue with the title ‚ÄúRound 1 Submission‚Äù. You can use the template issue in the GitHub repo. Make sure I am tagged in the issue (@matackett), so I receive notification of your Round 1 submission. See Creating an issue from a repository for instructions on opening an issue. Please ask a member of the teaching team for assistance if you need help opening the issue.\n\n\n\n\n\n\nImportant\n\n\n\nYou must complete both steps by Tuesday, November 22, 11:59pm to receive preliminary feedback.\n\nReports submitted after that date will not receive preliminary feedback.\n\n\nNote that this is optional, so there is no grading penalty for turning in nothing for the Round 1 submission. Due to time constraints at the end of the semester, only high-level feedback will be given for the reports submitted at the final written report deadline in December."
  },
  {
    "objectID": "project-instructions.html#written-report",
    "href": "project-instructions.html#written-report",
    "title": "Final project",
    "section": "",
    "text": "Your written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\n\n\n\n\n\n\nNote\n\n\n\nBefore you finalize your write up, make sure the code chunks are not visible and all messages and warnings are suppressed.\n\n\nYou will submit the PDF of your final report on GitHub.\nThe PDF you submit must match the .qmd in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. There is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\n\n\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won‚Äôt fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you‚Äôre fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model‚Äôs predictive power is thoroughly assessed.\n\n\n\nIn this section you‚Äôll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nThis is an assessment of the overall presentation and formatting of the written report.\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe written report is due on Friday, December 09, 11:59pm and will be accepted with no late penalty until Sunday, December 11, 11:59pm.\nPush the file written-report.qmd and the rendered written-report.pdf go the GitHub repo by the deadline. You will &lt;u&gt;not submit the report on Gradescope.\nThe version of the report in the repo by the Sunday, December 11, 11:59pm will be the one that is graded."
  },
  {
    "objectID": "project-instructions.html#video-presentation-slides",
    "href": "project-instructions.html#video-presentation-slides",
    "title": "Final project",
    "section": "",
    "text": "In addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\n\n\n\n\nImportant\n\n\n\nCreate a presentation folder in your GitHub repo. Put a PDF of the slides in the presentation folder. The slides PDF of your slides must be in the GitHub repo by Wednesday, December 14, 11:59pm.\nYou will &lt;u&gt;not submit the slides on Gradescope.\n\n\n\n\n\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 7 minutes. It is fine if the video is shorter than 7 minutes, but it cannot exceed 7 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Sakai.\n\n\n\n\n\n\nNote\n\n\n\nNote: Every team member is expected to speak in the presentation. Part of the grade will be whether every group member had a meaningful speaking role in the presentation.\n\n\n\n\n\nClick the Warpwire tab in the Sakai site for your section.\nClick the ‚Äú+‚Äù and select ‚ÄúUpload files‚Äù.\nLocate the video on your computer and click to upload.\nOnce you‚Äôve uploaded the video to Warpwire, click to share the video and copy the video‚Äôs URL. You will need this when you post the video in the discussion forum.\n\n\n\n\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick ‚ÄúStart a new conversation‚Äù.\nMake the title ‚ÄúYour Team Name: Project Title‚Äù.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click ‚ÄúInsert 1 item.‚Äù This will embed your video in the conversation.\nUnder the video, paste the URL to your video. This is to ensure peers can see the presentation if they‚Äôre unable to view the embedded video.\nYou‚Äôre done!\n\nYou can see the Teaching Team example in Sakai.\n\n\n\n\n\n\nImportant\n\n\n\nThe presentation video must be uploaded to Sakai by Wednesday, December 14, 11:59pm."
  },
  {
    "objectID": "project-instructions.html#presentation-comments",
    "href": "project-instructions.html#presentation-comments",
    "title": "Final project",
    "section": "",
    "text": "Each student will be assigned 2 presentations to watch. Click here to see your viewing assignments.\nWatch the group‚Äôs video, then click ‚ÄúReply‚Äù to post a question for the group. You may not post a question that‚Äôs already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e.¬†it shouldn‚Äôt be ‚ÄúWhy did you use a bar plot instead of a pie chart‚Äù?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group‚Äôs specific presentation, i.e demonstrating that you‚Äôve watched the presentation.\n\n\n\n\n\n\nImportant\n\n\n\nYou may start posting questions and comments on Thursday, December 15. All comments must be posted by Friday, December 16 at 11:59pm.\n\n\nThis portion of the project will be assessed individually."
  },
  {
    "objectID": "project-instructions.html#reproducibility-organization",
    "href": "project-instructions.html#reproducibility-organization",
    "title": "Final project",
    "section": "",
    "text": "All written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas, project-proposal , and written-report-comments.pdf (if applicable) files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n*.Rproj: File specifying the RStudio project\n.gitignore: File listing all files that are in the local RStudio project but not the GitHub repo\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable.\n\n\n\n\n\n\nImportant\n\n\n\nThe repo must be ready for grading by Wednesday, December 14, 11:59pm."
  },
  {
    "objectID": "project-instructions.html#peer-teamwork-evaluation",
    "href": "project-instructions.html#peer-teamwork-evaluation",
    "title": "Final project",
    "section": "",
    "text": "You will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly."
  },
  {
    "objectID": "project-instructions.html#overall-grading",
    "href": "project-instructions.html#overall-grading",
    "title": "Final project",
    "section": "",
    "text": "The grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n10 pts\n\n\nProject proposal\n15 pts\n\n\nWritten report\n45 pts\n\n\nSlides + video presentation\n15 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\n\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\n\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you‚Äôll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you‚Äôll use for the course."
  },
  {
    "objectID": "slides/lab-02.html#reminders",
    "href": "slides/lab-02.html#reminders",
    "title": "Lab 02",
    "section": "Reminders",
    "text": "Reminders\n\n\nSelect the pages corresponding to each exercise when you when you submit the assignment on Gradescope.\n\nClick here for written and video instructions on submitting an assignment and marking pages on Gradescope.\n\nIn your write up:\n\nWrite all narrative in complete sentences.\nInclude an informative title and axis labels on graphs.\nWrite responses in the context of the data.\nDescribe distribution using shape, center, spread, and potential outliers. Describe relationships between variables using strength, direction, and shape."
  },
  {
    "objectID": "slides/lab-02.html#axis-labels-and-titles",
    "href": "slides/lab-02.html#axis-labels-and-titles",
    "title": "Lab 02",
    "section": "Axis labels and titles",
    "text": "Axis labels and titles\n\nBelow is a graph of association between flipper length in millimeters and body mass in grams of three species of penguins in Palmer Station, Antarctica. What are informative title and axis labels for this graph?"
  },
  {
    "objectID": "slides/lab-02.html#code-style",
    "href": "slides/lab-02.html#code-style",
    "title": "Lab 02",
    "section": "Code style",
    "text": "Code style\nWhich code chunk would you rather read?\n\n# code chunk 1\npenguins|&gt;filter(!is.na(flipper_length_mm))|&gt;group_by(species)|&gt;summarise(min=min(flipper_length_mm),mean=mean(flipper_length_mm),sd=sd(flipper_length_mm),max=max(flipper_length_mm),n=n())\n\n\n\n\n# code chunk 2\npenguins |&gt; \n  filter(!is.na(flipper_length_mm)) |&gt; \n  group_by(species) |&gt; \n  summarise(min = min(flipper_length_mm), \n            mean = mean(flipper_length_mm), \n            max = max(flipper_length_mm),\n            n = n())"
  },
  {
    "objectID": "slides/lab-02.html#code-style-contd",
    "href": "slides/lab-02.html#code-style-contd",
    "title": "Lab 02",
    "section": "Code style cont‚Äôd",
    "text": "Code style cont‚Äôd\nMake code easier to read and debug by\n\nPutting each element on a different line (start a new line after + and |&gt;)\nPutting spaces before and after operators (+, -, *, =, |&gt; )\nIn general, avoiding long lines of code, i.e.¬†lines longer than 120 characters.\n\nSee the Tidyverse Style Guide for more tips on code styling."
  },
  {
    "objectID": "slides/lab-02.html#todays-lab",
    "href": "slides/lab-02.html#todays-lab",
    "title": "Lab 02",
    "section": "Today‚Äôs lab",
    "text": "Today‚Äôs lab\n\nRemember to use a reproducible workflow with regular commits (and informative commit messages).\n\nPush all update files after each commit!\n\nUse lectures and AEs from Week 02 and Week 03 as reference as you complete the lab.\n\n\n\n\n\nüîó Week 03"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#announcements",
    "href": "slides/12-feature-engineering-pt2.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nClick here for slides from presentation about the Academic Resource Center.\nGroup labs start this week.\nUpcoming events:\n\nUConn Sports Analytics Symposium: October 08 (online options available)\nCode for Good hosted by Hack Duke: October 22 - 23\n\nClick here for Week 06 activities."
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#topics",
    "href": "slides/12-feature-engineering-pt2.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\n\nFeature engineering with recipes\nWorkflows to bring together models and recipes\nRMSE and \\(R^2\\) for model evaluation on training and tests sets"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#computational-setup",
    "href": "slides/12-feature-engineering-pt2.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#the-office",
    "href": "slides/12-feature-engineering-pt2.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#data-goal",
    "href": "slides/12-feature-engineering-pt2.html#data-goal",
    "title": "Feature engineering",
    "section": "Data & goal",
    "text": "Data & goal\n\nData: The data come from data.world, by way of TidyTuesday\nGoal: Predict imdb_rating from other variables in the dataset\n\n\n\n# A tibble: 188 √ó 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ‚Ä¶ with 178 more rows"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#train-test",
    "href": "slides/12-feature-engineering-pt2.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#training-data",
    "href": "slides/12-feature-engineering-pt2.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 √ó 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ä¶ with 131 more rows"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#recap-feature-engineering",
    "href": "slides/12-feature-engineering-pt2.html#recap-feature-engineering",
    "title": "Feature engineering",
    "section": "Recap: Feature engineering",
    "text": "Recap: Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#recap-modeling-workflow-revisited",
    "href": "slides/12-feature-engineering-pt2.html#recap-modeling-workflow-revisited",
    "title": "Feature engineering",
    "section": "Recap: Modeling workflow, revisited",
    "text": "Recap: Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#initiate-a-recipe",
    "href": "slides/12-feature-engineering-pt2.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-1-alter-roles",
    "href": "slides/12-feature-engineering-pt2.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn‚Äôt a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-2-add-features",
    "href": "slides/12-feature-engineering-pt2.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#working-with-recipes",
    "href": "slides/12-feature-engineering-pt2.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don‚Äôt get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\n\nNote\n\n\nThis is optional, we‚Äôll show the results for demonstrative purposes. It doesn‚Äôt need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-2-prep-and-bake",
    "href": "slides/12-feature-engineering-pt2.html#step-2-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 2: Prep and bake",
    "text": "Step 2: Prep and bake\n\n# determine required parameters to be estimated\noffice_rec_trained &lt;- prep(office_rec)\n\n# apply recipe computations to data\nbake(office_rec_trained, office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 8\n$ season         &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2‚Ä¶\n$ episode        &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20,‚Ä¶\n$ title          &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Review‚Ä¶\n$ total_votes    &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1‚Ä¶\n$ air_date       &lt;date&gt; 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-0‚Ä¶\n$ imdb_rating    &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, ‚Ä¶\n$ air_date_dow   &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, ‚Ä¶\n$ air_date_month &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr, Jan, May, ‚Ä¶"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-3-add-more-features",
    "href": "slides/12-feature-engineering-pt2.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-3-prep-and-bake",
    "href": "slides/12-feature-engineering-pt2.html#step-3-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 3: Prep and bake",
    "text": "Step 3: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7‚Ä¶\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26‚Ä¶\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor‚Ä¶\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2‚Ä¶\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0‚Ä¶\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu‚Ä¶\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr‚Ä¶\n$ air_date_USThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USChristmasDay    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USNewYearsDay     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USIndependenceDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-4-convert-numbers-to-factors",
    "href": "slides/12-feature-engineering-pt2.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-4-prep-and-bake",
    "href": "slides/12-feature-engineering-pt2.html#step-4-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 4: Prep and bake",
    "text": "Step 4: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;fct&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7‚Ä¶\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26‚Ä¶\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor‚Ä¶\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2‚Ä¶\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0‚Ä¶\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu‚Ä¶\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr‚Ä¶\n$ air_date_USThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USChristmasDay    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USNewYearsDay     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USIndependenceDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-5-make-dummy-variables",
    "href": "slides/12-feature-engineering-pt2.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-5-prep-and-bake",
    "href": "slides/12-feature-engineering-pt2.html#step-5-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 5: Prep and bake",
    "text": "Step 5: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 33\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26‚Ä¶\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor‚Ä¶\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2‚Ä¶\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0‚Ä¶\n$ air_date_USThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USChristmasDay    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USNewYearsDay     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_USIndependenceDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X2                  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X3                  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X4                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ season_X5                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0‚Ä¶\n$ season_X6                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n$ season_X7                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1‚Ä¶\n$ season_X8                  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0‚Ä¶\n$ season_X9                  &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0‚Ä¶\n$ air_date_dow_Mon           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Tue           &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Wed           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Thu           &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ air_date_dow_Fri           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_dow_Sat           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Feb         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Mar         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Apr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1‚Ä¶\n$ air_date_month_May         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0‚Ä¶\n$ air_date_month_Jun         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Jul         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Aug         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Sep         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n$ air_date_month_Oct         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Nov         &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ air_date_month_Dec         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-6-remove-zero-variance-predictors",
    "href": "slides/12-feature-engineering-pt2.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#step-6-prep-and-bake",
    "href": "slides/12-feature-engineering-pt2.html#step-6-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 6: Prep and bake",
    "text": "Step 6: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 22\n$ episode            &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,‚Ä¶\n$ title              &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Re‚Ä¶\n$ total_votes        &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144‚Ä¶\n$ imdb_rating        &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8‚Ä¶\n$ season_X2          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X3          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X4          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X5          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, ‚Ä¶\n$ season_X6          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ‚Ä¶\n$ season_X7          &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ‚Ä¶\n$ season_X8          &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ season_X9          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ air_date_dow_Tue   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_dow_Thu   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ air_date_month_Feb &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_month_Mar &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_month_Apr &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ‚Ä¶\n$ air_date_month_May &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ air_date_month_Sep &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ‚Ä¶\n$ air_date_month_Oct &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ‚Ä¶\n$ air_date_month_Nov &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ‚Ä¶\n$ air_date_month_Dec &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#putting-it-all-together",
    "href": "slides/12-feature-engineering-pt2.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#putting-it-all-together-1",
    "href": "slides/12-feature-engineering-pt2.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#recipe-workflow",
    "href": "slides/12-feature-engineering-pt2.html#recipe-workflow",
    "title": "Feature engineering",
    "section": "Recipe workflow",
    "text": "Recipe workflow\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#specify-model",
    "href": "slides/12-feature-engineering-pt2.html#specify-model",
    "title": "Feature engineering",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#build-workflow",
    "href": "slides/12-feature-engineering-pt2.html#build-workflow",
    "title": "Feature engineering",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\noffice_wflow &lt;- workflow() |&gt;\n  add_model(office_spec) |&gt;\n  add_recipe(office_rec)\n\n\nSee next slide for workflow‚Ä¶"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#view-workflow",
    "href": "slides/12-feature-engineering-pt2.html#view-workflow",
    "title": "Feature engineering",
    "section": "View workflow",
    "text": "View workflow\n\noffice_wflow\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_date()\n‚Ä¢ step_holiday()\n‚Ä¢ step_num2factor()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#fit-model-to-training-data",
    "href": "slides/12-feature-engineering-pt2.html#fit-model-to-training-data",
    "title": "Feature engineering",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\noffice_fit &lt;- office_wflow |&gt;\n  fit(data = office_train)\n\ntidy(office_fit)\n\n# A tibble: 21 √ó 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  6.40     0.510        12.5   1.51e-23\n 2 episode     -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes  0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2    0.811    0.327         2.48  1.44e- 2\n 5 season_X3    1.04     0.343         3.04  2.91e- 3\n 6 season_X4    1.09     0.295         3.70  3.32e- 4\n 7 season_X5    1.08     0.348         3.11  2.34e- 3\n 8 season_X6    1.00     0.367         2.74  7.18e- 3\n 9 season_X7    1.02     0.352         2.89  4.52e- 3\n10 season_X8    0.497    0.348         1.43  1.55e- 1\n# ‚Ä¶ with 11 more rows\n\n\n\n\nSo many predictors!"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#model-fit-summary",
    "href": "slides/12-feature-engineering-pt2.html#model-fit-summary",
    "title": "Feature engineering",
    "section": "Model fit summary",
    "text": "Model fit summary\n\ntidy(office_fit) |&gt; print(n = 21)\n\n# A tibble: 21 √ó 5\n   term                estimate std.error statistic  p.value\n   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)         6.40     0.510        12.5   1.51e-23\n 2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2           0.811    0.327         2.48  1.44e- 2\n 5 season_X3           1.04     0.343         3.04  2.91e- 3\n 6 season_X4           1.09     0.295         3.70  3.32e- 4\n 7 season_X5           1.08     0.348         3.11  2.34e- 3\n 8 season_X6           1.00     0.367         2.74  7.18e- 3\n 9 season_X7           1.02     0.352         2.89  4.52e- 3\n10 season_X8           0.497    0.348         1.43  1.55e- 1\n11 season_X9           0.621    0.345         1.80  7.41e- 2\n12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#application-exercise",
    "href": "slides/12-feature-engineering-pt2.html#application-exercise",
    "title": "Feature engineering",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 08: Feature engineering"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#make-predictions-for-training-data",
    "href": "slides/12-feature-engineering-pt2.html#make-predictions-for-training-data",
    "title": "Feature engineering",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\noffice_train_pred &lt;- predict(office_fit, office_train) |&gt;\n  bind_cols(office_train)\n\noffice_train_pred\n\n# A tibble: 141 √ó 7\n   .pred season episode title               imdb_rating total_votes air_date  \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1  7.57      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2  7.77      9      14 Vandalism                   7.6        1402 2013-01-31\n 3  8.31      2       8 Performance Review          8.2        2416 2005-11-15\n 4  7.67      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5  8.84      3      22 Beach Games                 9.1        2783 2007-05-10\n 6  8.33      7       1 Nepotism                    8.4        1897 2010-09-23\n 7  8.46      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8  8.14      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9  7.87      9      18 Promos                      8          1445 2013-04-04\n10  7.74      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ä¶ with 131 more rows"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#r-squared",
    "href": "slides/12-feature-engineering-pt2.html#r-squared",
    "title": "Feature engineering",
    "section": "R-squared",
    "text": "R-squared\nPercentage of variability in the IMDB ratings explained by the model.\n\n\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.670\n\n\n\n\n\nAre models with high or low \\(R^2\\) more preferable?"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#rmse",
    "href": "slides/12-feature-engineering-pt2.html#rmse",
    "title": "Feature engineering",
    "section": "RMSE",
    "text": "RMSE\nAn alternative model performance statistic: root mean square error.\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} \\]\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\n\nAre models with high or low RMSE are more preferable?"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#interpreting-rmse",
    "href": "slides/12-feature-engineering-pt2.html#interpreting-rmse",
    "title": "Feature engineering",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\n\nIs this RMSE considered low or high?\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\nDepends‚Ä¶\n\noffice_train |&gt;\n  summarise(min = min(imdb_rating), max = max(imdb_rating))\n\n# A tibble: 1 √ó 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#but-really",
    "href": "slides/12-feature-engineering-pt2.html#but-really",
    "title": "Feature engineering",
    "section": "But, really‚Ä¶",
    "text": "But, really‚Ä¶\nwho cares about predictions on training data?"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#make-predictions-for-testing-data",
    "href": "slides/12-feature-engineering-pt2.html#make-predictions-for-testing-data",
    "title": "Feature engineering",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\noffice_test_pred &lt;- predict(office_fit, office_test) |&gt;\n  bind_cols(office_test)\n\noffice_test_pred\n\n# A tibble: 47 √ó 7\n   .pred season episode title               imdb_rating total_votes air_date  \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1  8.03      1       2 Diversity Day               8.3        3566 2005-03-29\n 2  7.98      1       3 Health Care                 7.9        2983 2005-04-05\n 3  8.41      2       4 The Fire                    8.4        2713 2005-10-11\n 4  8.35      2       5 Halloween                   8.2        2561 2005-10-18\n 5  8.35      2       9 E-Mail Surveillance         8.4        2527 2005-11-22\n 6  8.68      2      12 The Injury                  9          3282 2006-01-12\n 7  8.32      2      14 The Carpet                  7.9        2342 2006-01-26\n 8  8.93      2      22 Casino Night                9.3        3644 2006-05-11\n 9  8.80      3       1 Gay Witch Hunt              8.9        3087 2006-09-21\n10  8.37      3       5 Initiation                  8.2        2254 2006-10-19\n# ‚Ä¶ with 37 more rows"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#evaluate-performance-for-testing-data",
    "href": "slides/12-feature-engineering-pt2.html#evaluate-performance-for-testing-data",
    "title": "Feature engineering",
    "section": "Evaluate performance for testing data",
    "text": "Evaluate performance for testing data\nRMSE of model fit to testing data\n\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.411\n\n\nR-sq of model fit to testing data\n\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.468"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#training-vs.-testing",
    "href": "slides/12-feature-engineering-pt2.html#training-vs.-testing",
    "title": "Feature engineering",
    "section": "Training vs.¬†testing",
    "text": "Training vs.¬†testing\n\n\n\n\n\n\n\n\n\nmetric\ntrain\ntest\ncomparison\n\n\n\n\nRMSE\n0.302\n0.411\nRMSE lower for training\n\n\nR-squared\n0.67\n0.468\nR-squared higher for training"
  },
  {
    "objectID": "slides/12-feature-engineering-pt2.html#evaluating-performance-on-training-data",
    "href": "slides/12-feature-engineering-pt2.html#evaluating-performance-on-training-data",
    "title": "Feature engineering",
    "section": "Evaluating performance on training data",
    "text": "Evaluating performance on training data\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.\n\n\n\n\nüîó Week 06"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#announcements",
    "href": "slides/09-mlr-predictors.html#announcements",
    "title": "MLR: Types of predictors",
    "section": "Announcements",
    "text": "Announcements\n\nLab 03 due\n\nToday at 11:59pm (Thursday labs)\nTue, Sep 27 at 11:59pm (Friday labs)\n\nExam 01: Sep 28 - 30\n\nExam 01 review on Sep 28\nVideos for Weeks 01 - 05 available until Sep 28 at 11:59pm\n\nSee Week 05 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#topics",
    "href": "slides/09-mlr-predictors.html#topics",
    "title": "MLR: Types of predictors",
    "section": "Topics",
    "text": "Topics\n\nPrediction for multiple linear regression\nTypes of predictors for multiple linear regression\nMean-centering quantitative predictors\nUsing indicator variables for categorical predictors\nUsing interaction terms"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#computational-setup",
    "href": "slides/09-mlr-predictors.html#computational-setup",
    "title": "MLR: Types of predictors",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#the-data",
    "href": "slides/09-mlr-predictors.html#the-data",
    "title": "MLR: Types of predictors",
    "section": "The data",
    "text": "The data\n\nlevittown &lt;- read_csv(here::here(\"slides/data/homeprices.csv\"))\nlevittown\n\n# A tibble: 85 √ó 7\n   bedrooms bathrooms living_area lot_size year_built property_tax sale_price\n      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1        4       1          1380     6000       1948         8360     350000\n 2        4       2          1761     7400       1951         5754     360000\n 3        4       2          1564     6000       1948         8982     350000\n 4        5       2          2904     9898       1949        11664     375000\n 5        5       2.5        1942     7788       1948         8120     370000\n 6        4       2          1830     6000       1948         8197     335000\n 7        4       1          1585     6000       1948         6223     295000\n 8        4       1           941     6800       1951         2448     250000\n 9        4       1.5        1481     6000       1948         9087     299990\n10        3       2          1630     5998       1948         9430     375000\n# ‚Ä¶ with 75 more rows"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#variables",
    "href": "slides/09-mlr-predictors.html#variables",
    "title": "MLR: Types of predictors",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nliving_area: Total living area of the house (in square feet)\nlot_size: Total area of the lot (in square feet)\nyear_built: Year the house was built\nproperty_tax: Annual property taxes (in USD)\n\n\nResponse: sale_price: Sales price (in USD)"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#model-fit",
    "href": "slides/09-mlr-predictors.html#model-fit",
    "title": "MLR: Types of predictors",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7148818.957\n3820093.694\n-1.871\n0.065\n\n\nbedrooms\n-12291.011\n9346.727\n-1.315\n0.192\n\n\nbathrooms\n51699.236\n13094.170\n3.948\n0.000\n\n\nliving_area\n65.903\n15.979\n4.124\n0.000\n\n\nlot_size\n-0.897\n4.194\n-0.214\n0.831\n\n\nyear_built\n3760.898\n1962.504\n1.916\n0.059\n\n\nproperty_tax\n1.476\n2.832\n0.521\n0.604"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#prediction-1",
    "href": "slides/09-mlr-predictors.html#prediction-1",
    "title": "MLR: Types of predictors",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1,050 square feet of living area, 6,000 square foot lot size, built in 1948 with $6,306 in property taxes?\n\n\n\n-7148818.957 - 12291.011 * 3 + 51699.236 * 1 + \n  65.903 * 1050 - 0.897 * 6000 + 3760.898 * 1948 + \n  1.476 * 6306\n\n[1] 265360.4\n\n\n\nThe predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes is $265,360."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#prediction-revisit",
    "href": "slides/09-mlr-predictors.html#prediction-revisit",
    "title": "MLR: Types of predictors",
    "section": "Prediction, revisit",
    "text": "Prediction, revisit\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_house &lt;- tibble(\n  bedrooms = 3, bathrooms = 1, \n  living_area = 1050, lot_size = 6000, \n  year_built = 1948, property_tax = 6306\n  )\n\npredict(price_fit, new_house)\n\n# A tibble: 1 √ó 1\n    .pred\n    &lt;dbl&gt;\n1 265360."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#confidence-interval-for-hatmu_y",
    "href": "slides/09-mlr-predictors.html#confidence-interval-for-hatmu_y",
    "title": "MLR: Types of predictors",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 95% confidence interval for the estimated mean price of houses in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     238482.     292239."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#prediction-interval-for-haty",
    "href": "slides/09-mlr-predictors.html#prediction-interval-for-haty",
    "title": "MLR: Types of predictors",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 95% prediction interval for an individual house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     167277.     363444."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#cautions",
    "href": "slides/09-mlr-predictors.html#cautions",
    "title": "MLR: Types of predictors",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#data-peer-to-peer-lender",
    "href": "slides/09-mlr-predictors.html#data-peer-to-peer-lender",
    "title": "MLR: Types of predictors",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday‚Äôs data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 √ó 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ‚Ä¶ with 40 more rows"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#variables-1",
    "href": "slides/09-mlr-predictors.html#variables-1",
    "title": "MLR: Types of predictors",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#outcome-interest_rate",
    "href": "slides/09-mlr-predictors.html#outcome-interest_rate",
    "title": "MLR: Types of predictors",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#predictors",
    "href": "slides/09-mlr-predictors.html#predictors",
    "title": "MLR: Types of predictors",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#data-manipulation-1-rescale-income",
    "href": "slides/09-mlr-predictors.html#data-manipulation-1-rescale-income",
    "title": "MLR: Types of predictors",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#outcome-vs.-predictors",
    "href": "slides/09-mlr-predictors.html#outcome-vs.-predictors",
    "title": "MLR: Types of predictors",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#fit-regression-model",
    "href": "slides/09-mlr-predictors.html#fit-regression-model",
    "title": "MLR: Types of predictors",
    "section": "Fit regression model",
    "text": "Fit regression model\n\nint_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n      data = loan50)"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#summarize-model-results",
    "href": "slides/09-mlr-predictors.html#summarize-model-results",
    "title": "MLR: Types of predictors",
    "section": "Summarize model results",
    "text": "Summarize model results\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#mean-centering",
    "href": "slides/09-mlr-predictors.html#mean-centering",
    "title": "MLR: Types of predictors",
    "section": "Mean-centering",
    "text": "Mean-centering\nIf we are interested in interpreting the intercept, we can mean-center the quantitative predictors in the model.\nWe can mean-center a quantitative predictor \\(X_j\\) using the following:\n\\[X_{j_{Cent}} = X_{j}- \\bar{X}_{j}\\]\n\nIf we mean-center all quantitative variables, then the intercept is interpreted as the expected value of the response variable when all quantitative variables are at their mean value."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/09-mlr-predictors.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "MLR: Types of predictors",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#visualize-mean-centered-predictors",
    "href": "slides/09-mlr-predictors.html#visualize-mean-centered-predictors",
    "title": "MLR: Types of predictors",
    "section": "Visualize mean-centered predictors",
    "text": "Visualize mean-centered predictors"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#using-mean-centered-variables-in-the-model",
    "href": "slides/09-mlr-predictors.html#using-mean-centered-variables-in-the-model",
    "title": "MLR: Types of predictors",
    "section": "Using mean-centered variables in the model",
    "text": "Using mean-centered variables in the model\n\nHow do you expect the model to change if we use the debt_inc_cent and annual_income_cent in the model?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#original-vs.-mean-centered-model",
    "href": "slides/09-mlr-predictors.html#original-vs.-mean-centered-model",
    "title": "MLR: Types of predictors",
    "section": "Original vs.¬†mean-centered model",
    "text": "Original vs.¬†mean-centered model\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n10.726\n\n\ndebt_to_income\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th\n-0.021\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n9.444\n\n\ndebt_inc_cent\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th_cent\n-0.021"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#indicator-variables-1",
    "href": "slides/09-mlr-predictors.html#indicator-variables-1",
    "title": "MLR: Types of predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/09-mlr-predictors.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "MLR: Types of predictors",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 √ó 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#indicators-in-the-model",
    "href": "slides/09-mlr-predictors.html#indicators-in-the-model",
    "title": "MLR: Types of predictors",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn‚Äôt have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 √ó 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#interpreting-verified_income",
    "href": "slides/09-mlr-predictors.html#interpreting-verified_income",
    "title": "MLR: Types of predictors",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nThe baseline category is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\nPeople with verified income are expected to take a loan with an interest rate that is 6.880% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#interaction-terms-1",
    "href": "slides/09-mlr-predictors.html#interaction-terms-1",
    "title": "MLR: Types of predictors",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#interest-rate-vs.-annual-income",
    "href": "slides/09-mlr-predictors.html#interest-rate-vs.-annual-income",
    "title": "MLR: Types of predictors",
    "section": "Interest rate vs.¬†annual income",
    "text": "Interest rate vs.¬†annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#interaction-term-in-model",
    "href": "slides/09-mlr-predictors.html#interaction-term-in-model",
    "title": "MLR: Types of predictors",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent + verified_income + annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#interpreting-interaction-terms",
    "href": "slides/09-mlr-predictors.html#interpreting-interaction-terms",
    "title": "MLR: Types of predictors",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/09-mlr-predictors.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/09-mlr-predictors.html#data-manipulation-4-create-interaction-variables",
    "title": "MLR: Types of predictors",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, ‚Ä¶\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.‚Ä¶\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, ‚Ä¶\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, ‚Ä¶\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, ‚Ä¶\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,‚Ä¶\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0‚Ä¶\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11‚Ä¶"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#recap",
    "href": "slides/09-mlr-predictors.html#recap",
    "title": "MLR: Types of predictors",
    "section": "Recap",
    "text": "Recap\n\nMean-centering quantitative predictors\nUsing indicator variables for categorical predictors\nUsing interaction terms"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#looking-backward",
    "href": "slides/09-mlr-predictors.html#looking-backward",
    "title": "MLR: Types of predictors",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):\n\nloan50 |&gt;\n  select(interest_rate, annual_income, debt_to_income, verified_income) |&gt;\n  mutate(\n    # 1. rescale income\n    annual_income_th = annual_income / 1000,\n    # 2. mean-center quantitative predictors\n    debt_inc_cent = debt_to_income - mean(debt_to_income),\n    annual_income_th_cent = annual_income_th - mean(annual_income_th),\n    # 3. create dummy variables for verified_income\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0),\n    # 4. create interaction variables\n    `annual_income_th_cent:verified_incomeSource Verified` = annual_income_th_cent * source_verified,\n    `annual_income_th_cent:verified_incomeVerified` = annual_income_th_cent * verified\n  )"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#looking-forward",
    "href": "slides/09-mlr-predictors.html#looking-forward",
    "title": "MLR: Types of predictors",
    "section": "Looking forward",
    "text": "Looking forward\nFeature engineering, with recipes (from tidymodels):\n\nloan_rec &lt;- recipe( ~ ., data = loan50) |&gt;\n  # 1. rescale income\n  step_mutate(annual_income_th = annual_income / 1000) |&gt;\n  # 2. mean-center quantitative predictors\n  step_center(all_numeric_predictors()) |&gt;\n  # 3. create dummy variables for verified_income\n  step_dummy(verified_income) |&gt;\n  # 4. create interaction variables\n  step_interact(terms = ~ annual_income_th:verified_income)"
  },
  {
    "objectID": "slides/09-mlr-predictors.html#recipe",
    "href": "slides/09-mlr-predictors.html#recipe",
    "title": "MLR: Types of predictors",
    "section": "Recipe",
    "text": "Recipe\n\nloan_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n predictor         24\n\nOperations:\n\nVariable mutation for annual_income / 1000\nCentering for all_numeric_predictors()\nDummy variables from verified_income\nInteractions with annual_income_th:verified_income\n\n\n\n\n\nüîó Week 05"
  },
  {
    "objectID": "slides/lab-00.html#meet-your-ta",
    "href": "slides/lab-00.html#meet-your-ta",
    "title": "Welcome to STA 210 Labs!",
    "section": "Meet your TA!",
    "text": "Meet your TA!"
  },
  {
    "objectID": "slides/lab-00.html#meet-each-other",
    "href": "slides/lab-00.html#meet-each-other",
    "title": "Welcome to STA 210 Labs!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\nGet into groups of 4 - 5.\nIntroduce yourself - Name, year, major\nChoose a reporter\n\nNeed help choosing? Person with birthday closest to today‚Äôs date.\n\nIdentify 8 things everyone in the group has in common\n\nNot being a Duke student\nNot clothes (we‚Äôre all wearing socks)\nNot body parts (we all have a nose)\n\nReporter will share list with the class.\n\n\n\n\n\n06:00"
  },
  {
    "objectID": "slides/lab-00.html#what-to-expect-in-lab",
    "href": "slides/lab-00.html#what-to-expect-in-lab",
    "title": "Welcome to STA 210 Labs!",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nWork on the lab assignment (individual at first, but in teams for the rest of the semester)\nLab instructions will be posted on the course website\nStart each lab by finding your assignment repo in the course GitHub organization\n\nMore on the computing tools in Monday‚Äôs lecture"
  },
  {
    "objectID": "slides/lab-00.html#for-todays-lab",
    "href": "slides/lab-00.html#for-todays-lab",
    "title": "Welcome to STA 210 Labs!",
    "section": "For today‚Äôs lab",
    "text": "For today‚Äôs lab\n\nComplete the STA 210 Student Survey (will ask for a GitHub username)\n\nClick here for information on registering for a GitHub account and choosing a username.\n\nReserve a STA 210 Docker Container\n\nMake sure to reserve the container titled ‚ÄúSTA210‚Äù, not ‚ÄúRStudio‚Äù\n\n\n\n\n\nüîó Week 01"
  },
  {
    "objectID": "slides/lab-05.html#goals",
    "href": "slides/lab-05.html#goals",
    "title": "Lab 05",
    "section": "Goals",
    "text": "Goals\n\nReview interaction effects\nLab 05: Halloween candy"
  },
  {
    "objectID": "slides/lab-05.html#diamonds-data",
    "href": "slides/lab-05.html#diamonds-data",
    "title": "Lab 05",
    "section": "Diamonds data",
    "text": "Diamonds data\nThe data contain the price and other attributes of a random sample of 1000 diamonds. This sample was drawn from the diamonds data frame in the tidyverse R package.\nThe variables in this example are\n\nprice: Price in US dollars\ncarat: Weight of the diamond (units = carats)\ncut: Quality of the cut of the diamond (Fair, Good, Ideal)\n\nThe goal is to use the carat and cut to understand variability in the price."
  },
  {
    "objectID": "slides/lab-05.html#exploratory-data-analysis",
    "href": "slides/lab-05.html#exploratory-data-analysis",
    "title": "Lab 05",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nDoes the plot show evidence of an interaction effect between carats and cut?"
  },
  {
    "objectID": "slides/lab-05.html#model",
    "href": "slides/lab-05.html#model",
    "title": "Lab 05",
    "section": "Model",
    "text": "Model\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-2473.555\n\n\ncarat\n6383.913\n\n\ncutGood\n60.018\n\n\ncutIdeal\n0.044\n\n\ncarat:cutGood\n982.952\n\n\ncarat:cutIdeal\n2132.642\n\n\n\n\n\n\n\n\n\nInterpret the coefficient of cutGood in the context of the data.\nInterpret the coefficient of carat:cutGood in the context of the data.\nSuppose we fit a model for Fair cut diamonds. What is the intercept? What is the slope of carat ?\nSuppose we fit a model for Ideal cut diamonds. What is the intercept? What is the slope of carat?"
  },
  {
    "objectID": "slides/lab-05.html#resources-for-lab-05",
    "href": "slides/lab-05.html#resources-for-lab-05",
    "title": "Lab 05",
    "section": "Resources for Lab 05",
    "text": "Resources for Lab 05\n\nLecture notes:\n\nFeature engineering\nFeature engineering: Model workflow\nModel comparison\n\nRecipes package function reference\nTidy Modeling in R - Chapter 8: Feature engineering with recipes\n\n\n\n\nüîó Week 08"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#announcements",
    "href": "slides/25-multilevel-models-pt2.html#announcements",
    "title": "Fitting multilevel models",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nStatistics experience due Fri, Dec 09, 11:59pm\nProject written report due Fri, Dec 09, 11:59pm\nTeam Feedback #2 due Tue, Dec 06, 11:59pm (check for email from Teammates)\n\nExam 02: Mon, Dec 05 (evening) - Thu, Dec 08, 12pm (noon)\n\nExam 02 review on Mon Dec 05\nClick here for lecture recordings - available until Dec 05, 11:59pm\n\nSee Week 14 activities"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#learning-goals",
    "href": "slides/25-multilevel-models-pt2.html#learning-goals",
    "title": "Fitting multilevel models",
    "section": "Learning goals",
    "text": "Learning goals\n\nUnderstand how multilevel model can be used to take correlation into account\nInterpret fixed effects of multilevel model\nFit multilevel model in R"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#data-music-performance-anxiety",
    "href": "slides/25-multilevel-models-pt2.html#data-music-performance-anxiety",
    "title": "Fitting multilevel models",
    "section": "Data: Music performance anxiety",
    "text": "Data: Music performance anxiety\nThe data musicdata.csv come from the Sadler and Miller (2010) study of the emotional state of musicians before performances. The dataset contains information collected from 37 undergraduate music majors who completed the Positive Affect Negative Affect Schedule (PANAS), an instrument produces a measure of anxiety (negative affect) and a measure of happiness (positive affect). This analysis will focus on negative affect as a measure of performance anxiety."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#data-music-performance-anxiety-1",
    "href": "slides/25-multilevel-models-pt2.html#data-music-performance-anxiety-1",
    "title": "Fitting multilevel models",
    "section": "Data: Music performance anxiety",
    "text": "Data: Music performance anxiety\nThe primary variables we‚Äôll use are\n\nna: negative affect score on PANAS (the response variable)\nperform_type: type of performance (Solo, Large Ensemble, Small Ensemble)\n\nCreate variable large_ensemble: 1 if large ensemble performance, 0 otherwise\n\ninstrument: type of instrument (Voice, Orchestral, Piano)\n\nCreate variable orchestra: 1 if orchestral instrument, 0 otherwise"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#look-at-data",
    "href": "slides/25-multilevel-models-pt2.html#look-at-data",
    "title": "Fitting multilevel models",
    "section": "Look at data",
    "text": "Look at data\n\n\n\n\n\nid\ndiary\nlarge_ensemble\norchestra\nna\n\n\n\n\n1\n1\n0\n0\n11\n\n\n1\n2\n1\n0\n19\n\n\n1\n3\n1\n0\n14\n\n\n43\n1\n0\n0\n19\n\n\n43\n2\n0\n0\n13\n\n\n43\n3\n0\n0\n19\n\n\n\n\n\n\nDraw the data structure, and add the Level One and Level Two observational units and variables."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#unviariate-eda",
    "href": "slides/25-multilevel-models-pt2.html#unviariate-eda",
    "title": "Fitting multilevel models",
    "section": "Unviariate EDA",
    "text": "Unviariate EDA"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#bivariate-eda",
    "href": "slides/25-multilevel-models-pt2.html#bivariate-eda",
    "title": "Fitting multilevel models",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#bivariate-eda-1",
    "href": "slides/25-multilevel-models-pt2.html#bivariate-eda-1",
    "title": "Fitting multilevel models",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#questions-we-want-to-answer",
    "href": "slides/25-multilevel-models-pt2.html#questions-we-want-to-answer",
    "title": "Fitting multilevel models",
    "section": "Questions we want to answer",
    "text": "Questions we want to answer\nThe goal is to understand variability in performance anxiety (na) based on performance-level and musician-level characteristics. Specifically:\n\nWhat is the association between performance type (large ensemble or not) and performance anxiety? Does the association differ based on instrument type (orchestral or not)?"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#modeling-workflow",
    "href": "slides/25-multilevel-models-pt2.html#modeling-workflow",
    "title": "Fitting multilevel models",
    "section": "Modeling workflow",
    "text": "Modeling workflow\nWe will fit the model in two parts:\n1Ô∏è‚É£ Fit a separate model for each musician understand the association between performance type and anxiety (Level One models).\n2Ô∏è‚É£ Then fit a system of models to predict the fitted coefficients in the Level One models based on instrument type (Level Two models).\n\n\nHow many Level One models will we fit?\nHow many Level Two models will we fit?"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#level-one-model",
    "href": "slides/25-multilevel-models-pt2.html#level-one-model",
    "title": "Fitting multilevel models",
    "section": "1Ô∏è‚É£ Level One model",
    "text": "1Ô∏è‚É£ Level One model\nWe‚Äôll start with the Level One model to understand the association between performance type and performance anxiety for the \\(i^{th}\\) musician.\n\n\\[na_{ij} = a_i + b_i ~ LargeEnsemble_{ij} + \\epsilon_i, \\hspace{5mm} \\epsilon_{ij} \\sim N(0,\\sigma^2)\\]\n\nWhy is it more meaningful to use performance type for the Level One model than instrument?\n\n\nFor now, estimate \\(a_i\\) and \\(b_i\\) using least-squares regression."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#level-one-model-for-one-student",
    "href": "slides/25-multilevel-models-pt2.html#level-one-model-for-one-student",
    "title": "Fitting multilevel models",
    "section": "Level One model for one student",
    "text": "Level One model for one student\nBelow is partial data for observation #22\n\n\n\n\n\nid\ndiary\nlarge_ensemble\norchestra\nna\n\n\n\n\n22\n1\n0\n1\n24\n\n\n22\n2\n1\n1\n21\n\n\n22\n3\n1\n1\n14\n\n\n22\n13\n1\n1\n12\n\n\n22\n14\n1\n1\n19\n\n\n22\n15\n0\n1\n25"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#level-one-model-for-musician-22",
    "href": "slides/25-multilevel-models-pt2.html#level-one-model-for-musician-22",
    "title": "Fitting multilevel models",
    "section": "Level One model for musician 22",
    "text": "Level One model for musician 22\n\nid_22 &lt;- music |&gt;\n  filter(id == 22)\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(na ~ large_ensemble, data = id_22) |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.500\n1.96\n12.503\n0.000\n\n\nlarge_ensemble\n-7.833\n2.53\n-3.097\n0.009"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#application-exercise",
    "href": "slides/25-multilevel-models-pt2.html#application-exercise",
    "title": "Fitting multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nSee Part 3: Level One Models to fit the Level One model for all 37 musicians."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#level-one-model-summaries",
    "href": "slides/25-multilevel-models-pt2.html#level-one-model-summaries",
    "title": "Fitting multilevel models",
    "section": "Level One model summaries",
    "text": "Level One model summaries\n\nRecreated from BMLR Figure 8.9\nNow let‚Äôs consider if there is an association between the estimated slopes, estimated intercepts, and the type of instrument."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#level-two-model",
    "href": "slides/25-multilevel-models-pt2.html#level-two-model",
    "title": "Fitting multilevel models",
    "section": "Level Two Model",
    "text": "Level Two Model\nThe slope and intercept for the \\(i^{th}\\) musician can be modeled as\n\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i + u_i \\\\\n&b_i = \\beta_0 + \\beta_1 ~ Orchestra_i + v_i\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\nThe response variable in the Level Two models are not observed outcomes but the (fitted) slope and intercept from each musician"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#application-exercise-1",
    "href": "slides/25-multilevel-models-pt2.html#application-exercise-1",
    "title": "Fitting multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nSee Part 4: Level Two Models."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#estimated-coefficients-by-instrument",
    "href": "slides/25-multilevel-models-pt2.html#estimated-coefficients-by-instrument",
    "title": "Fitting multilevel models",
    "section": "Estimated coefficients by instrument",
    "text": "Estimated coefficients by instrument"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#level-two-model-1",
    "href": "slides/25-multilevel-models-pt2.html#level-two-model-1",
    "title": "Fitting multilevel models",
    "section": "Level Two model",
    "text": "Level Two model\nModel for intercepts\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n16.283\n0.671\n24.249\n0.000\n\n\norchestra\n1.411\n0.991\n1.424\n0.163\n\n\n\n\n\nModel for slopes\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.771\n0.851\n-0.906\n0.373\n\n\norchestra\n-1.406\n1.203\n-1.168\n0.253"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#writing-out-the-models",
    "href": "slides/25-multilevel-models-pt2.html#writing-out-the-models",
    "title": "Fitting multilevel models",
    "section": "Writing out the models",
    "text": "Writing out the models\nLevel One\n\\[\\hat{na}_{ij}  = \\hat{a}_i + \\hat{b}_i ~ LargeEnsemble_{ij}\\]\nfor each musician.\n\nLevel Two\n\\[\\begin{aligned}&\\hat{a}_i = 16.283 + 1.411 ~ Orchestra_i \\\\\n&\\hat{b}_i = -0.771 - 1.406 ~ Orchestra_i\\end{aligned}\\]"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#composite-model",
    "href": "slides/25-multilevel-models-pt2.html#composite-model",
    "title": "Fitting multilevel models",
    "section": "Composite model",
    "text": "Composite model\n\\[\\begin{aligned}\\hat{na}_i &= 16.283 + 1.411 ~ Orchestra_i - 0.771 ~ LargeEnsemble_{ij} \\\\\n&- 1.406 ~ Orchestra:LargeEnsemble_{ij}\\end{aligned}\\]\n\n\nWhat is the predicted average performance anxiety before solos and small ensemble performances for vocalists and keyboardists? For those who play orchestral instruments?\nWhat is the predicted average performance anxiety before large ensemble performances for those who play orchestral instruments?\n\n\n\ncountdown(minutes = 3, seconds= 0)\n\n\n03:00"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#disadvantages-to-this-approach",
    "href": "slides/25-multilevel-models-pt2.html#disadvantages-to-this-approach",
    "title": "Fitting multilevel models",
    "section": "Disadvantages to this approach",
    "text": "Disadvantages to this approach\n‚ö†Ô∏è Weighs each musician the same regardless of number of diary entries\n‚ö†Ô∏è Drops subjects who have missing values for slope (7 individuals who didn‚Äôt play a large ensemble performance)\n‚ö†Ô∏è Does not share strength effectively across individuals."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#application-exercise-2",
    "href": "slides/25-multilevel-models-pt2.html#application-exercise-2",
    "title": "Fitting multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nSee Part 5: Distribution of \\(R^2\\) values."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#framework",
    "href": "slides/25-multilevel-models-pt2.html#framework",
    "title": "Fitting multilevel models",
    "section": "Framework",
    "text": "Framework\nLet \\(Y_{ij}\\) be the performance anxiety for the \\(i^{th}\\) musician before performance \\(j\\).\nLevel One\n\\[Y_{ij} = a_i + b_i ~ LargeEnsemble_{ij} + \\epsilon_{ij}\\]\nLevel Two\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i\\\\\n&b_i = \\beta_0 + \\beta_1~Orchestra_i + v_i\\end{aligned}\\]\nCoefficients are estimated using likelihood-based methods (instead of least squares) to address the previously mentioned disadvantages"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#composite-model-1",
    "href": "slides/25-multilevel-models-pt2.html#composite-model-1",
    "title": "Fitting multilevel models",
    "section": "Composite model",
    "text": "Composite model\nPlug in the equations for \\(a_i\\) and \\(b_i\\) to get the composite model \\[\\begin{aligned}Y_{ij} &= (\\alpha_0 + \\alpha_1 ~ Orchestra_i + \\beta_0 ~ LargeEnsemble_{ij} \\\\\n&+ \\beta_1 ~ Orchestra_i:LargeEnsemble_{ij})\\\\\n&+ (u_i + v_i ~ LargeEnsemble_{ij} + \\epsilon_{ij})\\end{aligned}\\]\n\nThe fixed effects to estimate are \\(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1\\)\nThe error terms are \\(u_i, v_i, \\epsilon_{ij}\\)\n\nWe will estimate variability in the error terms \\(\\sigma_u, \\sigma_v, \\sigma_\\epsilon\\)\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe no longer need to estimate \\(a_i\\) and \\(b_i\\) directly as we did earlier. They conceptually connect the Level One and Level Two models."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#error-terms",
    "href": "slides/25-multilevel-models-pt2.html#error-terms",
    "title": "Fitting multilevel models",
    "section": "Error terms",
    "text": "Error terms\n\nWe generally assume that the error terms are normally distributed, e.g.¬†error associated with each performance of a given musician is \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\nFor the Level Two models, the errors are\n\n\\(u_i\\): deviation of musician \\(i\\) from the mean performance anxiety before solos and small ensembles after accounting for the instrument\n\\(v_i\\): deviance of musician \\(i\\) from the mean difference in performance anxiety between large ensembles and other performance types after accounting for instrument\n\nWe will also estimate \\(\\rho_{uv}\\) to account for fact that \\(u_i\\) (the intercepts) and \\(v_i\\) (the slopes) are correlated for the \\(i^{th}\\) musician"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#slopes-vs-intercepts-by-musician",
    "href": "slides/25-multilevel-models-pt2.html#slopes-vs-intercepts-by-musician",
    "title": "Fitting multilevel models",
    "section": "Slopes vs intercepts by musician",
    "text": "Slopes vs intercepts by musician\n\nRecreated from Figure 8.11\nDescribe what we learn about the association between the slopes and intercepts based on this plot."
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#distribution-of-level-two-errors",
    "href": "slides/25-multilevel-models-pt2.html#distribution-of-level-two-errors",
    "title": "Fitting multilevel models",
    "section": "Distribution of Level Two errors",
    "text": "Distribution of Level Two errors\nUse a multivariate normal distribution for the Level Two error terms \\[\\left[ \\begin{array}{c}\n            u_{i} \\\\ v_{i}\n          \\end{array}  \\right] \\sim N \\left( \\left[\n          \\begin{array}{c}\n            0 \\\\ 0\n          \\end{array} \\right], \\left[\n          \\begin{array}{cc}\n            \\sigma_{u}^{2} & \\rho_{uv}\\sigma_{u}\\sigma_v \\\\\n            \\rho_{uv}\\sigma_{u}\\sigma_v & \\sigma_{v}^{2}\n          \\end{array} \\right] \\right)\\]\nwhere \\(\\sigma^2_u\\) and \\(\\sigma^2_v\\) are the variance of \\(u_i\\)‚Äôs and \\(v_i\\)‚Äôs respectively, and \\(\\sigma_{uv} = \\rho_{uv}\\sigma_u\\sigma_v\\) is covariance between \\(u_i\\) and \\(v_i\\)\n\nWhat does it mean for \\(\\rho_{uv} &gt; 0\\)?\nWhat does it mean for \\(\\rho_{uv} &lt; 0\\)?"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#visualizing-multivariate-normal-distribution",
    "href": "slides/25-multilevel-models-pt2.html#visualizing-multivariate-normal-distribution",
    "title": "Fitting multilevel models",
    "section": "Visualizing multivariate normal distribution",
    "text": "Visualizing multivariate normal distribution\n\nRecreated from Figure 8.12"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#fit-the-model-in-r",
    "href": "slides/25-multilevel-models-pt2.html#fit-the-model-in-r",
    "title": "Fitting multilevel models",
    "section": "Fit the model in R",
    "text": "Fit the model in R\nFit multilevel model using tidymodels and the multilevelmod R packages. Display results using the tidy() function from the broom.mixed package.\n\nlibrary(tidymodels)\nlibrary(multilevelmod)\nlibrary(broom.mixed)\n\nmusic_fit &lt;- \n  linear_reg() |&gt;\n  set_engine(\"lmer\") |&gt;\n  fit(na ~ orchestra + large_ensemble +\n        orchestra:large_ensemble + (large_ensemble|id),\n      data = music)\n\ntidy(music_fit) |&gt; kable(digits = 3)"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#fit-the-model-in-r-1",
    "href": "slides/25-multilevel-models-pt2.html#fit-the-model-in-r-1",
    "title": "Fitting multilevel models",
    "section": "Fit the model in R",
    "text": "Fit the model in R\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nfixed\nNA\n(Intercept)\n15.930\n0.641\n24.833\n\n\nfixed\nNA\norchestra1\n1.693\n0.945\n1.791\n\n\nfixed\nNA\nlarge_ensemble1\n-0.911\n0.845\n-1.077\n\n\nfixed\nNA\norchestra1:large_ensemble1\n-1.424\n1.099\n-1.295\n\n\nran_pars\nid\nsd__(Intercept)\n2.378\nNA\nNA\n\n\nran_pars\nid\ncor__(Intercept).large_ensemble1\n-0.635\nNA\nNA\n\n\nran_pars\nid\nsd__large_ensemble1\n0.672\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n4.670\nNA\nNA"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#final-model",
    "href": "slides/25-multilevel-models-pt2.html#final-model",
    "title": "Fitting multilevel models",
    "section": "Final model",
    "text": "Final model\n\\[\\begin{aligned}\\hat{na}_i &= 15.930 + 1.693 ~ Orchestra_i - 0.911 ~ LargeEnsemble_{ij} \\\\\n&- 1.424 ~ Orchestra_i:LargeEnsemble_{ij} \\\\[5pt]\n&\\hat{\\sigma}_{u} = 2.378 \\hspace{20mm} \\hat{\\sigma}_{v} = 0.672 \\hspace{20mm} \\hat{\\sigma}_{\\epsilon} = 4.670 \\\\\n&\\hat{\\rho}_{uv} = -0.635\\end{aligned}\\]"
  },
  {
    "objectID": "slides/25-multilevel-models-pt2.html#acknowledgements",
    "href": "slides/25-multilevel-models-pt2.html#acknowledgements",
    "title": "Fitting multilevel models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from - BMLR: Chapter 7 - Correlated data - BMLR: Chapter 8 - Introduction to Multilevel Models\n\nSadler, Michael E., and Christopher J. Miller. 2010. ‚ÄúPerformance Anxiety: A Longitudinal Study of the Roles of Personality and Experience in Musicians.‚Äù Social Psychological and Personality Science 1 (3): 280‚Äì87. http://dx.doi.org/10.1177/1948550610370492.\n\n\n\n\nüîó Week 14"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#announcements",
    "href": "slides/03-slr-tidymodels.html#announcements",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Announcements",
    "text": "Announcements\n\nNo office hours today. Office hours start Tuesday, September 6. Click here for full schedule\nCheck your email for an email to join the course GitHub organization. You will receive one by Tuesday, September 6.\nSee Week 02 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#topics",
    "href": "slides/03-slr-tidymodels.html#topics",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Topics",
    "text": "Topics\n\nIntroduce the computing toolkit - RStudio and GitHub\nUse tidymodels to fit and summarize regression models in R\nComplete an application exercise on exploratory data analysis and modeling"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#computational-setup",
    "href": "slides/03-slr-tidymodels.html#computational-setup",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#reproducibility-checklist",
    "href": "slides/03-slr-tidymodels.html#reproducibility-checklist",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Are the tables and figures reproducible from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#toolkit",
    "href": "slides/03-slr-tidymodels.html#toolkit",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\nMore on this in this week‚Äôs lab"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#what-are-r-and-rstudio",
    "href": "slides/03-slr-tidymodels.html#what-are-r-and-rstudio",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "What are R and RStudio?",
    "text": "What are R and RStudio?\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\n\nSource: Modern Dive"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#rstudio-ide",
    "href": "slides/03-slr-tidymodels.html#rstudio-ide",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#quarto",
    "href": "slides/03-slr-tidymodels.html#quarto",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports ‚Äì the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#quarto-1",
    "href": "slides/03-slr-tidymodels.html#quarto-1",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#how-will-we-use-quarto",
    "href": "slides/03-slr-tidymodels.html#how-will-we-use-quarto",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou‚Äôll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#application-exercise",
    "href": "slides/03-slr-tidymodels.html#application-exercise",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#recap-of-last-lecture",
    "href": "slides/03-slr-tidymodels.html#recap-of-last-lecture",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#movie-ratings",
    "href": "slides/03-slr-tidymodels.html#movie-ratings",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango‚Äôs\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#data-prep",
    "href": "slides/03-slr-tidymodels.html#data-prep",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#tidymodels",
    "href": "slides/03-slr-tidymodels.html#tidymodels",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "tidymodels",
    "text": "tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\n\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.1.0 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.5     ‚úî rsample      1.1.1\n‚úî dials        1.2.0     ‚úî tune         1.1.1\n‚úî infer        1.0.4     ‚úî workflows    1.1.3\n‚úî modeldata    1.1.0     ‚úî workflowsets 1.0.1\n‚úî parsnip      1.1.0     ‚úî yardstick    1.2.0\n‚úî recipes      1.0.6     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ scales::discard() masks purrr::discard()\n‚úñ dplyr::filter()   masks stats::filter()\n‚úñ recipes::fixed()  masks stringr::fixed()\n‚úñ dplyr::lag()      masks stats::lag()\n‚úñ yardstick::spec() masks readr::spec()\n‚úñ recipes::step()   masks stats::step()\n‚Ä¢ Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#why-tidymodels",
    "href": "slides/03-slr-tidymodels.html#why-tidymodels",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Why tidymodels?",
    "text": "Why tidymodels?\n\nConsistent syntax for different model types (linear, logistic, random forest, Bayesian, etc.)\nStreamline modeling workflow\n\nSplit data into train and test sets\nTransform and create new variables\nAssess model performance\nUse model for prediction and inference"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#step-1-specify-model",
    "href": "slides/03-slr-tidymodels.html#step-1-specify-model",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#step-2-set-model-fitting-engine",
    "href": "slides/03-slr-tidymodels.html#step-2-set-model-fitting-engine",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#a-closer-look-at-model-output",
    "href": "slides/03-slr-tidymodels.html#a-closer-look-at-model-output",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\nmovie_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is likely just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#the-regression-output",
    "href": "slides/03-slr-tidymodels.html#the-regression-output",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "The regression output",
    "text": "The regression output\nWe‚Äôll focus on the first column for now‚Ä¶\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#prediction",
    "href": "slides/03-slr-tidymodels.html#prediction",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Prediction",
    "text": "Prediction\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 70)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 √ó 1\n  .pred\n  &lt;dbl&gt;\n1  68.6"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#application-exercise-1",
    "href": "slides/03-slr-tidymodels.html#application-exercise-1",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#recap",
    "href": "slides/03-slr-tidymodels.html#recap",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Recap",
    "text": "Recap\n\nIntroduced the computing toolkit - RStudio and GitHub\nUsed tidymodels to fit and summarize regression models in R\nCompleted an application exercise on exploratory data analysis and modeling\n\n\n\n\nüîó Week 02"
  },
  {
    "objectID": "slides/lab-project-writeup.html#goals",
    "href": "slides/lab-project-writeup.html#goals",
    "title": "Lab: Project Analysis + written report",
    "section": "Goals",
    "text": "Goals\n\nWork on project analysis and write up"
  },
  {
    "objectID": "slides/lab-project-writeup.html#project-timeline",
    "href": "slides/lab-project-writeup.html#project-timeline",
    "title": "Lab: Project Analysis + written report",
    "section": "Project timeline",
    "text": "Project timeline\n\nProject meetings (optional): Nov 21 & 22\n\nClick here to sign up by Sun, Nov 20 at 11:59pm. Only one meeting per team.\n\nRound 1 submission (optional) due Nov 22 at 11:59pm\nWritten report due Fri, Dec 09 at 11:59pm\nVideo presentations during exam week"
  },
  {
    "objectID": "slides/lab-project-writeup.html#tips-for-report",
    "href": "slides/lab-project-writeup.html#tips-for-report",
    "title": "Lab: Project Analysis + written report",
    "section": "Tips for report",
    "text": "Tips for report\nThere is a 10-page limit on the final report.\n\nAll code, warnings, and messages must be suppressed in the final report. Add the following code to the YAML:\n\nexecute:\n  echo: false\n  warning: false\n  message: false\n\nTell a cohesive and focused story. You won‚Äôt have room to give extensive details about everything you did in the analysis, so carefully consider where to put more/ less detail.\n\nUse the appendix for additional graphs and text you want to share. The reader may not read the appendix, so the body of the report should still be comprehensive without the material in the appendix."
  },
  {
    "objectID": "slides/lab-project-writeup.html#tips-for-report-1",
    "href": "slides/lab-project-writeup.html#tips-for-report-1",
    "title": "Lab: Project Analysis + written report",
    "section": "Tips for report",
    "text": "Tips for report\n\nUse code chunk options to change the size of figures.\n\nDon‚Äôt make them so small they become unreadable!\n\nUse variable descriptions, notvariable names in the narrative.\n\nSee Project Tips + Resources for more tips.\n\n\n\nüîó Week 12"
  },
  {
    "objectID": "slides/01-welcome.html#meet-the-professor",
    "href": "slides/01-welcome.html#meet-the-professor",
    "title": "Welcome to STA 210!",
    "section": "Meet the professor",
    "text": "Meet the professor\n\n\n\n\n\n\n\n\nDr.¬†Maria Tackett\n\n\n\nAssistant Professor of the Practice, Department of Statistical Science\nWork focuses on statistics education, specifically active learning, motivation, and classroom community\nFind out more at maria-tackett.netlify.app"
  },
  {
    "objectID": "slides/01-welcome.html#meet-tas",
    "href": "slides/01-welcome.html#meet-tas",
    "title": "Welcome to STA 210!",
    "section": "Meet TAs",
    "text": "Meet TAs\n\n\n\nCarson Garcia (UG)\nSara Meta (UG)\nMedy Mu (UG)\nGlenn Palmer (PhD)\nBraden Scherting (PhD)\n\n\n\nLuke Vrotsos (PhD)\nBen Wallace (UG)\nAaditya Warrier (UG)\nGrace Zhao (MS)"
  },
  {
    "objectID": "slides/01-welcome.html#check-in-on-ed-discussion",
    "href": "slides/01-welcome.html#check-in-on-ed-discussion",
    "title": "Welcome to STA 210!",
    "section": "Check in on Ed Discussion!",
    "text": "Check in on Ed Discussion!\n\nGo to the class Ed Discussion\n\nSection 001 (10:15am lecture)\nSection 002 (3:30pm lecture)\n\nAnswer the poll question: How are you doing?"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-regression-analysis",
    "href": "slides/01-welcome.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\n‚ÄúIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or ‚Äòpredictors‚Äô). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‚Äòcriterion variable‚Äô) changes when any one of the independent variables is varied, while the other independent variables are held fixed.‚Äù\n\nSource: Wikipedia (previous definition)"
  },
  {
    "objectID": "slides/01-welcome.html#course-faq",
    "href": "slides/01-welcome.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Introductory statistics or probability course at Duke\n\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R and version control platform GitHub\n\n\nQ - Will we learn the mathematical theory of regression?\nA - Yes and No.¬†The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. There a 0.5-credit course STA 211: Mathematics of Regression to take simultaneously or after this course to dive into more of the mathematics."
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nAssess whether a proposed model is appropriate and describe its limitations.\nUse Quarto to write reproducible reports and GitHub for version control and collaboration.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to STA 210!",
    "section": "Course topics",
    "text": "Course topics\n\n\nUnit 1: Quantitative Response Variable\n\nSimple Linear Regression\nMultiple Linear Regression\n\n\nUnit 2: Categorical Response Variable\n\nLogistic Regression\n\n\nUnit 3: Looking Ahead\n\nMixed and random effects\nIntroduction to linear mixed models\nPresenting statistical results"
  },
  {
    "objectID": "slides/01-welcome.html#examples-of-regression-in-practice",
    "href": "slides/01-welcome.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight‚Äôs 2020 Presidential Forecast Works ‚Äî And What‚Äôs Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it‚Äôs so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website: sta210-fa22.netlify.app\n\nCentral hub for the course!\nTour of the website\n\nSakai: sakai.duke.edu\n\nGradebook\nAnnouncements\nGradescope\n\nEd Discussion: edstem.org/us/courses/26900/discussion\n\nClass Q&A and discussion forum"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through STA 210 Docker Containers"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit-1",
    "href": "slides/01-welcome.html#computing-toolkit-1",
    "title": "Welcome to STA 210!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\nAccess assignments\n\n\n\nFacilitates version control and collaboration\nAll work in STA 210 course organization"
  },
  {
    "objectID": "slides/01-welcome.html#activities-prepare-participate-practice-perform",
    "href": "slides/01-welcome.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you‚Äôve learned to analyze real-world data\n\nLab assignments (first individual, later team-based)\nHomework assignments (individual)\nTwo take-home exams\nTerm project presented during the final exam period"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n35%\n\n\nFinal project\n15%\n\n\nLab\n15%\n\n\nExam 01\n15%\n\n\nExam 02\n15%\n\n\nApplication Exercises\n2.5%\n\n\nTeamwork\n2.5%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/01-welcome.html#support",
    "href": "slides/01-welcome.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours to meet with a member of the teaching team\n\nFull office hours schedule begins Tuesday, September 6\n\nAsk and answer questions on course discussion forum\nUse email for questions regarding personal matters and/or grades\nRead the Course Support page for more details"
  },
  {
    "objectID": "slides/01-welcome.html#diversity-inclusion",
    "href": "slides/01-welcome.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\n\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know.\nPlease let me know your preferred pronouns, if you are comfortable sharing.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said or done in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/01-welcome.html#accessibility",
    "href": "slides/01-welcome.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I‚Äôm always learning how to do this better. If any course component is not accessible to you in any way, please don‚Äôt hesitate to let me know."
  },
  {
    "objectID": "slides/01-welcome.html#covid-policies",
    "href": "slides/01-welcome.html#covid-policies",
    "title": "Welcome to STA 210!",
    "section": "COVID policies",
    "text": "COVID policies\n\nüò∑ Wear a mask at all times in lectures and labs based on current university policy\nRead and follow the university guidelines"
  },
  {
    "objectID": "slides/01-welcome.html#late-work-waivers-and-regrade-requests",
    "href": "slides/01-welcome.html#late-work-waivers-and-regrade-requests",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, and regrade requests",
    "text": "Late work, waivers, and regrade requests\n\nWe have policies!\nRead about them in the syllabus and refer back to them as needed"
  },
  {
    "objectID": "slides/01-welcome.html#collaboration-sharing-code",
    "href": "slides/01-welcome.html#collaboration-sharing-code",
    "title": "Welcome to STA 210!",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nWe have policies!\nRead about them in the syllabus and refer to them as needed\nWe‚Äôll discuss these more before the first assignments"
  },
  {
    "objectID": "slides/01-welcome.html#academic-integrity",
    "href": "slides/01-welcome.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the Duke Community Standard."
  },
  {
    "objectID": "slides/01-welcome.html#if-youre-not-sure",
    "href": "slides/01-welcome.html#if-youre-not-sure",
    "title": "Welcome to STA 210!",
    "section": "If you‚Äôre not sure‚Ä¶",
    "text": "If you‚Äôre not sure‚Ä¶\nAsk if you‚Äôre not sure if something violates a policy!"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success",
    "href": "slides/01-welcome.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions.\nDo the homeworks and labs and get started on homework early when possible.\nDon‚Äôt procrastinate and don‚Äôt let a week pass by with lingering questions.\nStay up-to-date on announcements (posted on Sakai & sent via email)."
  },
  {
    "objectID": "slides/01-welcome.html#learning-during-a-pandemic",
    "href": "slides/01-welcome.html#learning-during-a-pandemic",
    "title": "Welcome to STA 210!",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don‚Äôt hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you‚Äôre always welcome to talk to me. If I can‚Äôt help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded.\n\nSee the Course Support page for information on Duke‚Äôs academic and wellness resources."
  },
  {
    "objectID": "slides/01-welcome.html#application-exercise",
    "href": "slides/01-welcome.html#application-exercise",
    "title": "Welcome to STA 210!",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 01 - Movie Budgets and Revenues"
  },
  {
    "objectID": "slides/01-welcome.html#for-this-week",
    "href": "slides/01-welcome.html#for-this-week",
    "title": "Welcome to STA 210!",
    "section": "For this week‚Ä¶",
    "text": "For this week‚Ä¶\n\nRead the syllabus\nSee Week 01 on the schedule for lecture slides, assignments, and readings.\nComplete the STA 210 Student Survey (will ask for a GitHub username)\n\nClick here for information on registering for a GitHub account and choosing a username.\nYou will go over this in lab this week.\n\nReserve an STA 210 Docker Container\n\n\n\n\nüîó Week 01"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#announcements",
    "href": "slides/06-slr-sim-testing.html#announcements",
    "title": "SLR: Simulation-based inference",
    "section": "Announcements",
    "text": "Announcements\n\nHW 01\n\nReleased later today (will get email when HW is available)\ndue Wed, Sep 21 at 11:59pm\n\nStatistics experience - due Fri, Dec 09 at 11:59pm\nSee Week 03 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#topics",
    "href": "slides/06-slr-sim-testing.html#topics",
    "title": "SLR: Simulation-based inference",
    "section": "Topics",
    "text": "Topics\n\nEvaluate a claim about the slope using hypothesis testing\nDefine mathematical models to conduct inference for slope"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#computational-setup",
    "href": "slides/06-slr-sim-testing.html#computational-setup",
    "title": "SLR: Simulation-based inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#data-duke-forest-houses",
    "href": "slides/06-slr-sim-testing.html#data-duke-forest-houses",
    "title": "SLR: Simulation-based inference",
    "section": "Data: Duke Forest houses",
    "text": "Data: Duke Forest houses"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#the-regression-model",
    "href": "slides/06-slr-sim-testing.html#the-regression-model",
    "title": "SLR: Simulation-based inference",
    "section": "The regression model",
    "text": "The regression model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#inference-for-simple-linear-regression",
    "href": "slides/06-slr-sim-testing.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation-based inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#sampling-is-natural",
    "href": "slides/06-slr-sim-testing.html#sampling-is-natural",
    "title": "SLR: Simulation-based inference",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn‚Äôt salty enough, that‚Äôs exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that‚Äôs an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#confidence-interval-via-bootstrapping",
    "href": "slides/06-slr-sim-testing.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-i",
    "href": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-i",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrapping pipeline I",
    "text": "Bootstrapping pipeline I\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 √ó 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-ii",
    "href": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrapping pipeline II",
    "text": "Bootstrapping pipeline II\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate   price  area\n       &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1         1  290000  2414\n 2         1  285000  2108\n 3         1  265000  1300\n 4         1  416000  2949\n 5         1  541000  2740\n 6         1  525000  2256\n 7         1 1270000  3909\n 8         1  265000  1300\n 9         1  815000  3904\n10         1  535000  2937\n# ‚Ä¶ with 97,990 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-iii",
    "href": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrapping pipeline III",
    "text": "Bootstrapping pipeline III\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\n# A tibble: 2,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   80699.\n 2         1 area          168.\n 3         2 intercept  -18821.\n 4         2 area          205.\n 5         3 intercept  234297.\n 6         3 area          117.\n 7         4 intercept  134481.\n 8         4 area          150.\n 9         5 intercept   23861.\n10         5 area          190.\n# ‚Ä¶ with 1,990 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-iv",
    "href": "slides/06-slr-sim-testing.html#bootstrapping-pipeline-iv",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrapping pipeline IV",
    "text": "Bootstrapping pipeline IV\n\nset.seed(210)\n\nboot_dist &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#visualize-the-bootstrap-distribution",
    "href": "slides/06-slr-sim-testing.html#visualize-the-bootstrap-distribution",
    "title": "SLR: Simulation-based inference",
    "section": "Visualize the bootstrap distribution",
    "text": "Visualize the bootstrap distribution\n\nboot_dist |&gt;\n  filter(term == \"area\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10)"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#compute-the-ci",
    "href": "slides/06-slr-sim-testing.html#compute-the-ci",
    "title": "SLR: Simulation-based inference",
    "section": "Compute the CI",
    "text": "Compute the CI"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#but-first",
    "href": "slides/06-slr-sim-testing.html#but-first",
    "title": "SLR: Simulation-based inference",
    "section": "But first‚Ä¶",
    "text": "But first‚Ä¶\n\nobs_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobs_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#compute-95-confidence-interval",
    "href": "slides/06-slr-sim-testing.html#compute-95-confidence-interval",
    "title": "SLR: Simulation-based inference",
    "section": "Compute 95% confidence interval",
    "text": "Compute 95% confidence interval\n\nboot_dist |&gt;\n  get_confidence_interval(\n    level = 0.95,\n    type = \"percentile\",\n    point_estimate = obs_fit\n  )\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          91.7     211.\n2 intercept -18290.   287711."
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#research-question-and-hypotheses",
    "href": "slides/06-slr-sim-testing.html#research-question-and-hypotheses",
    "title": "SLR: Simulation-based inference",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n\n‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù\nNull hypothesis: there is no linear relationship between area and price\n\\[\nH_0: \\beta_1 = 0\n\\]\nAlternative hypothesis: there is a linear relationship between area and price\n\\[\nH_A: \\beta_1 \\ne 0\n\\]"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/06-slr-sim-testing.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Simulation-based inference",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: ‚ÄúCould these data plausibly have happened by chance if the null hypothesis were true?‚Äù\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#hypothesis-testing-framework",
    "href": "slides/06-slr-sim-testing.html#hypothesis-testing-framework",
    "title": "SLR: Simulation-based inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e.¬†what we‚Äôre testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#quantify-the-variability-of-the-slope",
    "href": "slides/06-slr-sim-testing.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor testing\n\n\nTwo approaches:\n\nVia simulation\nVia mathematical models\n\nRandomization to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-described",
    "href": "slides/06-slr-sim-testing.html#permutation-described",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nSet the null hypothesis to be true, and measure the natural variability in the data due to sampling but not due to variables being correlated by permuting\n\nPermute one variable to eliminate any existing relationship between the variables\n\nEach price value is randomly assigned to area of a given house, i.e.¬†area and price are no longer matched for a given house\n\n\n\n\n# A tibble: 98 √ó 3\n   price_Observed price_Permuted  area\n            &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1        1520000         342500  6040\n 2        1030000         750000  4475\n 3         420000         645000  1745\n 4         680000         697500  2091\n 5         428500         428500  1772\n 6         456000         481000  1950\n 7        1270000         610000  3909\n 8         557450         680000  2841\n 9         697500         485000  3924\n10         650000         105000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-visualized",
    "href": "slides/06-slr-sim-testing.html#permutation-visualized",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nThe permutation removes the linear relationship between area and price"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-repeated",
    "href": "slides/06-slr-sim-testing.html#permutation-repeated",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#concluding-the-hypothesis-test",
    "href": "slides/06-slr-sim-testing.html#concluding-the-hypothesis-test",
    "title": "SLR: Simulation-based inference",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 159\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: ‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-pipeline-i",
    "href": "slides/06-slr-sim-testing.html#permutation-pipeline-i",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 √ó 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-pipeline-ii",
    "href": "slides/06-slr-sim-testing.html#permutation-pipeline-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98 √ó 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-pipeline-iii",
    "href": "slides/06-slr-sim-testing.html#permutation-pipeline-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98,000 √ó 3\n# Groups:   replicate [1,000]\n     price  area replicate\n     &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;\n 1  465000  6040         1\n 2  481000  4475         1\n 3 1020000  1745         1\n 4  520000  2091         1\n 5  592000  1772         1\n 6  650000  1950         1\n 7  473000  3909         1\n 8  705000  2841         1\n 9  785000  3924         1\n10  671500  2173         1\n# ‚Ä¶ with 97,990 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-pipeline-iv",
    "href": "slides/06-slr-sim-testing.html#permutation-pipeline-iv",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# A tibble: 2,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate term       estimate\n       &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1         1 intercept 553355.  \n 2         1 area           2.35\n 3         2 intercept 635824.  \n 4         2 area         -27.3 \n 5         3 intercept 536072.  \n 6         3 area           8.57\n 7         4 intercept 598649.  \n 8         4 area         -13.9 \n 9         5 intercept 556202.  \n10         5 area           1.33\n# ‚Ä¶ with 1,990 more rows"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#permutation-pipeline-v",
    "href": "slides/06-slr-sim-testing.html#permutation-pipeline-v",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\nset.seed(1125)\n\nnull_dist &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#visualize-the-null-distribution",
    "href": "slides/06-slr-sim-testing.html#visualize-the-null-distribution",
    "title": "SLR: Simulation-based inference",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\nnull_dist |&gt;\n  filter(term == \"area\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10, color = \"white\")"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#reason-around-the-p-value",
    "href": "slides/06-slr-sim-testing.html#reason-around-the-p-value",
    "title": "SLR: Simulation-based inference",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the area of a Duke Forest house and in its price (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 98 houses where the slope fo the model predicting price from area is 159 or even more extreme?"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#compute-the-p-value",
    "href": "slides/06-slr-sim-testing.html#compute-the-p-value",
    "title": "SLR: Simulation-based inference",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = obs_fit,\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\n\n# A tibble: 2 √ó 2\n  term      p_value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 area            0\n2 intercept       0"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#the-regression-model-revisited",
    "href": "slides/06-slr-sim-testing.html#the-regression-model-revisited",
    "title": "SLR: Simulation-based inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#inference-revisited",
    "href": "slides/06-slr-sim-testing.html#inference-revisited",
    "title": "SLR: Simulation-based inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we‚Äôll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#mathematical-representation-of-the-model",
    "href": "slides/06-slr-sim-testing.html#mathematical-representation-of-the-model",
    "title": "SLR: Simulation-based inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn‚Äôt tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#mathematical-representation-visualized",
    "href": "slides/06-slr-sim-testing.html#mathematical-representation-visualized",
    "title": "SLR: Simulation-based inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#regression-standard-error",
    "href": "slides/06-slr-sim-testing.html#regression-standard-error",
    "title": "SLR: Simulation-based inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error (the spread of the distribution of the response, for a given value of the predictor variable):\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/06-slr-sim-testing.html#standard-error-of-hatbeta_1",
    "href": "slides/06-slr-sim-testing.html#standard-error-of-hatbeta_1",
    "title": "SLR: Simulation-based inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\nor‚Ä¶\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\nüîó Week 03"
  },
  {
    "objectID": "slides/17-mlr-inference.html#announcements",
    "href": "slides/17-mlr-inference.html#announcements",
    "title": "MLR Inference",
    "section": "Announcements",
    "text": "Announcements\n\nSee Gradescope for feedback on project topic ideas.\n\nRead comments carefully. Even if a data set is marked ‚Äúusable‚Äù, there may be suggestions about extensive data cleaning required to make it appropriate for the project.\nAttend office hours or talk with TAs in lab if you have questions.\n\nSee Week 09 activities."
  },
  {
    "objectID": "slides/17-mlr-inference.html#topics",
    "href": "slides/17-mlr-inference.html#topics",
    "title": "MLR Inference",
    "section": "Topics",
    "text": "Topics\n\nConduct a hypothesis test for \\(\\beta_j\\)\nCalculate a confidence interval for \\(\\beta_j\\)\nInference pitfalls"
  },
  {
    "objectID": "slides/17-mlr-inference.html#computational-setup",
    "href": "slides/17-mlr-inference.html#computational-setup",
    "title": "MLR Inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)      # for tables\nlibrary(patchwork)  # for laying out plots\nlibrary(rms)        # for vif\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/17-mlr-inference.html#modeling-workflow",
    "href": "slides/17-mlr-inference.html#modeling-workflow",
    "title": "MLR Inference",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nSplit data into training and test sets.\nUse cross validation on the training set to fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\nRefit the model using the entire training set and do ‚Äúfinal‚Äù evaluation on the test set (make sure you have not overfit the model).\n\nAdjust as needed if there is evidence of overfit.\n\nUse model fit on training set for inference and prediction."
  },
  {
    "objectID": "slides/17-mlr-inference.html#data-rail_trail",
    "href": "slides/17-mlr-inference.html#data-rail_trail",
    "title": "MLR Inference",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\n# A tibble: 90 √ó 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ‚Ä¶ with 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/17-mlr-inference.html#variables",
    "href": "slides/17-mlr-inference.html#variables",
    "title": "MLR Inference",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of ‚ÄúFall‚Äù, ‚ÄúSpring‚Äù, or ‚ÄúSummer‚Äù\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of ‚Äúweekday‚Äù or ‚Äúweekend‚Äù"
  },
  {
    "objectID": "slides/17-mlr-inference.html#review-simple-linear-regression-slr",
    "href": "slides/17-mlr-inference.html#review-simple-linear-regression-slr",
    "title": "MLR Inference",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\nggplot(rail_trail, aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"High temp (F)\", y = \"Number of riders\")"
  },
  {
    "objectID": "slides/17-mlr-inference.html#slr-model-summary",
    "href": "slides/17-mlr-inference.html#slr-model-summary",
    "title": "MLR Inference",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.079280\n59.3953040\n-0.2875527\n0.7743652\n\n\nhightemp\n5.701878\n0.8480074\n6.7238541\n0.0000000"
  },
  {
    "objectID": "slides/17-mlr-inference.html#slr-hypothesis-test",
    "href": "slides/17-mlr-inference.html#slr-hypothesis-test",
    "title": "MLR Inference",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t= 6.72\\) . The p-value is calculated using a \\(t\\) distribution with 88 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e.¬†there is a linear relationship between high temperature and number of daily riders."
  },
  {
    "objectID": "slides/17-mlr-inference.html#multiple-linear-regression",
    "href": "slides/17-mlr-inference.html#multiple-linear-regression",
    "title": "MLR Inference",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11"
  },
  {
    "objectID": "slides/17-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "href": "slides/17-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "title": "MLR Inference",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) vs.¬†\\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\). The p-value is calculated using a \\(t\\) distribution with 86 (n - p - 1) degrees of freedom. The p-value is \\(\\approx 0\\).\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/17-mlr-inference.html#the-model-for-season-spring",
    "href": "slides/17-mlr-inference.html#the-model-for-season-spring",
    "title": "MLR Inference",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/17-mlr-inference.html#the-model-for-season-summer",
    "href": "slides/17-mlr-inference.html#the-model-for-season-summer",
    "title": "MLR Inference",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/17-mlr-inference.html#the-model-for-season-fall",
    "href": "slides/17-mlr-inference.html#the-model-for-season-fall",
    "title": "MLR Inference",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/17-mlr-inference.html#the-models",
    "href": "slides/17-mlr-inference.html#the-models",
    "title": "MLR Inference",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/17-mlr-inference.html#application-exercise",
    "href": "slides/17-mlr-inference.html#application-exercise",
    "title": "MLR Inference",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 11: MLR Inference\n\n\nEx 1. Add an interaction effect between hightemp and season to the model. Do the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms.\n\n\n\n\n08:00"
  },
  {
    "objectID": "slides/17-mlr-inference.html#confidence-interval-for-beta_j-1",
    "href": "slides/17-mlr-inference.html#confidence-interval-for-beta_j-1",
    "title": "MLR Inference",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically, we are \\(C%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context, we are \\(C%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant."
  },
  {
    "objectID": "slides/17-mlr-inference.html#confidence-interval-for-beta_j-2",
    "href": "slides/17-mlr-inference.html#confidence-interval-for-beta_j-2",
    "title": "MLR Inference",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE) |&gt;\n  kable(digits= 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00"
  },
  {
    "objectID": "slides/17-mlr-inference.html#ci-for-hightemp",
    "href": "slides/17-mlr-inference.html#ci-for-hightemp",
    "title": "MLR Inference",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/17-mlr-inference.html#ci-for-seasonspring",
    "href": "slides/17-mlr-inference.html#ci-for-seasonspring",
    "title": "MLR Inference",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n\nIs season a significant predictor of the number of riders, after accounting for high temperature?"
  },
  {
    "objectID": "slides/17-mlr-inference.html#large-sample-sizes",
    "href": "slides/17-mlr-inference.html#large-sample-sizes",
    "title": "MLR Inference",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/17-mlr-inference.html#small-sample-sizes",
    "href": "slides/17-mlr-inference.html#small-sample-sizes",
    "title": "MLR Inference",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON‚ÄôT immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\n\n\nüîó Week 09"
  },
  {
    "objectID": "slides/20-odds-ratios.html#announcements",
    "href": "slides/20-odds-ratios.html#announcements",
    "title": "Odds ratios",
    "section": "Announcements",
    "text": "Announcements\n\nHW 03 TODAY at 11:59pm\nLab 05 due\n\nTODAY at 11:59pm (Thu labs)\nTue, Nov 08 at 11:59pm (Fri labs)\n\nTeam Feedback #1 due Tue, Nov 08 at 11:59pm\n\nReceived email from Teammates around 10am\nCheck your junk/spam folder if you do not see the email.\nEmail Prof.¬†Tackett if you still don‚Äôt see it.\n\nSee Week 11 activities"
  },
  {
    "objectID": "slides/20-odds-ratios.html#topics",
    "href": "slides/20-odds-ratios.html#topics",
    "title": "Odds ratios",
    "section": "Topics",
    "text": "Topics\n\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/20-odds-ratios.html#computational-setup",
    "href": "slides/20-odds-ratios.html#computational-setup",
    "title": "Odds ratios",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/20-odds-ratios.html#risk-of-coronary-heart-disease",
    "href": "slides/20-odds-ratios.html#risk-of-coronary-heart-disease",
    "title": "Odds ratios",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College"
  },
  {
    "objectID": "slides/20-odds-ratios.html#high-risk-vs.-education",
    "href": "slides/20-odds-ratios.html#high-risk-vs.-education",
    "title": "Odds ratios",
    "section": "High risk vs.¬†education",
    "text": "High risk vs.¬†education\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/20-odds-ratios.html#compare-the-odds-for-two-groups",
    "href": "slides/20-odds-ratios.html#compare-the-odds-for-two-groups",
    "title": "Odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\n\nWe want to compare the risk of heart disease for those with a High School diploma/GED and those with a college degree.\nWe‚Äôll use the odds to compare the two groups\n\n\\[\n\\text{odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{\\text{# of successes}}{\\text{# of failures}}\n\\]"
  },
  {
    "objectID": "slides/20-odds-ratios.html#compare-the-odds-for-two-groups-1",
    "href": "slides/20-odds-ratios.html#compare-the-odds-for-two-groups-1",
    "title": "Odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nOdds of having high risk for the High school or GED group: \\(\\frac{147}{1106} = 0.133\\)\nOdds of having high risk for the College group: \\(\\frac{70}{403} = 0.174\\)\nBased on this, we see those with a college degree had higher odds of having high risk for heart disease than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/20-odds-ratios.html#odds-ratio-or",
    "href": "slides/20-odds-ratios.html#odds-ratio-or",
    "title": "Odds ratios",
    "section": "Odds ratio (OR)",
    "text": "Odds ratio (OR)\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\nLet‚Äôs summarize the relationship between the two groups. To do so, we‚Äôll use the odds ratio (OR).\n\\[\nOR = \\frac{\\text{odds}_1}{\\text{odds}_2} = \\frac{\\omega_1}{\\omega_2}\n\\]"
  },
  {
    "objectID": "slides/20-odds-ratios.html#or-college-vs.-high-school-or-ged",
    "href": "slides/20-odds-ratios.html#or-college-vs.-high-school-or-ged",
    "title": "Odds ratios",
    "section": "OR: College vs.¬†High school or GED",
    "text": "OR: College vs.¬†High school or GED\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{HS}} = \\frac{0.174}{0.133} = \\mathbf{1.308}\\]\n\nThe odds of having high risk for heart disease are 1.30 times higher for those with a college degree than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/20-odds-ratios.html#or-college-vs.-some-high-school",
    "href": "slides/20-odds-ratios.html#or-college-vs.-some-high-school",
    "title": "Odds ratios",
    "section": "OR: College vs.¬†Some high school",
    "text": "OR: College vs.¬†Some high school\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{Some HS}} = \\frac{70/403}{323/1397} = 0.751\\]\n\nThe odds of having high risk for having heart disease for those with a college degree are 0.751 times the odds of having high risk for heart disease for those with some high school."
  },
  {
    "objectID": "slides/20-odds-ratios.html#more-natural-interpretation",
    "href": "slides/20-odds-ratios.html#more-natural-interpretation",
    "title": "Odds ratios",
    "section": "More natural interpretation",
    "text": "More natural interpretation\n\nIt‚Äôs more natural to interpret the odds ratio with a statement with the odds ratio greater than 1.\nThe odds of having high risk for heart disease are 1.33 times higher for those with some high school than those with a college degree."
  },
  {
    "objectID": "slides/20-odds-ratios.html#making-the-table-1",
    "href": "slides/20-odds-ratios.html#making-the-table-1",
    "title": "Odds ratios",
    "section": "Making the table 1",
    "text": "Making the table 1\nFirst, rename the levels of the categorical variables:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(\n    high_risk_names = if_else(high_risk == \"1\", \"High risk\", \"Not high risk\"),\n    education_names = case_when(\n      education == \"1\" ~ \"Some high school\",\n      education == \"2\" ~ \"High school or GED\",\n      education == \"3\" ~ \"Some college or vocational school\",\n      education == \"4\" ~ \"College\"\n    ),\n    education_names = fct_relevel(education_names, \"Some high school\", \"High school or GED\", \"Some college or vocational school\", \"College\")\n  )"
  },
  {
    "objectID": "slides/20-odds-ratios.html#making-the-table-2",
    "href": "slides/20-odds-ratios.html#making-the-table-2",
    "title": "Odds ratios",
    "section": "Making the table 2",
    "text": "Making the table 2\nThen, make the table:\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))"
  },
  {
    "objectID": "slides/20-odds-ratios.html#deeper-look-into-the-code",
    "href": "slides/20-odds-ratios.html#deeper-look-into-the-code",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names)\n\n# A tibble: 8 √ó 3\n  education_names                   high_risk_names     n\n  &lt;fct&gt;                             &lt;chr&gt;           &lt;int&gt;\n1 Some high school                  High risk         323\n2 Some high school                  Not high risk    1397\n3 High school or GED                High risk         147\n4 High school or GED                Not high risk    1106\n5 Some college or vocational school High risk          88\n6 Some college or vocational school Not high risk     601\n7 College                           High risk          70\n8 College                           Not high risk     403"
  },
  {
    "objectID": "slides/20-odds-ratios.html#deeper-look-into-the-code-1",
    "href": "slides/20-odds-ratios.html#deeper-look-into-the-code-1",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n)\n\n# A tibble: 4 √ó 3\n  education_names                   `High risk` `Not high risk`\n  &lt;fct&gt;                                   &lt;int&gt;           &lt;int&gt;\n1 Some high school                          323            1397\n2 High school or GED                        147            1106\n3 Some college or vocational school          88             601\n4 College                                    70             403"
  },
  {
    "objectID": "slides/20-odds-ratios.html#deeper-look-into-the-code-2",
    "href": "slides/20-odds-ratios.html#deeper-look-into-the-code-2",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable()\n\n\n\n\neducation_names\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/20-odds-ratios.html#deeper-look-into-the-code-3",
    "href": "slides/20-odds-ratios.html#deeper-look-into-the-code-3",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/20-odds-ratios.html#categorical-predictor",
    "href": "slides/20-odds-ratios.html#categorical-predictor",
    "title": "Odds ratios",
    "section": "Categorical predictor",
    "text": "Categorical predictor\nRecall: Education - 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\nheart_edu_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ education, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046"
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpreting-education4-log-odds",
    "href": "slides/20-odds-ratios.html#interpreting-education4-log-odds",
    "title": "Odds ratios",
    "section": "Interpreting education4: log-odds",
    "text": "Interpreting education4: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe log-odds of having high risk for heart disease are expected to be 0.286 less for those with a college degree compared to those with some high school (the baseline group).\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpreting-education4-odds",
    "href": "slides/20-odds-ratios.html#interpreting-education4-odds",
    "title": "Odds ratios",
    "section": "Interpreting education4: odds",
    "text": "Interpreting education4: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe odds of having high risk for heart disease for those with a college degree are expected to be 0.751 (exp(-0.286)) times the odds for those with some high school."
  },
  {
    "objectID": "slides/20-odds-ratios.html#coefficients-odds-ratios",
    "href": "slides/20-odds-ratios.html#coefficients-odds-ratios",
    "title": "Odds ratios",
    "section": "Coefficients + odds ratios",
    "text": "Coefficients + odds ratios\nThe model coefficient, -0.286, is the expected change in the log-odds when going from the Some high school group to the College group.\n\nTherefore, \\(e^{-0.286}\\) = 0.751 is the expected change in the odds when going from the Some high school group to the College group.\n\n\n\\[\nOR  = e^{\\hat{\\beta}_j} = \\exp\\{\\hat{\\beta}_j\\}\n\\]"
  },
  {
    "objectID": "slides/20-odds-ratios.html#quantitative-predictor",
    "href": "slides/20-odds-ratios.html#quantitative-predictor",
    "title": "Odds ratios",
    "section": "Quantitative predictor",
    "text": "Quantitative predictor\n\nheart_age_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_age_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0"
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpreting-age-log-odds",
    "href": "slides/20-odds-ratios.html#interpreting-age-log-odds",
    "title": "Odds ratios",
    "section": "Interpreting age: log-odds",
    "text": "Interpreting age: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\nFor each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.076.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpreting-age-odds",
    "href": "slides/20-odds-ratios.html#interpreting-age-odds",
    "title": "Odds ratios",
    "section": "Interpreting age: odds",
    "text": "Interpreting age: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\n\nFor each additional year in age, the odds of having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.076)).\nAlternate interpretation: For each additional year in age, the odds of having high risk for heart disease are expected to increase by 8%."
  },
  {
    "objectID": "slides/20-odds-ratios.html#multiple-predictors",
    "href": "slides/20-odds-ratios.html#multiple-predictors",
    "title": "Odds ratios",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nheart_edu_age_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ education + age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_age_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000"
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpretation-in-terms-of-log-odds",
    "href": "slides/20-odds-ratios.html#interpretation-in-terms-of-log-odds",
    "title": "Odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The log-odds of having high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, holding age constant.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpretation-in-terms-of-log-odds-1",
    "href": "slides/20-odds-ratios.html#interpretation-in-terms-of-log-odds-1",
    "title": "Odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.073, holding education level constant.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpretation-in-terms-of-odds",
    "href": "slides/20-odds-ratios.html#interpretation-in-terms-of-odds",
    "title": "Odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The odds of having high risk for heart disease for those with a college degree are expected to be 0.98 (exp(-0.020)) times the odds for those with some high school, holding age constant."
  },
  {
    "objectID": "slides/20-odds-ratios.html#interpretation-in-terms-of-odds-1",
    "href": "slides/20-odds-ratios.html#interpretation-in-terms-of-odds-1",
    "title": "Odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the odds having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.073)), holding education level constant."
  },
  {
    "objectID": "slides/20-odds-ratios.html#recap",
    "href": "slides/20-odds-ratios.html#recap",
    "title": "Odds ratios",
    "section": "Recap",
    "text": "Recap\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors\n\n\n\n\n\nüîó Week 11"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#announcements",
    "href": "slides/24-intro-multilevel-models.html#announcements",
    "title": "Introduction to multilevel models",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nStatistics experience due Fri, Dec 09, 11:59pm\n\nExam 02: Mon, Dec 05 (evening) - Thu, Dec 08, 12pm (noon)\n\nExam 02 review on Mon Dec 05\nClick here for lecture recordings - available until Dec 05, 11:59pm\n\nSee Week 14 activities"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#learning-goals",
    "href": "slides/24-intro-multilevel-models.html#learning-goals",
    "title": "Introduction to multilevel models",
    "section": "Learning goals",
    "text": "Learning goals\n\nRecognize a potential for correlation in a data set\nIdentify observational units at varying levels\nUnderstand issues correlated data may cause in modeling\nUnderstand how random effects models can be used to take correlation into account"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#examples-of-correlated-data",
    "href": "slides/24-intro-multilevel-models.html#examples-of-correlated-data",
    "title": "Introduction to multilevel models",
    "section": "Examples of correlated data",
    "text": "Examples of correlated data\n\n\nIn an education study, scores for students from a particular teacher are typically more similar than scores of other students with a different teacher\nIn a study measuring depression indices weekly over a month, the four measures for the same patient tend to be more similar than depression indices from other patients\nIn political polling, opinions of members from the same household tend to be more similar than opinions of members from another household\n\n\n\nCorrelation among outcomes within the same group (teacher, patient, household) is called intraclass correlation"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#multilevel-data",
    "href": "slides/24-intro-multilevel-models.html#multilevel-data",
    "title": "Introduction to multilevel models",
    "section": "Multilevel data",
    "text": "Multilevel data\n\nWe can think of correlated data as having a multilevel structure\n\nPopulation elements are aggregated into groups\nThere are observational units and measurements at each level\n\n\n\n\nFor now we will focus on data with two levels:\n\nLevel one: Most basic level of observation\nLevel two: Groups formed from aggregated level-one observations\n\n\n\n\n\nExample: education\n\nLevel one: students in a class\nLevel two: class / teacher"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#two-types-of-effects",
    "href": "slides/24-intro-multilevel-models.html#two-types-of-effects",
    "title": "Introduction to multilevel models",
    "section": "Two types of effects",
    "text": "Two types of effects\n\n\nFixed effects: Effects that are of interest in the study\n\nCan think of these as effects whose interpretations would be included in a write up of the study\n\nRandom effects: Effects we‚Äôre not interested in studying but whose variability we want to understand\n\nCan think of these as effects whose interpretations would not necessarily be included in a write up of the study"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#example",
    "href": "slides/24-intro-multilevel-models.html#example",
    "title": "Introduction to multilevel models",
    "section": "Example",
    "text": "Example\nResearchers are interested in understanding the effect social media has on opinions about a proposed economic plan. They randomly select 1000 households. They ask each adult in the household how many minutes they spend on social media daily and whether they support the proposed economic plan.\n\nFixed effect: daily minutes on social media\nRandom effect: household"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#practice",
    "href": "slides/24-intro-multilevel-models.html#practice",
    "title": "Introduction to multilevel models",
    "section": "Practice",
    "text": "Practice\nResearchers conducted a randomized controlled study where patients were randomly assigned to either an anti-epileptic drug or a placebo. For each patient, the number of seizures at baseline was measured over a 2-week period. For four consecutive visits the number of seizures were determined over the past 2-week period. Patient age and sex along with visit number were recorded.\n\n\nWhat are the level one and level two observational units?\nWhat is the response variable?\nDescribe the within-group variation.\nWhat are the fixed effects? What are the random effects?\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#data-music-performance-anxiety",
    "href": "slides/24-intro-multilevel-models.html#data-music-performance-anxiety",
    "title": "Introduction to multilevel models",
    "section": "Data: Music performance anxiety",
    "text": "Data: Music performance anxiety\nThe data musicdata.csv come from the Sadler and Miller (2010) study of the emotional state of musicians before performances. The dataset contains information collected from 37 undergraduate music majors who completed the Positive Affect Negative Affect Schedule (PANAS), an instrument produces a measure of anxiety (negative affect) and a measure of happiness (positive affect). This analysis will focus on negative affect as a measure of performance anxiety."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#data-music-performance-anxiety-1",
    "href": "slides/24-intro-multilevel-models.html#data-music-performance-anxiety-1",
    "title": "Introduction to multilevel models",
    "section": "Data: Music performance anxiety",
    "text": "Data: Music performance anxiety\nThe primary variables we‚Äôll use are\n\nna: negative affect score on PANAS (the response variable)\nperform_type: type of performance (Solo, Large Ensemble, Small Ensemble)\ninstrument: type of instrument (Voice, Orchestral, Piano)"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#look-at-data",
    "href": "slides/24-intro-multilevel-models.html#look-at-data",
    "title": "Introduction to multilevel models",
    "section": "Look at data",
    "text": "Look at data\n\n\n\n\n\nid\ndiary\nperform_type\nna\ngender\ninstrument\n\n\n\n\n1\n1\nSolo\n11\nFemale\nvoice\n\n\n1\n2\nLarge Ensemble\n19\nFemale\nvoice\n\n\n1\n3\nLarge Ensemble\n14\nFemale\nvoice\n\n\n43\n1\nSolo\n19\nFemale\nvoice\n\n\n43\n2\nSolo\n13\nFemale\nvoice\n\n\n43\n3\nSmall Ensemble\n19\nFemale\nvoice\n\n\n\n\n\n\nWhat are the Level One observations? Level Two observations?\nWhat are the Level One variables? Level Two variables?"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#univariate-exploratory-data-analysis",
    "href": "slides/24-intro-multilevel-models.html#univariate-exploratory-data-analysis",
    "title": "Introduction to multilevel models",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis\nLevel One variables\nTwo ways to approach univariate EDA (visualizations and summary statistics) for Level One variables:\n\nUse individual observations (i.e., treat observations as independent)\nUse aggregated values for each Level Two observation\n\n\nLevel Two variables\n\nUse a data set that contains one row per Level Two observation"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#bivariate-exploratory-data-analysis",
    "href": "slides/24-intro-multilevel-models.html#bivariate-exploratory-data-analysis",
    "title": "Introduction to multilevel models",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis\nGoals\n\nExplore general association between the predictor and response variable\nExplore whether subjects at a given level of the predictor tend to have similar mean responses\nExplore whether variation in response differs at different levels of a predictor\n\nThere are two ways to visualize these associations:\n\nOne plot of individual observations (i.e., treat observations as independent)\nSeparate plots of responses vs.¬†predictor for each Level Two observation (lattice plots)"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#application-exercise",
    "href": "slides/24-intro-multilevel-models.html#application-exercise",
    "title": "Introduction to multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nComplete Part 2: Bivariate EDA\n\n\n\n08:00"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#questions-we-want-to-answer",
    "href": "slides/24-intro-multilevel-models.html#questions-we-want-to-answer",
    "title": "Introduction to multilevel models",
    "section": "Questions we want to answer",
    "text": "Questions we want to answer\nThe goal is to understand variability in performance anxiety (na) based on performance-level and musician-level characteristics.\nSpecifically:\n\nWhat is the association between performance type (large ensemble or not) and performance anxiety? Does the association differ based on instrument type (orchestral or not)?"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#linear-regression-model",
    "href": "slides/24-intro-multilevel-models.html#linear-regression-model",
    "title": "Introduction to multilevel models",
    "section": "Linear regression model",
    "text": "Linear regression model\n\nWhat is the problem with using the following model to draw conclusions?\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n15.721\n0.359\n43.778\n0.000\n\n\norchestra\n1.789\n0.552\n3.243\n0.001\n\n\nlarge_ensemble\n-0.277\n0.791\n-0.350\n0.727\n\n\norchestra:large_ensemble\n-1.709\n1.062\n-1.609\n0.108"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#other-modeling-approaches",
    "href": "slides/24-intro-multilevel-models.html#other-modeling-approaches",
    "title": "Introduction to multilevel models",
    "section": "Other modeling approaches",
    "text": "Other modeling approaches\n1Ô∏è‚É£ Condense each musician‚Äôs set of responses into a single outcome (e.g., mean max, last observation, etc.) and fit a linear model on these condensed observations\n\nLeaves few observations (37) to fit the model\nIgnoring a lot of information in the multiple observations for each musician\n\n\n2Ô∏è‚É£ Fit a separate model for each musician understand the association between performance type (Level One models). Then fit a system of Level Two models to predict the fitted coefficients in the Level One model for each subject based on instrument type (Level Two model).\n\n\nLet‚Äôs look at approach #2"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#level-one-model",
    "href": "slides/24-intro-multilevel-models.html#level-one-model",
    "title": "Introduction to multilevel models",
    "section": "Level One model",
    "text": "Level One model\nWe‚Äôll start with the Level One model to understand the association between performance type and performance anxiety for the \\(i^{th}\\) musician.\n\n\\[na_{ij} = a_i + b_i ~ LargeEnsemble_{ij} + \\epsilon_i, \\hspace{5mm} \\epsilon_{ij} \\sim N(0,\\sigma^2)\\]\n\nWhy is it more meaningful to use performance type for the Level One model than instrument?\n\n\nFor now, estimate \\(a_i\\) and \\(b_i\\) using least-squares regression."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#level-one-model-for-one-student",
    "href": "slides/24-intro-multilevel-models.html#level-one-model-for-one-student",
    "title": "Introduction to multilevel models",
    "section": "Level One model for one student",
    "text": "Level One model for one student\nBelow is partial data for observation #22\n\n\n\n\n\nid\ndiary\nperform_type\ninstrument\nna\n\n\n\n\n22\n1\nSolo\norchestral instrument\n24\n\n\n22\n2\nLarge Ensemble\norchestral instrument\n21\n\n\n22\n3\nLarge Ensemble\norchestral instrument\n14\n\n\n22\n13\nLarge Ensemble\norchestral instrument\n12\n\n\n22\n14\nLarge Ensemble\norchestral instrument\n19\n\n\n22\n15\nSolo\norchestral instrument\n25"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#level-one-model-for-musician-22",
    "href": "slides/24-intro-multilevel-models.html#level-one-model-for-musician-22",
    "title": "Introduction to multilevel models",
    "section": "Level One model for musician 22",
    "text": "Level One model for musician 22\n\nid_22 &lt;- music |&gt;\n  filter(id == 22)\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(na ~ large_ensemble, data = id_22) |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.500\n1.96\n12.503\n0.000\n\n\nlarge_ensemble\n-7.833\n2.53\n-3.097\n0.009"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#application-exercise-1",
    "href": "slides/24-intro-multilevel-models.html#application-exercise-1",
    "title": "Introduction to multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nSee Part 3: Level One Models to fit the Level One model for all 37 musicians."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#level-one-model-summaries",
    "href": "slides/24-intro-multilevel-models.html#level-one-model-summaries",
    "title": "Introduction to multilevel models",
    "section": "Level One model summaries",
    "text": "Level One model summaries\n\nRecreated from BMLR Figure 8.9\nNow let‚Äôs consider if there is an association between the estimated slopes, estimated intercepts, and the type of instrument."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#level-two-model",
    "href": "slides/24-intro-multilevel-models.html#level-two-model",
    "title": "Introduction to multilevel models",
    "section": "Level Two Model",
    "text": "Level Two Model\nThe slope and intercept for the \\(i^{th}\\) musician can be modeled as\n\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i + u_i \\\\\n&b_i = \\beta_0 + \\beta_1 ~ Orchestra_i + v_i\\end{aligned}\\]\nNote the response variable in the Level Two models are not observed outcomes but the (fitted) slope and intercept from each musician"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#application-exercise-2",
    "href": "slides/24-intro-multilevel-models.html#application-exercise-2",
    "title": "Introduction to multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nSee Part 4: Level Two Models."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#estimated-coefficients-by-instrument",
    "href": "slides/24-intro-multilevel-models.html#estimated-coefficients-by-instrument",
    "title": "Introduction to multilevel models",
    "section": "Estimated coefficients by instrument",
    "text": "Estimated coefficients by instrument"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#level-two-model-1",
    "href": "slides/24-intro-multilevel-models.html#level-two-model-1",
    "title": "Introduction to multilevel models",
    "section": "Level Two model",
    "text": "Level Two model\nModel for intercepts\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n16.283\n0.671\n24.249\n0.000\n\n\norchestra\n1.411\n0.991\n1.424\n0.163\n\n\n\n\n\nModel for slopes\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.771\n0.851\n-0.906\n0.373\n\n\norchestra\n-1.406\n1.203\n-1.168\n0.253"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#writing-out-the-models",
    "href": "slides/24-intro-multilevel-models.html#writing-out-the-models",
    "title": "Introduction to multilevel models",
    "section": "Writing out the models",
    "text": "Writing out the models\nLevel One\n\\[\\hat{na}_{ij}  = \\hat{a}_i + \\hat{b}_i ~ LargeEnsemble_{ij}\\]\nfor each musician.\n\nLevel Two\n\\[\\begin{aligned}&\\hat{a}_i = 16.283 + 1.441 ~ Orchestra_i \\\\\n&\\hat{b}_i = -0.771 - 1.406 ~ Orchestra_i\\end{aligned}\\]"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#composite-model",
    "href": "slides/24-intro-multilevel-models.html#composite-model",
    "title": "Introduction to multilevel models",
    "section": "Composite model",
    "text": "Composite model\n\\[\\begin{aligned}\\hat{na}_i &= 16.283 + 1.441 ~ Orchestra_i - 0.771 ~ LargeEnsemble_{ij} \\\\\n&- 1.406 ~ Orchestra:LargeEnsemble_{ij}\\end{aligned}\\]\n(Note that we also have the error terms \\(\\epsilon_{ij}, u_i, v_i\\) that we will discuss next class.)\n\n\nWhat is the predicted average performance anxiety before solos and small ensemble performances for vocalists and keyboardists? For those who place orchestral instruments?\nWhat is the predicted average performance anxiety before large ensemble performances for those who play orchestral instruments?"
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#disadvantages-to-this-approach",
    "href": "slides/24-intro-multilevel-models.html#disadvantages-to-this-approach",
    "title": "Introduction to multilevel models",
    "section": "Disadvantages to this approach",
    "text": "Disadvantages to this approach\n‚ö†Ô∏è Weighs each musician the same regardless of number of diary entries\n‚ö†Ô∏è Drops subjects who have missing values for slope (7 individuals who didn‚Äôt play a large ensemble performance)\n‚ö†Ô∏è Does not share strength effectively across individuals."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#application-exercise-3",
    "href": "slides/24-intro-multilevel-models.html#application-exercise-3",
    "title": "Introduction to multilevel models",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 15: Introduction to Multilevel models\n\nSee Part 5: Distribution of \\(R^2\\) values."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#next-time",
    "href": "slides/24-intro-multilevel-models.html#next-time",
    "title": "Introduction to multilevel models",
    "section": "Next time",
    "text": "Next time\nWe will use a unified approach that utilizes likelihood-based methods to address some of these drawbacks."
  },
  {
    "objectID": "slides/24-intro-multilevel-models.html#acknowledgements",
    "href": "slides/24-intro-multilevel-models.html#acknowledgements",
    "title": "Introduction to multilevel models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nThe content in the slides is from\n\nBMLR: Chapter 7 - Correlated data\nBMLR: Chapter 8 - Introduction to Multilevel Models\n\nSadler, Michael E., and Christopher J. Miller. 2010. ‚ÄúPerformance Anxiety: A Longitudinal Study of the Roles of Personality and Experience in Musicians.‚Äù Social Psychological and Personality Science 1 (3): 280‚Äì87. http://dx.doi.org/10.1177/1948550610370492.\n\n\n\n\nüîó Week 14"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#announcements",
    "href": "slides/18-mlr-conditions.html#announcements",
    "title": "MLR conditions + multicollinearity",
    "section": "Announcements",
    "text": "Announcements\n\nProject proposal due Fri, Nov 04\nHW 03 due Mon, Nov 07 at 11:59pm\n\nReleased later today\n\nSee Week 10 activities"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#topics",
    "href": "slides/18-mlr-conditions.html#topics",
    "title": "MLR conditions + multicollinearity",
    "section": "Topics",
    "text": "Topics\n\nConditions for inference\nMulticollinearity"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#computational-setup",
    "href": "slides/18-mlr-conditions.html#computational-setup",
    "title": "MLR conditions + multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)      # for tables\nlibrary(patchwork)  # for laying out plots\nlibrary(rms)        # for vif\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#data-rail_trail",
    "href": "slides/18-mlr-conditions.html#data-rail_trail",
    "title": "MLR conditions + multicollinearity",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\nrail_trail &lt;- read_csv(here::here(\"slides\", \"data/rail_trail.csv\"))\nrail_trail\n\n# A tibble: 90 √ó 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ‚Ä¶ with 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#variables",
    "href": "slides/18-mlr-conditions.html#variables",
    "title": "MLR conditions + multicollinearity",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of ‚ÄúFall‚Äù, ‚ÄúSpring‚Äù, or ‚ÄúSummer‚Äù\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of ‚Äúweekday‚Äù or ‚Äúweekend‚Äù"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#full-model",
    "href": "slides/18-mlr-conditions.html#full-model",
    "title": "MLR conditions + multicollinearity",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 √ó 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit$fit)"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#model-conditions",
    "href": "slides/18-mlr-conditions.html#model-conditions",
    "title": "MLR conditions + multicollinearity",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#residuals-vs.-predicted-values",
    "href": "slides/18-mlr-conditions.html#residuals-vs.-predicted-values",
    "title": "MLR conditions + multicollinearity",
    "section": "Residuals vs.¬†predicted values",
    "text": "Residuals vs.¬†predicted values\n\nggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#linearity-residuals-vs.-predicted",
    "href": "slides/18-mlr-conditions.html#linearity-residuals-vs.-predicted",
    "title": "MLR conditions + multicollinearity",
    "section": "Linearity: Residuals vs.¬†predicted",
    "text": "Linearity: Residuals vs.¬†predicted\n\nDoes the linearity condition appear to be met?"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#linearity-residuals-vs.-predicted-1",
    "href": "slides/18-mlr-conditions.html#linearity-residuals-vs.-predicted-1",
    "title": "MLR conditions + multicollinearity",
    "section": "Linearity: Residuals vs.¬†predicted",
    "text": "Linearity: Residuals vs.¬†predicted\nLook at individual plots of residuals vs.¬†each predictor, particularly if there are potential issues in the plot of residuals vs.¬†predicted values."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#linearity-residuals-vs.-each-predictor",
    "href": "slides/18-mlr-conditions.html#linearity-residuals-vs.-each-predictor",
    "title": "MLR conditions + multicollinearity",
    "section": "Linearity: Residuals vs.¬†each predictor",
    "text": "Linearity: Residuals vs.¬†each predictor"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-linearity",
    "href": "slides/18-mlr-conditions.html#checking-linearity",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plots of residuals vs.¬†hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition is not satisfied given these plots."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-constant-variance",
    "href": "slides/18-mlr-conditions.html#checking-constant-variance",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-constant-variance-1",
    "href": "slides/18-mlr-conditions.html#checking-constant-variance-1",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-normality",
    "href": "slides/18-mlr-conditions.html#checking-normality",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking normality",
    "text": "Checking normality"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#overlaying-a-density-plot-on-a-histogram",
    "href": "slides/18-mlr-conditions.html#overlaying-a-density-plot-on-a-histogram",
    "title": "MLR conditions + multicollinearity",
    "section": "Overlaying a density plot on a histogram",
    "text": "Overlaying a density plot on a histogram\n\nüìã AE 11: MLR Inference\n\n\n\n\nEx 2. Fill in the code to recreate the following visualization in R based on the results of the model."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-independence",
    "href": "slides/18-mlr-conditions.html#checking-independence",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-independence-1",
    "href": "slides/18-mlr-conditions.html#checking-independence-1",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†order of data collection:\n\nggplot(rt_full_aug, aes(y = .resid, x = 1:nrow(rt_full_aug))) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-independence-2",
    "href": "slides/18-mlr-conditions.html#checking-independence-2",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†predicted values by season:"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-independence-3",
    "href": "slides/18-mlr-conditions.html#checking-independence-3",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†predicted values by day_type:"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#checking-independence-4",
    "href": "slides/18-mlr-conditions.html#checking-independence-4",
    "title": "MLR conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\n\nNo clear pattern in the residuals vs.¬†order of data collection plot and the model predicts similarly for seasons and day types.\nIndependence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#why-multicollinearity-is-a-problem",
    "href": "slides/18-mlr-conditions.html#why-multicollinearity-is-a-problem",
    "title": "MLR conditions + multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWe can‚Äôt include two variables that have a perfect linear association with each other\nIf we did so, we could not find unique estimates for the model coefficients"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#example",
    "href": "slides/18-mlr-conditions.html#example",
    "title": "MLR conditions + multicollinearity",
    "section": "Example",
    "text": "Example\nSuppose the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#example-1",
    "href": "slides/18-mlr-conditions.html#example-1",
    "title": "MLR conditions + multicollinearity",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the ‚Äúbest‚Äù combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#why-multicollinearity-is-a-problem-1",
    "href": "slides/18-mlr-conditions.html#why-multicollinearity-is-a-problem-1",
    "title": "MLR conditions + multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have almost perfect collinearities (i.e.¬†highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference or prediction"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#detecting-multicollinearity",
    "href": "slides/18-mlr-conditions.html#detecting-multicollinearity",
    "title": "MLR conditions + multicollinearity",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when‚Ä¶\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\n\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nInclude a quadratic in the model mean-centering the variable first\nIncluding interactions between two or more continuous variables"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/18-mlr-conditions.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR conditions + multicollinearity",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#detecting-multicollinearity-vif",
    "href": "slides/18-mlr-conditions.html#detecting-multicollinearity-vif",
    "title": "MLR conditions + multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#detecting-multicollinearity-vif-1",
    "href": "slides/18-mlr-conditions.html#detecting-multicollinearity-vif-1",
    "title": "MLR conditions + multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity - Variables with similar values of VIF are typically the ones correlated with each other\n\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#vif-for-rail-trail-model",
    "href": "slides/18-mlr-conditions.html#vif-for-rail-trail-model",
    "title": "MLR conditions + multicollinearity",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit$fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#model-without-hightemp",
    "href": "slides/18-mlr-conditions.html#model-without-hightemp",
    "title": "MLR conditions + multicollinearity",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#model-without-avgtemp",
    "href": "slides/18-mlr-conditions.html#model-without-avgtemp",
    "title": "MLR conditions + multicollinearity",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#choosing-a-model",
    "href": "slides/18-mlr-conditions.html#choosing-a-model",
    "title": "MLR conditions + multicollinearity",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel with hightemp removed:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel with avgtemp removed:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model with avgtemp removed is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#final-model",
    "href": "slides/18-mlr-conditions.html#final-model",
    "title": "MLR conditions + multicollinearity",
    "section": "Final model",
    "text": "Final model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#recap",
    "href": "slides/18-mlr-conditions.html#recap",
    "title": "MLR conditions + multicollinearity",
    "section": "Recap",
    "text": "Recap\n\nConditions for inference\nMulticollinearity"
  },
  {
    "objectID": "slides/23-logistic-inf.html#announcements",
    "href": "slides/23-logistic-inf.html#announcements",
    "title": "LR: Inference + conditions",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nHW 04 due Mon, Nov 21, 11:59pm\nProject Round 1 submission (optional) due Tue, Nov 22, 11:59pm\nStatistics experience due Fri, Dec 09, 11:59pm\n\nNo class Monday - Click here to sign up for project meeting (optional)\n\nMust sign up by Sun, Nov 20 at 11:59pm\n\nSee Week 12 activities"
  },
  {
    "objectID": "slides/23-logistic-inf.html#topics",
    "href": "slides/23-logistic-inf.html#topics",
    "title": "LR: Inference + conditions",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/23-logistic-inf.html#computational-setup",
    "href": "slides/23-logistic-inf.html#computational-setup",
    "title": "LR: Inference + conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/23-logistic-inf.html#risk-of-coronary-heart-disease",
    "href": "slides/23-logistic-inf.html#risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/23-logistic-inf.html#data-prep",
    "href": "slides/23-logistic-inf.html#data-prep",
    "title": "LR: Inference + conditions",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(here::here(\"slides\", \"data/framingham.csv\")) |&gt;\n  select(age, education, TenYearCHD, totChol, currentSmoker) |&gt;\n  drop_na() |&gt;\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n\n# A tibble: 4,086 √ó 6\n     age education TenYearCHD totChol currentSmoker high_risk\n   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;    \n 1    39 4                  0     195 0             0        \n 2    46 2                  0     250 0             0        \n 3    48 1                  0     245 1             0        \n 4    61 3                  1     225 1             1        \n 5    46 3                  0     285 1             0        \n 6    43 2                  0     228 0             0        \n 7    63 1                  1     205 0             1        \n 8    45 2                  0     313 1             0        \n 9    52 1                  0     260 0             0        \n10    43 1                  0     225 1             0        \n# ‚Ä¶ with 4,076 more rows"
  },
  {
    "objectID": "slides/23-logistic-inf.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/23-logistic-inf.html#modeling-risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nUsing age and education:\n\nrisk_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/23-logistic-inf.html#model-output",
    "href": "slides/23-logistic-inf.html#model-output",
    "title": "LR: Inference + conditions",
    "section": "Model output",
    "text": "Model output\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.508 + 0.076 ~ \\text{age} - 0.245 ~ \\text{ed2} - 0.236 ~ \\text{ed3} - 0.024 ~ \\text{ed4}}\n\\]"
  },
  {
    "objectID": "slides/23-logistic-inf.html#hypothesis-test-for-beta_j",
    "href": "slides/23-logistic-inf.html#hypothesis-test-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Hypothesis test for \\(\\beta_j\\)",
    "text": "Hypothesis test for \\(\\beta_j\\)\nHypotheses: \\(H_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_j \\neq 0\\)\n\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}\\]\n\n\nP-value: \\(P(|Z| &gt; |z|)\\), where \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/23-logistic-inf.html#confidence-interval-for-beta_j",
    "href": "slides/23-logistic-inf.html#confidence-interval-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\nWe can calculate the C% confidence interval for \\(\\beta_j\\) as the following:\n\\[\n\\Large{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}\n\\]\nwhere \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\nNote\n\n\nThis is an interval for the change in the log-odds for every one unit increase in \\(x_j\\)"
  },
  {
    "objectID": "slides/23-logistic-inf.html#interpretation-in-terms-of-the-odds",
    "href": "slides/23-logistic-inf.html#interpretation-in-terms-of-the-odds",
    "title": "LR: Inference + conditions",
    "section": "Interpretation in terms of the odds",
    "text": "Interpretation in terms of the odds\nThe change in odds for every one unit increase in \\(x_j\\).\n\\[\n\\Large{e^{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}}\n\\]\n\nInterpretation: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), the odds multiply by a factor of \\(e^{\\hat{\\beta}_j - z^* SE_{\\hat{\\beta}_j}}\\) to \\(e^{\\hat{\\beta}_j + z^* SE_{\\hat{\\beta}_j}}\\), holding all else constant."
  },
  {
    "objectID": "slides/23-logistic-inf.html#coefficient-for-age",
    "href": "slides/23-logistic-inf.html#coefficient-for-age",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\n\nHypotheses:\n\\[\nH_0: \\beta_{age} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{age} \\neq 0\n\\]"
  },
  {
    "objectID": "slides/23-logistic-inf.html#coefficient-for-age-1",
    "href": "slides/23-logistic-inf.html#coefficient-for-age-1",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\nTest statistic:\n\\[z = \\frac{0.07559 - 0}{0.00554} = 13.64\n\\]"
  },
  {
    "objectID": "slides/23-logistic-inf.html#coefficient-for-age-2",
    "href": "slides/23-logistic-inf.html#coefficient-for-age-2",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\nP-value:\n\\[\nP(|Z| &gt; |13.64|) \\approx 0\n\\]\n\n\n2 * pnorm(13.64,lower.tail = FALSE)\n\n[1] 2.315606e-42"
  },
  {
    "objectID": "slides/23-logistic-inf.html#coefficient-for-age-3",
    "href": "slides/23-logistic-inf.html#coefficient-for-age-3",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\nConclusion:\nThe p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that age is a statistically significant predictor of whether someone is high risk of having heart disease, after accounting for education."
  },
  {
    "objectID": "slides/23-logistic-inf.html#ci-for-age",
    "href": "slides/23-logistic-inf.html#ci-for-age",
    "title": "LR: Inference + conditions",
    "section": "CI for age",
    "text": "CI for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\n\nInterpret the 95% confidence interval for age in terms of the odds of being high risk for heart disease."
  },
  {
    "objectID": "slides/23-logistic-inf.html#log-likelihood",
    "href": "slides/23-logistic-inf.html#log-likelihood",
    "title": "LR: Inference + conditions",
    "section": "Log likelihood",
    "text": "Log likelihood\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\nDeviance = \\(-2 \\log L\\)\n\n\\(-2 \\log L\\) follows a \\(\\chi^2\\) distribution with \\(n - p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/23-logistic-inf.html#model-with-age-and-education",
    "href": "slides/23-logistic-inf.html#model-with-age-and-education",
    "title": "LR: Inference + conditions",
    "section": "Model with age and education",
    "text": "Model with age and education\n\nShould we add currentSmoker to this model?\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264"
  },
  {
    "objectID": "slides/23-logistic-inf.html#should-we-add-currentsmoker-to-the-model",
    "href": "slides/23-logistic-inf.html#should-we-add-currentsmoker-to-the-model",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nFirst model, reduced:\n\nrisk_fit_reduced &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\n\n\nSecond model, full:\n\nrisk_fit_full &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/23-logistic-inf.html#model-selection",
    "href": "slides/23-logistic-inf.html#model-selection",
    "title": "LR: Inference + conditions",
    "section": "Model selection",
    "text": "Model selection\nUse AIC or BIC for model selection\n\\[\n\\begin{align}\n&AIC = - 2 * \\log L - \\color{purple}{n\\log(n)}+ 2(p +1)\\\\[5pt]\n&BIC =- 2 * \\log L - \\color{purple}{n\\log(n)} + log(n)\\times(p+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/23-logistic-inf.html#aic-from-the-glance-function",
    "href": "slides/23-logistic-inf.html#aic-from-the-glance-function",
    "title": "LR: Inference + conditions",
    "section": "AIC from the glance() function",
    "text": "AIC from the glance() function\nLet‚Äôs look at the AIC for the model that includes age, education, and currentSmoker\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nCalculating AIC\n\n- 2 * glance(risk_fit_full)$logLik + 2 * (5 + 1)\n\n[1] 3233.901"
  },
  {
    "objectID": "slides/23-logistic-inf.html#comparing-the-models-using-aic",
    "href": "slides/23-logistic-inf.html#comparing-the-models-using-aic",
    "title": "LR: Inference + conditions",
    "section": "Comparing the models using AIC",
    "text": "Comparing the models using AIC\nLet‚Äôs compare the full and reduced models using AIC.\n\nglance(risk_fit_reduced)$AIC\n\n[1] 3254.187\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\n\nBased on AIC, which model would you choose?"
  },
  {
    "objectID": "slides/23-logistic-inf.html#comparing-the-models-using-bic",
    "href": "slides/23-logistic-inf.html#comparing-the-models-using-bic",
    "title": "LR: Inference + conditions",
    "section": "Comparing the models using BIC",
    "text": "Comparing the models using BIC\nLet‚Äôs compare the full and reduced models using BIC\n\nglance(risk_fit_reduced)$BIC\n\n[1] 3285.764\n\nglance(risk_fit_full)$BIC\n\n[1] 3271.793\n\n\n\n\nBased on BIC, which model would you choose?"
  },
  {
    "objectID": "slides/23-logistic-inf.html#the-model",
    "href": "slides/23-logistic-inf.html#the-model",
    "title": "LR: Inference + conditions",
    "section": "The model",
    "text": "The model\nLet‚Äôs predict high_risk from age, total cholesterol, and whether the patient is a current smoker:\n\nrisk_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + totChol + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-6.673\n0.378\n-17.647\n0.000\n-7.423\n-5.940\n\n\nage\n0.082\n0.006\n14.344\n0.000\n0.071\n0.094\n\n\ntotChol\n0.002\n0.001\n1.940\n0.052\n0.000\n0.004\n\n\ncurrentSmoker1\n0.443\n0.094\n4.733\n0.000\n0.260\n0.627"
  },
  {
    "objectID": "slides/23-logistic-inf.html#conditions-for-logistic-regression",
    "href": "slides/23-logistic-inf.html#conditions-for-logistic-regression",
    "title": "LR: Inference + conditions",
    "section": "Conditions for logistic regression",
    "text": "Conditions for logistic regression\n\nLinearity: The log-odds have a linear relationship with the predictors.\nRandomness: The data were obtained from a random process\nIndependence: The observations are independent from one another."
  },
  {
    "objectID": "slides/23-logistic-inf.html#empirical-logit",
    "href": "slides/23-logistic-inf.html#empirical-logit",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit",
    "text": "Empirical logit\nThe empirical logit is the log of the observed odds:\n\\[\n\\text{logit}(\\hat{p}) = \\log\\Big(\\frac{\\hat{p}}{1 - \\hat{p}}\\Big) = \\log\\Big(\\frac{\\# \\text{Yes}}{\\# \\text{No}}\\Big)\n\\]"
  },
  {
    "objectID": "slides/23-logistic-inf.html#calculating-empirical-logit-categorical-predictor",
    "href": "slides/23-logistic-inf.html#calculating-empirical-logit-categorical-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (categorical predictor)",
    "text": "Calculating empirical logit (categorical predictor)\nIf the predictor is categorical, we can calculate the empirical logit for each level of the predictor.\n\nheart_disease |&gt;\n  count(currentSmoker, high_risk) |&gt;\n  group_by(currentSmoker) |&gt;\n  mutate(prop = n/sum(n)) |&gt;\n  filter(high_risk == \"1\") |&gt;\n  mutate(emp_logit = log(prop/(1-prop)))\n\n# A tibble: 2 √ó 5\n# Groups:   currentSmoker [2]\n  currentSmoker high_risk     n  prop emp_logit\n  &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 0             1           301 0.145     -1.77\n2 1             1           318 0.158     -1.67"
  },
  {
    "objectID": "slides/23-logistic-inf.html#calculating-empirical-logit-quantitative-predictor",
    "href": "slides/23-logistic-inf.html#calculating-empirical-logit-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (quantitative predictor)",
    "text": "Calculating empirical logit (quantitative predictor)\n\nDivide the range of the predictor into intervals with approximately equal number of cases. (If you have enough observations, use 5 - 10 intervals.)\nCalculate the mean value of the predictor in each interval.\nCompute the empirical logit for each interval.\n\n\nThen, create a plot of the empirical logit versus the mean value of the predictor in each interval."
  },
  {
    "objectID": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor",
    "href": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nCreated using dplyr and ggplot functions."
  },
  {
    "objectID": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-1",
    "href": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-1",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nCreated using dplyr and ggplot functions.\n\nheart_disease |&gt; \n  mutate(age_bin = cut_interval(age, n = 10)) |&gt;\n  group_by(age_bin) |&gt;\n  mutate(mean_age = mean(age)) |&gt;\n  count(mean_age, high_risk) |&gt;\n  mutate(prop = n/sum(n)) |&gt;\n  filter(high_risk == \"1\") |&gt;\n  mutate(emp_logit = log(prop/(1-prop))) |&gt;\n  ggplot(aes(x = mean_age, y = emp_logit)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Mean Age\", \n       y = \"Empirical logit\")"
  },
  {
    "objectID": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-2",
    "href": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-2",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nUsing the emplogitplot1 function from the Stat2Data R package\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)"
  },
  {
    "objectID": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-interactions",
    "href": "slides/23-logistic-inf.html#empirical-logit-plot-in-r-interactions",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (interactions)",
    "text": "Empirical logit plot in R (interactions)\nUsing the emplogitplot2 function from the Stat2Data R package\n\nemplogitplot2(high_risk ~ age + currentSmoker, data = heart_disease, \n              ngroups = 10, \n              putlegend = \"bottomright\")"
  },
  {
    "objectID": "slides/23-logistic-inf.html#checking-linearity",
    "href": "slides/23-logistic-inf.html#checking-linearity",
    "title": "LR: Inference + conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\n\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\nemplogitplot1(high_risk ~ totChol, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the predictor variables."
  },
  {
    "objectID": "slides/23-logistic-inf.html#checking-randomness",
    "href": "slides/23-logistic-inf.html#checking-randomness",
    "title": "LR: Inference + conditions",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n‚úÖ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S. in regards to health characteristics and risk of heart disease."
  },
  {
    "objectID": "slides/23-logistic-inf.html#checking-independence",
    "href": "slides/23-logistic-inf.html#checking-independence",
    "title": "LR: Inference + conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n\n‚úÖ The independence condition is satisfied. It is reasonable to conclude that the participants‚Äô health characteristics are independent of one another."
  },
  {
    "objectID": "slides/23-logistic-inf.html#comparing-nested-models",
    "href": "slides/23-logistic-inf.html#comparing-nested-models",
    "title": "LR: Inference + conditions",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes predictors \\(x_1, \\ldots, x_q\\)\nFull Model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\nTo do so, we will use the Drop-in-deviance test, also known as the Nested Likelihood Ratio test"
  },
  {
    "objectID": "slides/23-logistic-inf.html#drop-in-deviance-test",
    "href": "slides/23-logistic-inf.html#drop-in-deviance-test",
    "title": "LR: Inference + conditions",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\n\n\nP-value: \\(P(\\chi^2 &gt; G)\\), calculated using a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models"
  },
  {
    "objectID": "slides/23-logistic-inf.html#chi2-distribution",
    "href": "slides/23-logistic-inf.html#chi2-distribution",
    "title": "LR: Inference + conditions",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/23-logistic-inf.html#should-we-add-currentsmoker-to-the-model-1",
    "href": "slides/23-logistic-inf.html#should-we-add-currentsmoker-to-the-model-1",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate deviance for each model:\n\n(dev_reduced &lt;- glance(risk_fit_reduced)$deviance)\n\n[1] 3244.187\n\n(dev_full &lt;- glance(risk_fit_full)$deviance)\n\n[1] 3221.901\n\n\n\nDrop-in-deviance test statistic:\n\n(test_stat &lt;- dev_reduced - dev_full)\n\n[1] 22.2863"
  },
  {
    "objectID": "slides/23-logistic-inf.html#should-we-add-currentsmoker-to-the-model-2",
    "href": "slides/23-logistic-inf.html#should-we-add-currentsmoker-to-the-model-2",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate the p-value using a pchisq(), with degrees of freedom equal to the number of new model terms in the second model:\n\npchisq(test_stat, 1, lower.tail = FALSE) \n\n[1] 2.348761e-06\n\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that the coefficient of currentSmoker is not equal to 0. Therefore, we should add it to the model."
  },
  {
    "objectID": "slides/23-logistic-inf.html#drop-in-deviance-test-in-r",
    "href": "slides/23-logistic-inf.html#drop-in-deviance-test-in-r",
    "title": "LR: Inference + conditions",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced$fit, risk_fit_full$fit, test = \"Chisq\") |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nResid..Df\nResid..Dev\ndf\nDeviance\np.value\n\n\n\n\nhigh_risk ~ age + education\n4081\n3244.187\nNA\nNA\nNA\n\n\nhigh_risk ~ age + education + currentSmoker\n4080\n3221.901\n1\n22.286\n0\n\n\n\n\n\n\n\n\n\n\nüîó Week 12"
  },
  {
    "objectID": "slides/22-model-compare.html#announcements",
    "href": "slides/22-model-compare.html#announcements",
    "title": "Logistic Regression: Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nHW 04 due Mon, Nov 21, 11:59pm (released later today)\nUpcoming dates:\n\nProject deadlines (see next slide)\nExam 02: Mon, Dec 05, 7pm - Thu, Dec 08, 12pm (noon)\nStatistics experience due Fri, Dec 09, 11:59pm\n\nSee Week 12 activities"
  },
  {
    "objectID": "slides/22-model-compare.html#project",
    "href": "slides/22-model-compare.html#project",
    "title": "Logistic Regression: Model comparison",
    "section": "Project",
    "text": "Project\n\nProposals\n\nFeedback is posted throughout document, so scroll through all pages of the document for comments (not just the comments section of the rubric)\nRecall: The final project grade is the sum of the points on each part\n\nComing up next:\n\nProject meetings (optional): Mon, Nov 21 (sign up sheet available later this week)\nRound 1 submission (optional) due Tue, Nov 22\nWritten report due Fri, Dec 09"
  },
  {
    "objectID": "slides/22-model-compare.html#application-exercise",
    "href": "slides/22-model-compare.html#application-exercise",
    "title": "Logistic Regression: Model comparison",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 14: Logistic Regression - Model Comparison\n\n\nSit with your lab groups.\nSubmit questions and responses on Ed Discussion:\n\n10:15am: edstem.org/us/courses/26900/discussion/2154489\n3:30pm: edstem.org/us/courses/26900/discussion/2154491\n\n\n\n\n\nüîó Week 12"
  },
  {
    "objectID": "slides/26-exam-02-review.html#announcements",
    "href": "slides/26-exam-02-review.html#announcements",
    "title": "Exam 02 review",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nTeam Feedback #2 due Tue, Dec 06, 11:59pm (check for email from Teammates)\nStatistics experience due Fri, Dec 09, 11:59pm\nProject written report due Fri, Dec 09, 11:59pm\n\nExam 02: Mon, Dec 05 (evening) - Thu, Dec 08, 12pm (noon)\n\nClick here for lecture recordings - available until Dec 05, 11:59pm"
  },
  {
    "objectID": "slides/26-exam-02-review.html#course-evals",
    "href": "slides/26-exam-02-review.html#course-evals",
    "title": "Exam 02 review",
    "section": "Course evals",
    "text": "Course evals\n\nCourse and TA evaluations are now available (check email for TA evaluations).\nIf there is at least 80% on the course evaluations and TA evaluations, everyone in the class will get 1 point on Exam 02.\n\nThis will be determined separately for the Sections 001 and 002."
  },
  {
    "objectID": "slides/26-exam-02-review.html#project",
    "href": "slides/26-exam-02-review.html#project",
    "title": "Exam 02 review",
    "section": "Project",
    "text": "Project\n\nWritten report submission: due Fri, Dec 09, 11:59pm (accepted until Sun, Dec 11, 11:59pm)\n\nSubmit by pushing to your GitHub repo. There is  no submission on Gradescope\n\nNext steps:\n\nVideo + slides: due Wed, Dec 14, 11:59pm\nRepo organization due Wed, Dec 14, 11:59pm\nVideo comments: due Fri, Dec 16, 11:59pm\n\nLink: sta210-fa22.netlify.app/project-instructions"
  },
  {
    "objectID": "slides/26-exam-02-review.html#exam-instructions",
    "href": "slides/26-exam-02-review.html#exam-instructions",
    "title": "Exam 02 review",
    "section": "Exam instructions",
    "text": "Exam instructions\n\n\nThe exam is an individual assignment. Everything in your repository is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor. For example, you may not communicate with other students, the TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nNo TA office hours will be held during the exam. You may not email the TAs questions about the exam.\nEd Discussion is only for questions about the final project.\nEmail Prof.¬†Tackett with ‚ÄúSTA 210 Exam‚Äù in the subject line if you have questions"
  },
  {
    "objectID": "slides/26-exam-02-review.html#exam-02-logistics",
    "href": "slides/26-exam-02-review.html#exam-02-logistics",
    "title": "Exam 02 review",
    "section": "Exam 02 logistics",
    "text": "Exam 02 logistics\n\n\nReleased today ~ 7pm (will receive email) and due Thu, Dec 08, 12pm (noon)\nExam instructions can be found in the README of the exam-02 Repo\n\nExercise prompts will be in exam-02.qmd\n\nCovers everything we‚Äôve done Weeks 06 - 13\nOffice hours during exam:\n\nWed, Dec 07 during your class period (Zoom link in Sakai)\nProf.¬†Tackett office hours\n\nMon, 1 - 2pm\nThu, 10 - 11am"
  },
  {
    "objectID": "slides/26-exam-02-review.html#assessment-criteria",
    "href": "slides/26-exam-02-review.html#assessment-criteria",
    "title": "Exam 02 review",
    "section": "Assessment criteria",
    "text": "Assessment criteria\n\nYou can identify the correct approach, analysis method, and/or inferential results required to answer the question.\nYou understand the correct conditions and diagnostics needed to determine whether the conclusions drawn from the model will be reliable\nYou can write results and conclusions in a meaningful way that can be understood by a general audience (think a business or research partner)\nYou can produce a report that is suitable for a professional audience (e.g., narrative is written in complete sentences, all graphs have proper titles and axis labels, there is not extraneous output, all LaTex is rendered)\nYou can conduct the analysis using a reproducible data analysis workflow that incorporates version control"
  },
  {
    "objectID": "slides/ANOVA-Table.html",
    "href": "slides/ANOVA-Table.html",
    "title": "ANOVA Output in R",
    "section": "",
    "text": "We will use the Tips data set for this example.\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nThe variables of interest in this analysis are"
  },
  {
    "objectID": "slides/ANOVA-Table.html#model-fit",
    "href": "slides/ANOVA-Table.html#model-fit",
    "title": "ANOVA Output in R",
    "section": "Model fit",
    "text": "Model fit\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.17\n0.37\n-0.46\n0.64\n\n\nParty\n1.84\n0.12\n14.76\n0.00\n\n\nAgeMiddle\n1.01\n0.41\n2.47\n0.01\n\n\nAgeSenCit\n1.39\n0.48\n2.86\n0.00"
  },
  {
    "objectID": "slides/ANOVA-Table.html#anova",
    "href": "slides/ANOVA-Table.html#anova",
    "title": "ANOVA Output in R",
    "section": "ANOVA",
    "text": "ANOVA\nBelow is the ANOVA output for the model fit above.\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA\n\n\n\n\n\nWe will focus on the sum of squares in this document. The sum of squares are as follows:\n$$\n\\[\\begin{aligned}\n&SS_{Party} = 1188.64 \\\\\n&SS_{Age|Party} = 38.03 \\\\\n&SS_{Error} = SS_{Residuals} = 686.44 \\\\\n&SS_{Total} = 1913.11\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/ANOVA-Table.html#sum-of-squares-in-anova-table",
    "href": "slides/ANOVA-Table.html#sum-of-squares-in-anova-table",
    "title": "ANOVA Output in R",
    "section": "Sum of squares in ANOVA table",
    "text": "Sum of squares in ANOVA table\nR uses a sequential method to calculate sum of squares for the variables in the model. This means that the sum of squares attributed to each variable is the variation in the response explained by that variable after accounting for the total variation explained by the other variables already in the model.\nThe order of the sequence is determined by the order of the variables in the model fit code. This order is reflected in the order the variables appear in the ANOVA output. The sequential sum of squares attributed to each variable will change if the order of the variables in the model changes; however, the sum of squares attributed to the model overall will not change, regardless of the order of the variables.\nLet‚Äôs take a look at the sum of squares for Party, the first variable in the model. This value is calculated as the total variation in Tips explained by Party only. We can calculate this value by looking at the ANOVA table for simple linear regression model where Party is the only predictor.\n\nparty_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party, data = tips)\n\nanova(party_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n274\n0\n\n\nResiduals\n167\n724.47\n4.34\nNA\nNA\n\n\n\n\n\nNotice that the sum of squares in this table is the value of \\(SS_{Party}\\) above.\nNext, let‚Äôs add Age to the model. The sum of squares associated with Age is the additional variation in Tips explained by Age after accounting for variation explained by Party. This can be understood as the additional model variation in the model with Party and Age compared to a model that only includes Party. We can calculate this additional variation as follows:\n\nanova(party_fit$fit, tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nTip ~ Party\n167\n724.47\nNA\nNA\nNA\nNA\n\n\nTip ~ Party + Age\n165\n686.44\n2\n38.03\n4.57\n0.01\n\n\n\n\n\nNotice that the sum of squares in this table is the value of \\(SS_{Age|Party}\\) above.\n\n\n\n\n\n\nNote\n\n\n\nWhen we input two model is the anova() function, e.g., anova(Model 1, Model 2), the output produced is the additional sum of squares accounted for by the new variable(s) in Model 2 after accounting for the variables in Model 1. In this case, it is the additional sum of squares accounted for by Age after accounting for Party.\n\n\nWhen we use the ANOVA table, we are most interested in the variation in the response explained by the entire model, not the contribution from each variable. Therefore, we will primarily consider the \\(SS_{Model}\\), Sum of Squares Model. Because sum of squares are additive, it can be calculated as\n\\[\n\\begin{aligned}\nSS_{Model} &= SS_{Total} - SS_{Error} \\\\\n&= 1913.11 - 686.44 \\\\\n& = 1226.67\n\\end{aligned}\n\\]It can also be calculated as\n\\[\n\\begin{aligned}\nSS_{Model} &= SS_{Party} + SS_{Age | Party} \\\\\n& = 1188.64 + 38.03 \\\\\n& = 1226.67\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#announcements",
    "href": "slides/05-slr-sim-inference.html#announcements",
    "title": "SLR: Simulation-based inference",
    "section": "Announcements",
    "text": "Announcements\n\nLab 01 due\n\nTODAY, 11:59pm (Thursday labs)\nTuesday, 11:59pm (Friday labs)\nMake sure all work is pushed to GitHub and the PDF is submitted on Gradescope by the deadline\n\n\n\n\nSee Week 03 for this week‚Äôs activities."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#topics",
    "href": "slides/05-slr-sim-inference.html#topics",
    "title": "SLR: Simulation-based inference",
    "section": "Topics",
    "text": "Topics\n\nAssess model‚Äôs predictive importance using data splitting and bootstrapping\nFind range of plausible values for the slope using bootstrap confidence intervals"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#computational-setup",
    "href": "slides/05-slr-sim-inference.html#computational-setup",
    "title": "SLR: Simulation-based inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/05-slr-sim-inference.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Simulation-based inference",
    "section": "Uninsurance vs.¬†HS graduation rates",
    "text": "Uninsurance vs.¬†HS graduation rates\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#fitting-the-model",
    "href": "slides/05-slr-sim-inference.html#fitting-the-model",
    "title": "SLR: Simulation-based inference",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nnc_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#augmenting-the-data",
    "href": "slides/05-slr-sim-inference.html#augmenting-the-data",
    "title": "SLR: Simulation-based inference",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug &lt;- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 √ó 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# ‚Ä¶ with 90 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#statistics-for-model-evaluation",
    "href": "slides/05-slr-sim-inference.html#statistics-for-model-evaluation",
    "title": "SLR: Simulation-based inference",
    "section": "Statistics for model evaluation",
    "text": "Statistics for model evaluation\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#obtaining-r2-and-rmse",
    "href": "slides/05-slr-sim-inference.html#obtaining-r2-and-rmse",
    "title": "SLR: Simulation-based inference",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.243\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        2.07"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#purpose-of-model-evaluation",
    "href": "slides/05-slr-sim-inference.html#purpose-of-model-evaluation",
    "title": "SLR: Simulation-based inference",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e.¬†out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#spending-our-data",
    "href": "slides/05-slr-sim-inference.html#spending-our-data",
    "title": "SLR: Simulation-based inference",
    "section": "Spending our data",
    "text": "Spending our data\n\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we‚Äôve done so far)"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#simulation-data-splitting",
    "href": "slides/05-slr-sim-inference.html#simulation-data-splitting",
    "title": "SLR: Simulation-based inference",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#predictive-performance",
    "href": "slides/05-slr-sim-inference.html#predictive-performance",
    "title": "SLR: Simulation-based inference",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrapping-our-data",
    "href": "slides/05-slr-sim-inference.html#bootstrapping-our-data",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#simulation-bootstrapping",
    "href": "slides/05-slr-sim-inference.html#simulation-bootstrapping",
    "title": "SLR: Simulation-based inference",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\n\nTake a bootstrap sample ‚Äì sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#predictive-performance-1",
    "href": "slides/05-slr-sim-inference.html#predictive-performance-1",
    "title": "SLR: Simulation-based inference",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#recap",
    "href": "slides/05-slr-sim-inference.html#recap",
    "title": "SLR: Simulation-based inference",
    "section": "Recap",
    "text": "Recap\n\nMotivated the importance of model evaluation\nDescribed how \\(R^2\\) and RMSE are used to evaluate models\nAssessed model‚Äôs predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#data-houses-in-duke-forest",
    "href": "slides/05-slr-sim-inference.html#data-houses-in-duke-forest",
    "title": "SLR: Simulation-based inference",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#exploratory-data-analysis",
    "href": "slides/05-slr-sim-inference.html#exploratory-data-analysis",
    "title": "SLR: Simulation-based inference",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nCode\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#modeling",
    "href": "slides/05-slr-sim-inference.html#modeling",
    "title": "SLR: Simulation-based inference",
    "section": "Modeling",
    "text": "Modeling\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2) #neatly format table to 2 digits\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#from-sample-to-population",
    "href": "slides/05-slr-sim-inference.html#from-sample-to-population",
    "title": "SLR: Simulation-based inference",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159.\n\n\n\nThis estimate is valid for the single sample of 98 houses.\nBut what if we‚Äôre not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all houses in Duke Forest?"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#statistical-inference",
    "href": "slides/05-slr-sim-inference.html#statistical-inference",
    "title": "SLR: Simulation-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference provide methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we‚Äôre interested in"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#inference-for-simple-linear-regression",
    "href": "slides/05-slr-sim-inference.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation-based inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope,\\(\\beta_1\\)"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#confidence-interval",
    "href": "slides/05-slr-sim-inference.html#confidence-interval",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#confidence-interval-for-the-slope-1",
    "href": "slides/05-slr-sim-inference.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like ‚ÄúFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus X dollars.‚Äù\n\n\nShould X be $10? $100? $1000?\nIf we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by $10? $100? $1000?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#quantify-the-variability-of-the-slope",
    "href": "slides/05-slr-sim-inference.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\n\nTwo approaches:\n\nVia simulation (what we‚Äôll do today)\nVia mathematical models (what we‚Äôll do in the next class)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-sample-1",
    "href": "slides/05-slr-sim-inference.html#bootstrap-sample-1",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-sample-2",
    "href": "slides/05-slr-sim-inference.html#bootstrap-sample-2",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-sample-3",
    "href": "slides/05-slr-sim-inference.html#bootstrap-sample-3",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-sample-4",
    "href": "slides/05-slr-sim-inference.html#bootstrap-sample-4",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-sample-5",
    "href": "slides/05-slr-sim-inference.html#bootstrap-sample-5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth‚Ä¶"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-samples-1---5",
    "href": "slides/05-slr-sim-inference.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#bootstrap-samples-1---100",
    "href": "slides/05-slr-sim-inference.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#slopes-of-bootstrap-samples",
    "href": "slides/05-slr-sim-inference.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus ___ dollars."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#slopes-of-bootstrap-samples-1",
    "href": "slides/05-slr-sim-inference.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus ___ dollars."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#confidence-level",
    "href": "slides/05-slr-sim-inference.html#confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between $0 and $250? How about $150 and $170? How about $90 and $210?"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#confidence-interval-1",
    "href": "slides/05-slr-sim-inference.html#confidence-interval-1",
    "title": "SLR: Simulation-based inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#application-exercise",
    "href": "slides/05-slr-sim-inference.html#application-exercise",
    "title": "SLR: Simulation-based inference",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã AE 03: Bootstrap confidence intervals"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-i",
    "href": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\nset.seed(1120)\n\nboot_fits &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n# A tibble: 200 √ó 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# ‚Ä¶ with 190 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-iv",
    "href": "slides/05-slr-sim-inference.html#computing-the-ci-for-the-slope-iv",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope IV",
    "text": "Computing the CI for the slope IV\nStandard error method: Alternatively, compute the 95% CI as the point estimate \\(\\pm\\) ~2 standard deviations of the bootstrap distribution:\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"se\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          90.8     228.\n2 intercept -56788.   290093."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#precision-vs.-accuracy",
    "href": "slides/05-slr-sim-inference.html#precision-vs.-accuracy",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs.¬†accuracy",
    "text": "Precision vs.¬†accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#precision-vs.-accuracy-1",
    "href": "slides/05-slr-sim-inference.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs.¬†accuracy",
    "text": "Precision vs.¬†accuracy\n\nHow can we get best of both worlds ‚Äì high precision and high accuracy?"
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#changing-confidence-level",
    "href": "slides/05-slr-sim-inference.html#changing-confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#changing-confidence-level-1",
    "href": "slides/05-slr-sim-inference.html#changing-confidence-level-1",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          104.     212.\n2 intercept  -24380.  256730.\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          56.3     226.\n2 intercept -61950.   370395."
  },
  {
    "objectID": "slides/05-slr-sim-inference.html#recap-1",
    "href": "slides/05-slr-sim-inference.html#recap-1",
    "title": "SLR: Simulation-based inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can‚Äôt continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability\n\n\n\n\nüîó Week 03"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "project-tips.html",
    "href": "project-tips.html",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Data sources\n\nSome resources that may be helpful as you find data:\n\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\nOther data repositories\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\n\n\n\n\nTips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your Qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you‚Äôre welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that‚Äôs fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\n\nFormatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\nAn alternative approach is to add the following code to the YAML:\n\nexecute:\n  echo: false\n  warning: false\n  message: false\n\n\n\n\nHeaders\n\nUse headers to clearly label each section. Make sure there is a space between the last # and the title, so the header renders correctly. For example, ###Section Title will not render as header, but ### Section Title will.\n\n\n\nReferences\n\nInclude all references in a section called ‚ÄúReferences‚Äù at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called ‚ÄúAppendix‚Äù.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\n\nResize plots and figures, so you have more space for the narrative.\n\nResize individual figures: Use the code chunk header {r plot1, fig.height = 3, fig.width = 5}, replacing plot1 with a meaningful label and the height and width with values appropriate for your write up.\nResize all figures: Include the fig_width and fig_height options in your YAML header as shown below:\n\n\n\n---\ntitle: \"Your Title\"\nauthor: \"Team Name + Group Members\"\noutput: \n  pdf_document:\n    fig_width: 5\n    fig_height: 3\n---\nReplace the height and width values with values appropriate for your write up.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\n\nIf you‚Äôre using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\nIf you‚Äôre using base R function, i.e.¬†when using the emplogit functions, put the code par(mfrow = c(rows,columns)) before the code to make the plots. For example, par(mfrow = c(2,3)) will arrange plots in a grid with 2 rows and 3 columns.\n\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\nUse coord_flip() to flip the x and y axes on the plot. This is useful if you a bar plot with an x-axis that is difficult to read due to overlapping text.\n\n‚ùå NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n‚úÖ YES! Names are readable\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  ggplot(aes(x = fct_reorder(manufacturer,n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_bw() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon‚Äôt use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n‚ùå There is a negative linear relationship between mpg and hp.\n‚úÖ There is a negative linear relationship between a car‚Äôs fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don‚Äôt assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the ‚Äúso what‚Äù: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e.¬†what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n‚ùå For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n‚úÖ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it‚Äôs from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\nAdditional resources\n\nExploring RStudio‚Äôs Visual Markdown Editor\nR for Data Science\nQuarto documentation:\n\nQuarto PDF Basics\nPresentations in Quarto\n\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you‚Äôre having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you‚Äôll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots)."
  }
]