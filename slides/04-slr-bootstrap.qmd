---
title: "SLR: Simulation-based inference"
subtitle: "Bootstrap confidence intervals for the slope"
date: "2023-09-11"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 210 - Fall 2023 -  Schedule](https://sta210-fa23.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)
library(tidyverse)
library(tidymodels)
library(knitr)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
```

### Tulane Health Policy Case Competition {.midi}

*"The Tulane Health Policy Case Competition gives teams of undergraduate students an opportunity to apply their talents and knowledge to put together a solution to a given health policy program Teams of up to three undergraduate students create policy-based solutions to a given problem. A workshop will be provided shortly after the case is released. Teams will construct PowerPoint presentations on solutions to the problem. Finalists will present to a panel of judges for real-time feedback."*

<br>

Deadline to register: September 30.

See website for more info: <https://sph.tulane.edu/tuhpcc>

## Statistics experience assignment

**Goal:** Engage with statistics / data science outside the classroom and connect your experience with what you're learning in the course.

**What:** Have a statistics experience + create a slide reflecting on the experience. Counts as a homework grade.

**When:** Must do the activity this semester. Reflection due **Monday, November 20 at 11:59pm**

For more info: [sta210-fa23.netlify.app/stats-experience](https://sta210-fa23.netlify.app/stats-experience.html)

## Statistician of the day {.midi}

**What**: Each week, we will highlight a statisticians, data scientists, or other scholars from groups who have been historically marginalized in the field and whose work has made a significant impact.

**Goal:** Learn about scholars you may not see in traditional textbooks and see the breadth of past and current work in the field.

**Who:** Scholars are from the [CURV (connecting, uplifting, and recognizing voices)](https://hardin47.github.io/CURV/) database started by Dr. Jo Hardin at Pomona College

**Participate:** Present a Statistician of the Day or contribute to the CURV data base as your statistics experience

## W.E.B. Du Bois {.midi}

::: columns
::: {.column width="40%"}
![](images/statistician-of-the-day/dubois.jpg)
:::

::: {.column width="60%"}
*Du Bois (1868 - 1963) was a sociologist who contributed to the field of data visualization through infographics related to the African American in the early twentieth century.*

*In 1900 Du Bois contributed approximately 60 data visualizations to an exhibit at the Exposition Universelle in Paris, an exhibit designed to illustrate the progress made by African Americans since the end of slavery (only 37 years prior, in 1863).*

<br>

Source: [hardin47.github.io/CURV/scholars/dubois.html](https://hardin47.github.io/CURV/scholars/dubois.html)
:::
:::

## W.E.B. Du Bois {.midi}

::: columns
::: {.column width="30%"}
***The set of visualizations demonstrate how powerfully a picture can tell 1000 words, as the information Du Bois used was primarily available from public records (e.g., census and other government reports).***

Source: [hardin47.github.io/CURV/scholars/dubois.html](https://hardin47.github.io/CURV/scholars/dubois.html)
:::

::: {.column width="70%"}
![Visualization by W.E.B. Du Bois](images/statistician-of-the-day/income-expenditures-orig.jpeg){fig-align="center"}
:::
:::

# AE 03 Follow-up

## AE 03 Follow-up {.midi}

**Goal:** Use simple linear regression to model the relationship between temperature and daily bike rentals in the **winter** season

```{r}
#| include: false


winter <- read_csv("../ae/data/dcbikeshare.csv") |>
  mutate(season = case_when(
    season == 1 ~ "winter", 
    season == 2 ~ "spring", 
    season == 3 ~ "summer", 
    season == 4 ~ "fall"
  ), 
  season = factor(season)) |>
  filter(season == "winter")
```

```{r}
#| echo: false
#| eval: true
#| out-width: "65%"

ggplot(data = winter, aes(x = temp_orig, y = count)) + 
  geom_point() + 
  labs(x = "Temperature in Celsius", 
       y = "Daily bike rentals", 
       title = "Capital Bikeshare daily bike rentals vs. temperature", 
       subtitle = "in winter months")
```

## AE 03 Follow-up

**Statistical Model**:

$$count = \beta_0 +\beta_1 ~ temp\_orig + \epsilon, \hspace{5mm} \epsilon \sim N(0, \sigma_{\epsilon}^2)$$

. . .

```{r}
winter_fit <- linear_reg() |>
  set_engine("lm") |>
  fit(count ~ temp_orig, data = winter)

tidy(winter_fit) |> kable(digits = 3)
```

## AE 03 Follow-up

Use the output to write out the estimated regression equation.

$$
\hat{count} =  -111.038 + 222.416 ~temp\_orig
$$

::: aside
LaTex:

\\\$\\\$\\hat{count} = -111.038 + 222.416 \~ temp\\\_orig\\\$\\\$
:::

::: callout-note
## Your turn!

-   Interpret the slope in the context of the data.

-   Why is there no error term in the regression equation?
:::

# Simulation-based inference

Bootstrap confidence intervals

## Topics

-   Find range of plausible values for the slope using bootstrap confidence intervals

## Computational setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(openintro)   # for Duke Forest dataset
library(scales)      # for pretty axis labels
library(glue)        # for constructing character strings
library(knitr)       # for neatly formatted tables
library(kableExtra)  # also for neatly formatted tablesf


# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
```

## Data: Houses in Duke Forest

::: columns
::: {.column width="50%"}
-   Data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020
-   Scraped from Zillow
-   Source: [`openintro::duke_forest`](http://openintrostat.github.io/openintro/reference/duke_forest.html)
:::

::: {.column width="50%"}
![](images/05/duke_forest_home.jpg){fig-alt="Home in Duke Forest"}
:::
:::

**Goal**: Use the area (in square feet) to understand variability in the price of houses in Duke Forest.

## Exploratory data analysis

```{r}
#| code-fold: true
ggplot(duke_forest, aes(x = area, y = price)) +
  geom_point(alpha = 0.7) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = "Price and area of houses in Duke Forest"
  ) +
  scale_y_continuous(labels = label_dollar()) 
```

## Modeling {.midi}

```{r}
#| echo: true
#| code-line-numbers: "|5|6"

df_fit <- linear_reg() |>
  #set_engine("lm") |>
  fit(price ~ area, data = duke_forest)

tidy(df_fit) |>
  kable(digits = 2) #neatly format table to 2 digits
```

. . .

<br>

```{r}
#| echo: false
intercept <- tidy(df_fit) |> filter(term == "(Intercept)") |> pull(estimate) |> round()
slope <- tidy(df_fit) |> filter(term == "area") |> pull(estimate) |> round()
```

-   **Intercept:** Duke Forest houses that are 0 square feet are expected to sell, for `r dollar(intercept)`, on average.
    -   Is this interpretation useful?
-   **Slope:** For each additional square foot, we expect the sale price of Duke Forest houses to be higher by `r dollar(slope)`, on average.

## From sample to population {.midi}

> For each additional square foot, we expect the sale price of Duke Forest houses to be higher by `r dollar(slope)`, on average.

<br>

-   This estimate is valid for the single sample of `r nrow(duke_forest)` houses.
-   But what if we're not interested quantifying the relationship between the size and price of a house in this single sample?
-   What if we want to say something about the relationship between these variables for all houses in Duke Forest?

## Statistical inference

-   **Statistical inference** provide methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from

-   For our inferences to be valid, the sample should be random and representative of the population we're interested in

## Inference for simple linear regression

-   Calculate a confidence interval for the slope, $\beta_1$

-   Conduct a hypothesis test for the slope,$\beta_1$

# Confidence interval for the slope

## Confidence interval {.midi}

::: incremental
-   A plausible range of values for a population parameter is called a **confidence interval**
-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net
    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish
    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter
:::

## Confidence interval for the slope {.midi}

A confidence interval will allow us to make a statement like "*For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`, plus or minus X dollars.*"

. . .

-   Should X be \$10? \$100? \$1000?

-   If we were to take another sample of `r nrow(duke_forest)` would we expect the slope calculated based on that sample to be exactly `r dollar(slope)`? Off by \$10? \$100? \$1000?

-   The answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is

-   We need a way to quantify the variability of the sample statistic

## Quantify the variability of the slope {.midi}

**for estimation**

::: incremental
-   Two approaches:
    1.  Via simulation (what we'll do today)
    2.  Via mathematical models (what we'll do in the next class)
-   **Bootstrapping** to quantify the variability of the slope for the purpose of estimation:
    -   Bootstrap new samples from the original sample
    -   Fit models to each of the samples and estimate the slope
    -   Use features of the distribution of the bootstrapped slopes to construct a confidence interval
:::

```{r}
#| echo: false
set.seed(119)

df_boot_samples_5 <- duke_forest |>
  specify(price ~ area) |>
  generate(reps = 5, type = "bootstrap")
```

## Bootstrap sample 1

<!-- Fix the scales for these slides, so the difference in slopes is easier to see-->

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs <- ggplot(duke_forest, aes(x = area, y = price)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = "Price and area of houses in Duke Forest"
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 1

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 2

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 2

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 3

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 3

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 4

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 4

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 5

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 5

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

. . .

*so on and so forth...*

## Bootstrap samples 1 - 5

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

ggplot(df_boot_samples_5, aes(x = area, y = price, group = replicate)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.5) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap samples 1 - 5")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap samples 1 - 100

```{r}
#| echo: false
set.seed(119)

df_boot_samples_100 <- duke_forest |>
  specify(price ~ area) |>
  generate(reps = 100, type = "bootstrap")
```

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_boot_samples_100 <- ggplot(df_boot_samples_100, aes(x = area, y = price, group = replicate)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.05) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap samples 1 - 100")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())

p_df_boot_samples_100
```
:::
:::

## Slopes of bootstrap samples

::: question
**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`, plus or minus \_\_\_ dollars.
:::

```{r}
#| echo: false
p_df_boot_samples_100 +
  geom_abline(intercept = intercept, slope = slope, color = "#8F2D56")
```

## Slopes of bootstrap samples

::: question
**Fill in the blank:** For each additional square foot, we expect the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`, plus or minus \_\_\_ dollars.
:::

```{r}
#| echo: false
df_boot_samples_100_fit <- df_boot_samples_100 |>
  fit()

df_boot_samples_100_hist <- ggplot(df_boot_samples_100_fit |> filter(term == "area"), aes(x = estimate)) +
  geom_histogram(binwidth = 10, color = "white") +
  geom_vline(xintercept = slope, color = "#8F2D56", size = 1) +
  labs(x = "Slope", y = "Count",
       title = "Slopes of 100 bootstrap samples") +
  scale_x_continuous(labels = label_dollar())

df_boot_samples_100_hist
```

## Confidence level

::: question
How confident are you that the true slope is between \$0 and \$250? How about \$150 and \$170? How about \$90 and \$210?
:::

```{r}
#| echo: false
df_boot_samples_100_hist
```

## 95% confidence interval {.midi}

```{r}
#| echo: false
lower <- df_boot_samples_100_fit |>
  ungroup() |>
  filter(term == "area") |>
  summarise(quantile(estimate, 0.025)) |>
  pull()

upper <- df_boot_samples_100_fit |>
  ungroup() |>
  filter(term == "area") |>
  summarise(quantile(estimate, 0.975)) |>
  pull()

df_boot_samples_100_hist +
  geom_vline(xintercept = lower, color = "steelblue", size = 1, linetype = "dashed") +
  geom_vline(xintercept = upper, color = "steelblue", size = 1, linetype = "dashed")
```

::: incremental
-   A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution
-   We are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(lower)` to `r dollar(upper)`.
:::

# Application exercise

::: appex
ðŸ“‹ [AE 04: Bootstrap confidence intervals](https://sta210-fa23.netlify.app/ae/ae-04-bootstrap.html)
:::

## Computing the CI for the slope I

Calculate the observed slope:

```{r}
#| echo: true

observed_fit <- duke_forest |>
  specify(price ~ area) |>
  fit()

observed_fit
```

## Computing the CI for the slope II {.smaller}

Take `100` bootstrap samples and fit models to each one:

```{r}
#| echo: true
#| code-line-numbers: "1,5,6"

set.seed(1120)

boot_fits <- duke_forest |>
  specify(price ~ area) |>
  generate(reps = 100, type = "bootstrap") |>
  fit()

boot_fits
```

## Computing the CI for the slope III

**Percentile method:** Compute the 95% CI as the middle 95% of the bootstrap distribution:

```{r}
#| echo: true
#| code-line-numbers: "5"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "percentile" #default method
)
```

## Precision vs. accuracy

::: question
If we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?
:::

. . .

![](images/05/garfield.png)

## Precision vs. accuracy

::: question
How can we get best of both worlds -- high precision and high accuracy?
:::

## Changing confidence level

::: question
How would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?
:::

```{r}
#| echo: true
#| code-line-numbers: "|4"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "percentile"
)
```

## Changing confidence level {.midi}

```{r}
#| echo: true

## confidence level: 90%
get_confidence_interval(
  boot_fits, point_estimate = observed_fit, 
  level = 0.90, type = "percentile"
)

## confidence level: 99%
get_confidence_interval(
  boot_fits, point_estimate = observed_fit, 
  level = 0.99, type = "percentile"
)
```

## Recap {.smaller}

-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)

-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)

-   Sample statistic $\ne$ population parameter, but if the sample is good, it can be a good estimate

-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process

-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population

-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability
