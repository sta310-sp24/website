---
title: "Logistic Regression: Prediction + classification"
author: "Prof. Maria Tackett"
date: "2023-11-06"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 210 - Fall 2023 -  Schedule](https://sta210-fa23.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  warning: false
  message: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 8, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)

library(countdown)
```

## Announcements

-   Lab this week: Work on project

-   Project draft due in your GitHub repo at **9am** on

    -   November 14 (Tuesday labs)

    -   November 16 (Thursday labs)

    -   Will do peer review in lab those days

-   Team Feedback #1 due Friday, November 10 at 11:5pm

    -   Will receive email from Teammates

## Odds ratios practice {.small}

Let's take a look at one of the models from [Lab 06](../labs/lab-06.html) using flipper length and species to predict the odds a penguin is large (has a body mass above average).

```{r}
#| echo: false
library(tidyverse)
library(tidymodels)
library(knitr)
library(palmerpenguins)

penguins <- penguins |>
  drop_na() |>
  mutate(large = factor(if_else(body_mass_g > mean(body_mass_g, na.rm = TRUE), 1, 0)))

logistic_reg() |>
  set_engine("glm") |>
  fit(large ~ species + flipper_length_mm, data = penguins) |>
  tidy() |>
  kable(digits = 3)
```

::: question
-   Interpret the coefficient of `flipper_length_mm` in terms of the **odds** a penguin is large.

-   Interpret the coefficient of `speciesChinstrap` in terms of the **odds** a penguin is large.
:::

```{r}
#| echo: false
countdown::countdown(minutes = 3)
```

## Topics

::: nonincremental
-   Building predictive logistic regression models
-   Sensitivity and specificity
-   Making classification decisions
:::

## Computational setup

```{r}
#| echo: true

# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 20))
```

# Data

## `openintro::email`

These data represent incoming emails for the first three months of 2012 for an email account.

::: nonincremental
-   Outcome: `spam` - Indicator for whether the email was spam.
-   Predictors: `spam`, `to_multiple`, `from`, `cc`, `sent_email`, `time`, `image`, `attach`, `dollar`, `winner`, `inherit`, `viagra`, `password`, `num_char`, `line_breaks`, `format`, `re_subj`, `exclaim_subj`, `urgent_subj`, `exclaim_mess`, `number`.
:::

Click [here](http://openintrostat.github.io/openintro/reference/email.html) for more detailed information on the variables.

## Training and testing split

```{r}
# Fix random numbers by setting the seed 
# Enables analysis to be reproducible when random numbers are used 
set.seed(1109)

# Put 75% of the data into the training set 
email_split <- initial_split(email)

# Create data frames for the two sets
email_train <- training(email_split)
email_test  <- testing(email_split)
```

## Exploratory data analysis

The sample is **unbalanced** with respect to `spam`.

```{r}
#| echo: false

ggplot(email_train, aes(x = spam)) + 
  geom_bar() +
  labs(title = "Distribution of spam in training data")
```

## Reminder: Modeling workflow

-   Create a recipe for feature engineering steps to be applied to the training data

-   Fit the model to the training data after these steps have been applied

    -   Use cross-validation if deciding between multiple models

-   Using the model estimates from the training data, predict outcomes for the test data

-   Evaluate the performance of the model on the test data

# Start with a recipe

## Initiate a recipe {.midi}

```{r initiate-recipe, results="hide"}
email_rec <- recipe(
  spam ~ .,          # formula
  data = email_train  # data to use for cataloging names and types of variables
  )
summary(email_rec)
```

```{r echo=FALSE}
summary(email_rec) |> print(n = 21)
```

## Remove certain variables

```{r}
email_rec <- email_rec |>
  step_rm(from, sent_email)
```

```{r echo=FALSE}
email_rec
```

## Feature engineer date

```{r}
email_rec <- email_rec |>
  step_date(time, features = c("dow", "month")) |>
  step_rm(time)
```

```{r echo=FALSE}
email_rec
```

## Discretize numeric variables

```{r}
email_rec <- email_rec |>
  step_cut(cc, attach, dollar, breaks = c(0, 1))
```

```{r echo=FALSE}
email_rec
```

## Create dummy variables

```{r}
email_rec <- email_rec |>
  step_dummy(all_nominal(), -all_outcomes())
```

```{r echo=FALSE}
email_rec
```

## Remove zero variance variables

Variables that contain only a single value

```{r}
email_rec <- email_rec |>
  step_zv(all_predictors())
```

```{r echo=FALSE}
email_rec
```

## Recipe: All in one place

```{r}
email_rec <- recipe(spam ~ ., data = email_train) |>
  step_rm(from, sent_email) |>
  step_date(time, features = c("dow", "month")) |>               
  step_rm(time) |>
  step_cut(cc, attach, dollar, breaks = c(0, 1)) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())
```

# Build a workflow

## Define model

```{r}
email_spec <- logistic_reg() |> 
  set_engine("glm")
email_spec
```

## Define workflow {.midi}

**Remember:** Workflows bring together models and recipes so that they can be easily applied to both the training and test data.

```{r}
email_wflow <- workflow() |> 
  add_model(email_spec) |> 
  add_recipe(email_rec)
```

```{r echo=FALSE}
email_wflow
```

## Fit model to training data {.midi}

```{r}
email_fit <- email_wflow |> 
  fit(data = email_train)

tidy(email_fit) |> print(n = 31)
```

# Make predictions

## Make predictions for test data

```{r}
email_pred <- predict(email_fit, email_test, type = "prob") |> 
  bind_cols(email_test) 
email_pred
```

## A closer look at predictions

::: question
Which of the following 10 emails will be misclassified?
:::

```{r}
email_pred |>
  arrange(desc(.pred_1)) |>
  select(contains("pred"), spam) |> slice(1:10)
```

# Sensitivity and specificity

## False positive and negative {.midi}

|                              | Email is spam                   | Email is not spam               |
|------------------------------|---------------------------------|---------------------------------|
| Email classified as spam     | True positive                   | False positive (*Type 1 error*) |
| Email classified as not spam | False negative (*Type 2 error*) | True negative                   |

<br>

. . .

-   False negative rate = P(classified as not spam \| Email spam)

= `FN / (TP + FN)`

-   False positive rate = P(classified as spam \| Email not spam)

= `FP / (FP + TN)`

## Sensitivity and specificity {.midi}

|                              | Email is spam                   | Email is not spam               |
|------------------------------|---------------------------------|---------------------------------|
| Email classified as spam     | True positive                   | False positive (*Type 1 error*) |
| Email classified as not spam | False negative (*Type 2 error*) | True negative                   |

. . .

-   Sensitivity = P(classified as spam \| Email spam) = `TP / (TP + FN)`
    -   **Sensitivity = 1 âˆ’ False negative rate**
-   Specificity = P(classified as not spam \| Email not spam) = `TN / (FP + TN)`
    -   **Specificity = 1 âˆ’ False positive rate**

. . .

::: question
If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?
:::

## Evaluate the performance

**Receiver operating characteristic (ROC) curve**<sup>+</sup> plots the true positive rate (sensitivity) vs. false positive rate (1 - specificity).

::: aside
<sup>+</sup> Originally developed for operators of military radar receivers, hence the name.
:::

::: columns
::: {.column width="40%"}
```{r}
#| label: roc-curve
#| fig.show: hide

email_pred |>
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) |>
  autoplot()
```
:::

::: {.column width="60%"}
```{r}
#| ref.label: roc-curve
#| echo: false
#| out.width: "100%"
```
:::
:::

## ROC curve, under the hood

```{r}
email_pred |>
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
```

## ROC curve

```{r}
#| echo: false

email_pred |>
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) |>
  autoplot() +
  annotate("point", x = 0, y = 1, color = "#5B888C") +
  annotate("point", x = 0, y = 1, color = "#5B888C", size = 3, shape = "circle open") +
  annotate(
    "label", x = 0.02, y = 0.99, label = "All spams classified as spam,\nall non-spams classified as nonspam", hjust = 0, color = "#5B888C", fontface = "bold", vjust = 1, fill = "white"
  ) +
  annotate("point", x = 1, y = 0, color = "#8F2D56") +
  annotate("point", x = 1, y = 0, color = "#8F2D56", size = 3, shape = "circle open") +
  annotate(
    "label", x = 0.98, y = 0.01, label = "All spams classified as non-spam,\nall non-spams classified as spam", hjust = 1, color = "#8F2D56", fontface = "bold", vjust = 0, fill = "white"
  ) +
  annotate(
    "segment", color = "#5b708c", x = 0, y = 0, xend = 1, yend = 1, size = 2, alpha = 0.6
  ) +
  annotate(
    "label", x = 0.58, y = 0.5, label = "True positive rate\n= false positive rate", hjust = 0, color = "#5b708c", fontface = "bold", fill = "white"
  )
```

## Evaluate the performance: AUC

```{r}
#| label: roc-auc

email_pred |>
  roc_auc(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
```

. . .

The **area under the curve (AUC)** can be used to assess how well the logistic model fits the data

-   AUC=0.5: model is a very bad fit (no better than a coin flip)

-   AUC close to 1: model is a good fit

# Make decisions

## Cutoff probability: 0.5 {.midi}

::: panel-tabset
## Output

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.5**.

```{r}
#| ref.label: confusion-50
#| echo: false
```

## Code

```{r}
#| label: confusion-50
#| results: hide

cutoff_prob <- 0.5
email_pred |>
  mutate(
    spam_pred = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0)),
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(spam_pred == 1, "Email classified as spam", "Email classified as not spam")
    ) |>
  count(spam_pred, spam) |>
  pivot_wider(names_from = spam, values_from = n) |>
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```
:::

## Confusion matrix

Cross-tabulation of observed and predicted classes:

```{r}

cutoff_prob <- 0.5
email_pred |>
  mutate(spam_predicted = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0))) |>
  conf_mat(truth = spam, estimate = spam_predicted)
```

## Classification

```{r}
#| fig.asp: 0.6
#| echo: false
#| fig.width: 10

email_pred |>
  mutate(
    spam_pred = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0)),
    classification = if_else(spam == spam_pred, "Correct classification", "Misclassification")
  ) |>
  ggplot(aes(x = .pred_1, y = spam,
             color = classification, shape = classification)) +
  geom_jitter(width = 0, alpha = 0.7, size = 2) +  geom_vline(xintercept = cutoff_prob, linetype = "dashed") +
  scale_color_manual(values = c("#5B888C", "#8F2D56")) +
  theme(legend.position = "bottom") +
  labs(
    color = NULL, shape = NULL,
    x = "Predicted probability (.pred_1)",
    y = "Observed"
  ) +
  annotate("label",
    x = cutoff_prob, y = 2.5,
    label = paste0("Cutoff probability = ", cutoff_prob)
  ) +
  coord_cartesian(clip = "off")
```

## Cutoff probability: 0.25 {.midi}

::: panel-tabset
## Output

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.25**.

```{r}
#| ref.label: confusion-25
#| echo: false
```

## Code

```{r confusion-25}
#| label: confusion-25
#| results: hide

cutoff_prob <- 0.25
email_pred |>
  mutate(
    spam_pred = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0)),
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(spam_pred == 1, "Email classified as spam", "Email classified as not spam")
    ) |>
  count(spam_pred, spam) |>
  pivot_wider(names_from = spam, values_from = n) |>
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```
:::

. . .

## Classification

```{r}
#| fig.asp: 0.6
#| echo: false
#| fig.width: 10

email_pred |>
  mutate(
    spam_pred = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0)),
    classification = if_else(spam == spam_pred, "Correct classification", "Misclassification")
  ) |>
  ggplot(aes(x = .pred_1, y = spam,
             color = classification, shape = classification)) +
  geom_jitter(width = 0, alpha = 0.6, size = 2) +
  geom_vline(xintercept = cutoff_prob, linetype = "dashed") +
  scale_color_manual(values = c("#5B888C", "#8F2D56")) +
  theme(legend.position = "bottom") +
  labs(
    color = NULL, shape = NULL,
    x = "Predicted probability (.pred_1)",
    y = "Observed"
  ) +
  annotate("label",
    x = cutoff_prob, y = 2.5,
    label = paste0("Cutoff probability = ", cutoff_prob)
  ) +
  coord_cartesian(clip = "off")
```

## Cutoff probability: 0.75 {.midi}

::: panel-tabset
## Output

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.75**.

```{r}
#| ref.label: confusion-75
#| echo: false
```

## Code

```{r confusion-75}
#| label: confusion-75
#| results: hide

cutoff_prob <- 0.75
email_pred |>
  mutate(
    spam_pred = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0)),
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(spam_pred == 1, "Email classified as spam", "Email classified as not spam")
    ) |>
  count(spam_pred, spam) |>
  pivot_wider(names_from = spam, values_from = n) |>
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```
:::

## Classification

```{r}
#| fig.asp: 0.6
#| echo: false
#| fig.width: 10

email_pred |>
  mutate(
    spam_pred = as_factor(if_else(.pred_1 >= cutoff_prob, 1, 0)),
    classification = if_else(spam == spam_pred, "Correct classification", "Misclassification")
  ) |>
  ggplot(aes(x = .pred_1, y = spam, 
             color = classification, shape = classification)) +  geom_jitter(width = 0, alpha = 0.6, size = 2) +
  geom_vline(xintercept = cutoff_prob, linetype = "dashed") +
  scale_color_manual(values = c("#5B888C", "#8F2D56")) +
  theme(legend.position = "bottom") +
  labs(
    color = NULL, shape = NULL,
    x = "Predicted probability (.pred_1)",
    y = "Observed"
  ) +
  annotate("label",
    x = cutoff_prob, y = 2.5,
    label = paste0("Cutoff probability = ", cutoff_prob)
  ) +
  coord_cartesian(clip = "off")
```

## Use ROC curve

Use the ROC curve to determine the best cutoff probability

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: 100%

email_pred |>
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) |> 
  autoplot()
```
:::

::: {.column width="50%"}
```{r}
#| echo: false

email_pred |>
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) |> 
  filter(sensitivity > 0.70, sensitivity < 0.8) |>
  slice(1:10)

```
:::
:::

## Recap

-   Built predictive logistic regression models
-   Defined and calculated sensitivity and specificity
-   Made classification decisions based on sensitivity and specificity
